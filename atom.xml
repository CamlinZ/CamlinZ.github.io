<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Camlin Zhang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://camlinzhang.com/"/>
  <updated>2019-04-30T07:05:25.135Z</updated>
  <id>http://camlinzhang.com/</id>
  
  <author>
    <name>Camlin Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>目标检测two-stage经典方法总结</title>
    <link href="http://camlinzhang.com/2019/03/25/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Btwo-stage%E7%BB%8F%E5%85%B8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://camlinzhang.com/2019/03/25/目标检测two-stage经典方法总结/</id>
    <published>2019-03-25T12:33:08.000Z</published>
    <updated>2019-04-30T07:05:25.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="计算机视觉的三大任务"><a href="#计算机视觉的三大任务" class="headerlink" title="计算机视觉的三大任务"></a>计算机视觉的三大任务</h2><ul><li><p>分类（是什么）：给定一张图片，为每张图片打一个标签，说出图片是什么物体。然而因为一张图片中往往有多个物体，因此我们允许你取出概率最大的5个，只要前五个概率最大的包含了我们人工标定标签，就认定正确（top-k预测）</p></li><li><p>定位（在哪儿）：除了需要预测出图片的类别，你还要定位出这个物体的位置，同时规定你定位的这个物体框与正确位置差不能超过规定的阈值</p></li><li><p>检测（在哪儿有什么）：给定一张图片，你把图片中的所有物体全部给我找出来（包括位置、类别）</p></li></ul><h2 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h2><blockquote><p>《OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks》</p></blockquote><p>在早期方法中，通常使用以下的传统框架进行目标检测任务(这里以人脸检测作为一个例子)：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1fc1sp37zj31000dek2d.jpg" alt="目标检测早期方法传统框架"></p><p>可以看到传统的框架通常使用下面的步骤进行检测任务：</p><ol><li><p>首先利用大量图片训练一个目标检测的分类器</p></li><li><p>在不同尺度的待检测图像中使用密集滑窗进行滑动，对每一个滑窗的位置使用第1步训练的分类器进行分类来判断是否为待检测目标</p></li><li><p>将密集滑窗策略得到的待检测目标的检测框进行NMS合并</p></li></ol><p>在上面的步骤中很明显可以发现传统检测方法是将分类和定位任务分开进行，并利用滑窗策略来对图像中的所有位置进行定位(后面简称为滑窗定位策略)，即将分类和滑窗定位策略合并起来最终完成检测任务。</p><h3 id="引入问题"><a href="#引入问题" class="headerlink" title="引入问题"></a>引入问题</h3><p>上面的传统方法检测框架中直觉上主要的改进可以朝着两个方向进行：</p><ol><li><p>对于分类器的改进，使用区别于早期方法的更有效的分类器，在AlexNet取得非常好的结果之后，基本上均是使用卷积神经网络来代替传统分类器，因此在这一点上主要关注的就是卷积神经网络主干网络的发展</p></li><li><p>对于滑动窗口方法的改进，OverFeat论文中主要指出几点：</p></li></ol><blockquote><p>The ﬁrst idea in addressing this is to apply a ConvNet at multiple locations in the image, in a sliding window fashion, and over multiple scales. Even with this, however, many viewing windows may contain a perfectly identiﬁable portion of the object (say, the head of a dog), but not the entire object, nor even the center of the object. This leads to decent classiﬁcation but poor localization and detection.</p></blockquote><blockquote><p>The second idea is to train the system to not only produce a distribution over categories for each window, but also to produce a prediction of the location and size of the bounding box containing the object relative to the window.</p></blockquote><blockquote><p>The third idea is to accumulate the evidence for each category at each location and size.</p></blockquote><h3 id="OverFeat改进方法"><a href="#OverFeat改进方法" class="headerlink" title="OverFeat改进方法"></a>OverFeat改进方法</h3><p>Overfeat方法便是在以上框架的基础上利用深度学习的卷积神经网络进行改进，将分类，定位和检测任务用一个网络完成，主要改进点为：</p><ul><li><p>改进AlexNet分类网络作为主干网络，也即本文提出的<strong>OverFeat</strong>，它是将该主干网络作为一个特征提取算子，提取出特征为后面的分类和定位任务做准备</p></li><li><p>利用在OverFeat后面接上全连接层进行训练，来完成分类任务；然后将全连接层替换成FCN网络，保持训练分类任务的OverFeat结构参数不变，训练检测框回归任务(可以看出早期的深度学习方法还是延续了传统方法的思想，即利用类似于SIFT和HOG这些特征提取算子来提取特征，然后再在后面接各种不同任务的分类器进行分类，实际上在卷积神经网络中是可以将所有方法合并成一个end-to-end的任务的)，从而实现将分类、定位和检测集成在一个网络中</p></li><li><p>引入offset pooling(即一种特征图滑窗策略)来替代传统的在输入图片上进行的密集滑窗策略，并且对特征图中每一个像素点(即每一次滑窗的位置)直接进行回归任务，预测出检测框的位置，而不是原始的密集滑窗策略那样由每一次滑动窗口的位置作为检测框的位置，这样就避免了固定尺寸的检测框会切断物体</p></li></ul><p>OverFeat整体流程图如下所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g1hecz4vepj30rb0vtafs.jpg" alt=""></p><h3 id="Offset-Pooling"><a href="#Offset-Pooling" class="headerlink" title="Offset Pooling"></a>Offset Pooling</h3><p>Offset pooling可以看做是一种特征图滑窗策略，用于针对同一尺度下的输入图片产生的特征图中不同位置进行分类和检测，用于替代传统方法中在输入图片上进行密集滑窗策略这种耗时的操作。在论文中的解释如下：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1h831y4p7j30yq0hf796.jpg" alt=""></p><p>具体操作步骤为：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1hf4re0nxj30o500r74a.jpg" alt=""><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1hf5izkj0j30o606fq4y.jpg" alt=""></p><p>其中的layer5即对应OverFeat整体流程图中特征图2</p><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1hf8akn3xj30o50gcgsu.jpg" alt=""></p><h3 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h3><p>OverFeat论文中的特征提取模型是在AlexNet的基础上进行改进(注意后面的分析均是基于高精度模型)：</p><ol><li><p>没有使用局部响应归一化层；</p></li><li><p>没有采用重叠池化的方法(即每一次池化层的卷积核的stride为卷积核大小的一半，因此卷积核在滑动的过程中没有重叠部分)；</p></li><li><p>在第一层卷积层，stride作者是选择了2，这个与AlexNet中的4不用，如果stride选择比较大得话，虽然可以减少网络层数，提高速度，但是会降低精度</p></li></ol><p>在此基础上，作者构造出了快速模型和高精度模型两种架构：</p><ul><li><p>快速模型架构<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1g0g45j59j30xt0ea0xs.jpg" alt=""></p></li><li><p>高精度模型架构<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1g0hof10hj30wg0an0vx.jpg" alt=""></p></li></ul><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>在分类任务的训练阶段，OverFeat采用AlexNet的图像增强方法：</p><ul><li><p>对于训练数据集中每一张256×256图片及其镜像，随机crop成一张224×224的图片</p></li><li><p>利用PCA来处理RGB三通道的值</p></li></ul><p>AlexNet的详细处理步骤为：<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1hkmalddvj30ox0f5jwo.jpg" alt=""><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1hkojp4x1j30oz07x40w.jpg" alt=""></p><p>再将上述图像增强后的图片输入网络进行训练</p><h4 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h4><p>AlexNet在模型测试阶段的方法为：</p><ul><li><p>对于输入的一张256×256图片，首先进行multi-view crop，即分别从图片的四个角以及中心裁剪出5张224×224的图片，把原始图片水平翻转一下，再用同样的方式进行裁剪，又可以得到5张图片</p></li><li><p>把这10张224×224图片作为输入，分别进行预测分类，在softmax得到10张crop图片对应的每一类的概率，再对其取平均值，得到最终每一类的概率值(这也是为什么caffe中AlexNet的网络模型中input的第一维也即是batch的大小是10的原因)</p></li></ul><p>由于AlexNet的测试方法对于检测任务来说，存在以下缺点：</p><ol><li><p>multi-view crop在检测中很容易将待检测物体截断，使crop之后的图片中待检测物体不完整</p></li><li><p>multi-view crop得到的图片中，各个图片块之间存在很大的重叠面积，因此在测试的过程中存在计算冗余</p></li><li><p>multi-view crop的方式只是在单一尺度上进行</p></li></ol><p>针对以上问题OverFeat采用以下测试方法：</p><ul><li>对原图做以下放大处理(由于通常检测问题中对于大尺寸物体的检测效果较好，对于小物体的检测效果不好，所以构造多尺度输入的时候只放大图片)得到6个尺寸的图片，并对所有尺寸的图片做翻转作为输入。</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1g6msbgx5j30m208r40p.jpg" alt=""></p><ul><li><p>将以上图片输入模型直接进行测试，并在此处引入offset pooling来实现原始检测框架中的密集滑窗策略，从而对于每一个尺寸的输入图片产生如上表最后一列所示数量的特征向量</p></li><li><p>对于网络输出的各个尺度对应的特征向量，例如第一个尺度产生的输出3x3xC，则是对于每一个类别对应的3x3个特征向量取平均值，得到该尺度下每一个类别的概率值，以此类推到各个尺度，便可以求出各个尺度中各个类别的概率。最后将各个尺度中相同类别的概率值做平均，便可以得到最终的各个类别的概率，根据评价指标的需要取最大的一个概率对应的类别作为top-1，或者最大的5个概率对应的类别作为top-5.</p></li></ul><h3 id="定位任务"><a href="#定位任务" class="headerlink" title="定位任务"></a>定位任务</h3><h4 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h4><p>保持分类模型中训练好的OverFeat模型参数不变，在模型后面加上定位任务的网络进行训练，从而得到最终的定位网络(文中并没有给出训练回归任务的loss函数)</p><h4 id="模型定位步骤"><a href="#模型定位步骤" class="headerlink" title="模型定位步骤"></a>模型定位步骤</h4><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1hl0abyfyj30p00lqdki.jpg" alt=""></p><p>从上图可以看到，以281x317的输入尺寸为例，OverFeat论文中对于上面OverFeat整体流程图中特征图2首先进行一个5x5的pooling层得到2x3的特征图，然后经过4096x1x1和1000x1x1的全连接层(这里使用FCN实现)之后最终输出每一个尺度产生的每一个特征图中的每一个像素点映射到原图中的检测框的坐标值，即图中的(top, left, right, right box edges)。对此，作者通过下面的几组图片分析了多尺度策略，offset pooling策略对模型产生检测框的密集度：</p><ul><li><p>利用不同的尺寸来进行预测，由于尺寸越大，对应的第五层产生的特征图的大小也越大，而特征图中每一个像素点都对应一个检测框，因此原图尺寸越大，越会产生更多的检测框<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1hl7zyg07j30nb0947hc.jpg" alt=""></p></li><li><p>在上面的基础上，运用offset pooling 即滑窗的方法，因此在每一个scale又产生了不同的3x3个不同滑动窗口，因此进一步的增加了检测窗口的数量<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1hl9emjztj30ng0at7i2.jpg" alt=""></p></li><li><p>此处展示的是上面的每一个尺寸的每一滑窗的每一个像素点对应的位置回归网络产生的四个坐标点回归值<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1hlag5o5mj30n90ajakt.jpg" alt=""></p></li><li><p>结合上图中各个尺度各个滑窗上产生的检测框<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1hlhptzrej30up03xq41.jpg" alt=""><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1hli3gbmej30un07aaba.jpg" alt=""></p></li></ul><h3 id="分类和定位任务"><a href="#分类和定位任务" class="headerlink" title="分类和定位任务"></a>分类和定位任务</h3><p>整体上OverFeat利用OverFeat特征提取器来提取特征，将该特征运用于后面的分类和定位任务，即可以看作对于后面产生的特征图中的每一个像素点映射到原始图中的区域，在该区域预测出分类任务中各个类别的概率以及回归任务中检测框的坐标值，下面同样以281x317的输入尺寸为例，给出网络同时进行两项任务的示意图：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1hm9wgdfaj30gz0ijtg2.jpg" alt=""></p><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p><a href="https://zhum.in/blog/project/TrafficSignRecognition/OverFeat%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">https://zhum.in/blog/project/TrafficSignRecognition/OverFeat%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</a></p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><blockquote><p>《Rich feature hierarchies for accurate object detection and semantic segmentation》</p></blockquote><p>2012年，AlexNet第一次利用卷积神经网络在ILSVRC比赛上获得了超越第二名几乎一半的成绩，引起了极大的反响，在目标检测领域，主要争论的问题是：</p><blockquote><p>To what extent do the CNN classiﬁcation results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?</p></blockquote><blockquote><p>ImageNet上的CNN分类结果在何种程度上能够应用到PASCAL VOC挑战的物体检测任务上？</p></blockquote><p>针对这个问题，本文主要关注两个方面：</p><ol><li>localizing objects with a deep network(主要关注如何使用AlexNet中的深度网络来完成定位问题)</li><li>training a high-capacity model with only a small quantity of annotated detection data(主要关注如何将AlexNet中深度网络的权重迁移到定位问题上)</li></ol><p>下面就上面两个问题进行分析R-CNN的脉络</p><h3 id="使用深度网络来定位物体"><a href="#使用深度网络来定位物体" class="headerlink" title="使用深度网络来定位物体"></a>使用深度网络来定位物体</h3><p>在图像中定位物体通常使用一下两种方法：</p><ol><li>将定位问题看作是回归问题(OverFeat采用的方法)，R-CNN论文中利用Szegedy等的工作说明这种方法不是很好</li><li>构造一个滑动窗口检测器(R-CNN中采用的方法)</li></ol><p>下面具体说明R-CNN的具体构造：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jgru0yqdj30fj05976j.jpg" alt=""></p><p>如上图所示，R-CNN的测试部分主要分为三个模块:</p><ol><li>产生类别无关的候选框</li><li>卷积神经网络产生定长的特征向量</li><li>指定类别的SVM线性分类器</li></ol><h4 id="产生类别无关的候选框"><a href="#产生类别无关的候选框" class="headerlink" title="产生类别无关的候选框"></a>产生类别无关的候选框</h4><p>R-CNN采用论文《Selective search for object recognition》中的Selective Search方法来产生<strong>2000个</strong>后面需要的候选框，这个方法主要有三个优势：</p><ul><li>捕捉不同尺度（Capture All Scales）</li><li>多样化（Diversification）</li><li>快速计算（Fast to Compute）</li></ul><p>Selective Search算法主要包含两个内容</p><ul><li>Hierarchical Grouping Algorithm</li><li>Diversification Strategies</li></ul><p><strong>Hierarchical Grouping Algorithm</strong></p><ol><li>使用论文《Efficient Graph-Based Image Segmentation》中的方法在图像中产生初始区域(参考：<a href="https://blog.csdn.net/ttransposition/article/details/38024557" target="_blank" rel="noopener">https://blog.csdn.net/ttransposition/article/details/38024557</a>)</li><li>计算所有邻近区域之间的相似性</li><li>两个最相似的区域被组合在一起</li><li>计算合并区域和相邻区域的相似度</li><li>重复2、3过程，直到整个图像变为一个地区。</li></ol><p><strong>Diversification Strategies</strong></p><p>这个部分涉及到多样性的一些策略，使得抽样多样化，主要有下面三个不同方面：</p><ol><li>利用各种不同不变性的色彩空间</li><li>采用不同的相似性度量</li><li>通过改变起始区域，作者对比了一些初始化区域的方法，发现《Efficient Graph-Based Image Segmentation》中的方法效果最好</li></ol><p>主要参考：<a href="https://zhuanlan.zhihu.com/p/39927488" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39927488</a></p><h4 id="卷积神经网络产生定长的特征向量"><a href="#卷积神经网络产生定长的特征向量" class="headerlink" title="卷积神经网络产生定长的特征向量"></a>卷积神经网络产生定长的特征向量</h4><ol><li>将第一步中产生的类别无关的候选框risize成227x227大小的图片</li></ol><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1ji8aeorsj30ng0lzk7w.jpg" alt=""></p><p>上图中展示了作者在附录A中实验的几种resize方式：</p><p>(B)考虑context(图像中context指RoI周边像素)的各向同性变形，检测框向周围像素扩充到227×227，若遇到图像边界则用候选框像素的均值进行填充</p><p>(C)不考虑context的各向同性变形，直接用建议框像素均值填充至227×227</p><p>(D)各向异性变形，简单粗暴对图像resize至227×227</p><p>变形前先进行边界像素填充padding处理，即向外扩展建议框边界，以上三种方法中第一行为采用padding=0，第二行为采用padding=16</p><blockquote><p>作者采用的方法即为上图中(D)那一列的下面那一行的方式，在候选框周围加上16的padding(即向外扩展候选框边界)，再进行各向异性缩放，作者对比了各种方法后，发现这种方法最好，使得mAp提高了3到5个百分点</p></blockquote><ol start="2"><li>将1中resize以后的候选框图片使用AlexNet中五个卷积层和两个全连接层进行前向传播，最终得到一个4096x1维的特征向量</li></ol><h4 id="指定类别的SVM线性分类器"><a href="#指定类别的SVM线性分类器" class="headerlink" title="指定类别的SVM线性分类器"></a>指定类别的SVM线性分类器</h4><p>通过对每个类别训练出来的SVM对以上每个候选框产生的4096维特征向量进行打分，然后给出一张图像中所有的打分区域，然后使用NMS（每个类别是独立进行的），拒绝掉一些和高分区域的IOU大于阈值的候选框。</p><h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><h4 id="CNN的训练"><a href="#CNN的训练" class="headerlink" title="CNN的训练"></a>CNN的训练</h4><p>由于目标检测的数据集标注数据较少，训练AlexNet这样的大型网络较为困难，于是产生了上面提到的第二个问题：</p><blockquote><p>training a high-capacity model with only a small quantity of annotated detection data</p></blockquote><ol><li>利用ImageNet数据集进行预训练</li><li>将ImageNet专用的1000-way分类层，换成了一个随机初始化的21-way分类层（其中20是VOC的类别数，1代表背景）而卷积部分都没有改变</li><li>将PASCAL VOC数据集利用Selective Search方法产生候选框，利用所有候选框区域的图片作为训练数据，其中和真实标注的框的IoU&gt;=0.5就认为是正例，否则就是负例</li><li>利用3中的数据进行finetune，SGD开始的learning_rate为0.001（是初始化预训练时的十分之一），这使得调优得以有效进行而不会破坏初始化的成果。每轮SGD迭代，统一使用32个正例窗口（跨所有类别）和96个背景窗口，即每个mini-batch的大小是128。另外倾向于采样正例窗口，因为和背景相比他们很稀少。</li></ol><h4 id="SVM分类器的训练"><a href="#SVM分类器的训练" class="headerlink" title="SVM分类器的训练"></a>SVM分类器的训练</h4><p>与上面CNN的finetune过程一样，同样采用步骤3中的方法来产生训练SVM的数据集，但是其中IoU低于0.3的被作为负例，GT作为正例，其余的全部丢弃。然后将这些候选框区域利用上面训练好的CNN提取特征，，对应标签数据，来训练SVM。但是由于这里训练数据太大，难以装进内存，所以R-CNN采用hard negative mining方法，hard negative mining方法使得训练能够很快的收敛，并且mAP在一个epoch之后就停止增长。</p><blockquote><p>hard negative mining：难负例挖掘算法，用途就是解决正负例数量不均衡，而负例分散代表性又不够的问题，用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。</p></blockquote><blockquote><p>这里存在一个问题：当数据量过大的时候，并且其中大部分是负例的时候，没有办法将所有的数据全部装入内存进行训练，所以作者采用难负例来作为负例给SVM进行训练，但是作者并没有说是怎样产生这些难负例的。个人认为，这里的难负例应该是训练CNN的时候产生的，因为SVM作为一个接在CNN之后的分类器，实际上充当的是CNN最后softmax分类的作用，由于其接受的是CNN提取到的特征，而由于CNN的训练样本没有非常清晰的分类边界(下一小节”CNN和SVM训练中的问题”中会讲到)，所以肯定会产生很多的false positive例子，而将这些例子进一步给SVM训练，便可以有效的增强SVM对于分类边界的判断。所以SVM的训练样本中的负例一方面来自CNN产生的难分负例，另一方面来自将候选框区域和某一单一类别的真实标注框进行对比，IoU阈值小于0.3的负例</p></blockquote><h4 id="CNN和SVM训练中的问题"><a href="#CNN和SVM训练中的问题" class="headerlink" title="CNN和SVM训练中的问题"></a>CNN和SVM训练中的问题</h4><p>一、 为什么使用不同的正负例IoU阈值</p><p>首先，两者的不同在于</p><ul><li>在CNN的训练中，是将候选框区域和真实标注的框的IoU&gt;=0.5就认为是正例，否则就是负例，其中候选框区域是针对所有类别的，因此每次匹配的时候只要和任意一个真是标注框的IoU&gt;=0.5就认为是正例</li><li>在SVM的训练中，是将候选框区域和某一单一类别的真实标注框进行对比，当IoU阈值小于0.3的时候为负例，而正例为真实标注框</li></ul><p>而产生两种不同策略的原因在于：</p><blockquote><p>CNN的模型复杂度远远高于SVM，在训练CNN的时候采用上面的策略会使得正例的数目是真正正例(即GT)的数目的30倍，这样一方面可以避免CNN过拟合，另一方面可以为SVM的训练抽取到足够有表现力的特征(因为如果采用SVM那种数据处理方式，会使得CNN得到的数据样本很少)</p></blockquote><blockquote><p>由于SVM的模型是采用上面训练好的CNN模型输出的4096-D的特征来进行训练，因此SVM不需要采用过多的样本来抽取特征；同时SVM模型的复杂度很低，为了获取到足够大的类间距，所以采用上面训练SVM的策略，这样可以让SVM更容易区分正例和负例的区别</p></blockquote><p>二、为什么不采用CNN后面接SoftMax直接进行分类，而是要在后面使用SVM分类</p><blockquote><p>在上面的训练策略中，为了CNN抽取到训练样本中足够丰富的特征，采用上面介绍的CNN训练数据的生成方法，引入了大量“jittered”样本(与真实标注框的IoU阈值在0.5到1之间的候选框样本)极大的扩充了正例，避免了CNN过拟合，但是由于“jittered”样本与真实标注框之间存在很大的偏差，CNN很难有效的区分正例和负例之间的区别，因此CNN网络在预测精确的定位位置上表现不佳</p></blockquote><blockquote><p>CNN的训练中采用的是随机选取正负例，保证两者之间的比例为1:3，而没有采用SVM中的hard negative mining方法，同样导致直接使用CNN进行分类的效果没有使用功能SVM的好</p></blockquote><h4 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h4><p>作者通过在上面提出的模型的基础上分别用Pool5、fc6以及fc7产生的特征来训练SVM，并使用SVM进行分类后的mAP指标如下：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g1jxef79pzj30ux04zabo.jpg" alt=""></p><ul><li>前三行数据说明调优之前的模型中fc7的特征泛化能力不如fc6的特征，同时移除fc6和fc7，仅仅使用pool5的特征，只使用CNN参数的6%也能有非常好的结果。可见CNN的主要表达力来自于卷积层，而不是全连接层</li><li>中间三行数据说明调优之后的模型提升非常明显，mAP提升了8个百分点，达到了54.2%。fc6和fc7的提升明显优于pool5，这说明pool5从ImageNet学习的特征通用性很强，在它之上层的大部分提升主要是在学习领域相关的非线性分类器(上面是文中的观点，但是个人觉得这个数据实际上在一定程度上反驳了上边关于全连接层对于模型的特征表达没有作用的观点)</li></ul><h4 id="检测错误分析"><a href="#检测错误分析" class="headerlink" title="检测错误分析"></a>检测错误分析</h4><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g1jxlzt7vaj30f20lrq88.jpg" alt=""></p><p>(这里没有看的太明白)</p><h4 id="Bounding-box回归"><a href="#Bounding-box回归" class="headerlink" title="Bounding-box回归"></a>Bounding-box回归</h4><p>通过上面的检测错误分析，作者指出：</p><blockquote><p>As an immediate consequence of this analysis, we demonstrate that a simple bounding-box regression method signiﬁcantly reduces mislocalizations, which are the dominant error mode.</p></blockquote><p>从而在SVM对每一个候选框进行分类后，针对预测有类别的候选框进行Bounding-box回归。详细参考：<a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="noopener">https://blog.csdn.net/zijin0802034/article/details/77685438</a><br>(个人觉得这篇博客很赞，详细的解释了此处回归的loss函数的设计原理)</p><h3 id="自己的一些问题"><a href="#自己的一些问题" class="headerlink" title="自己的一些问题"></a>自己的一些问题</h3><ul><li>R-CNN中产生候选框位置的算法是Selective Search方法，该方法还是在利用传统特征的思想来提取后面需要检测的位置，但是最后又使用了深卷积网络提取到的特征做Bounding-box回归来进行校正</li><li>在产生正负例样本的时候采用的是1:3的比例，感觉后的很多方法都沿用了这一比例</li></ul><h2 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h2><blockquote><p>《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</p></blockquote><h3 id="提出问题以及产生问题的原因"><a href="#提出问题以及产生问题的原因" class="headerlink" title="提出问题以及产生问题的原因"></a>提出问题以及产生问题的原因</h3><p>流行的CNN结构通常都需要输入图像的尺寸是固定的，这限制了输入图像的长宽比和缩放尺度。当遇到任意尺寸的图像时，都是先将图像利用裁剪和变形缩放的方式来适应成固定尺寸。但裁剪会导致信息的丢失，变形会导致位置信息的扭曲，就会影响识别的精度。另外，一个预先定义好的尺寸在物体是缩放可变的时候就不适用了。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g1m1yi7fl9j30sd0gjgur.jpg" alt=""></p><p>产生问题的原因主要来自网络的全连接层，也即是网络的最后阶段。</p><h3 id="解决问题的方案"><a href="#解决问题的方案" class="headerlink" title="解决问题的方案"></a>解决问题的方案</h3><p>解决上面CNN输入固定问题的解决方案当然是本文的主角SPP了，即空间金字塔池化，论文首先说明了该结构的来历以及优点</p><h4 id="SPP的由来"><a href="#SPP的由来" class="headerlink" title="SPP的由来"></a>SPP的由来</h4><p>SPP来源于SPM(Spatial Pyramid Matching)，是论文《Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories》提出的观点，出现的背景是将BOVW(Bag of visual words)模型被大量地用在了Image representation中，但是BOVW模型完全缺失了特征点的位置信息。<br>主要参考：<a href="https://blog.csdn.net/jwh_bupt/article/details/9625469" target="_blank" rel="noopener">https://blog.csdn.net/jwh_bupt/article/details/9625469</a></p><h3 id="SPP介绍"><a href="#SPP介绍" class="headerlink" title="SPP介绍"></a>SPP介绍</h3><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g1m3jopoybj31pk0lx1kx.jpg" alt=""></p><p>通过上图中将经典的7层网络结构(如AlexNet或VGGNet)中conv5层产生的特征图进行可视化可以发现，CNN中的滤波器会被一些特定的语义激活，而这些滤波器也会在图像中采集相应的语义信息。这项特性说明CNN也和传统方法一样使用以下步骤：</p><ol><li>将原始图像进行编码(即SIFT特征或者HOG特征表示)后形成特征图(可以看成是一个特征空间)</li><li>将特征图根据不同尺度提取不同大小的图像块(bins)，这一个个图像块便可以看成是特征空间中的特征向量</li><li>将2中的特征向量集合利用词袋模型或者空间金字塔进行池化</li></ol><p>而CNN作为一种有效的特征提取器便可以替换1中的SIFT特征或者HOG特征来完成特征提取的任务。通过以上的这种方式SPP-Net想要完成的目标是：</p><blockquote><p>卷积层接受任意大小的输入，而分类器固定输出向量的尺寸</p></blockquote><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g1m4l3enlgj30t00oidkc.jpg" alt=""></p><p>如图所示，SPP结构即针对任意大小的特征图，首先将特征图分为4x4个图像块，然后对每一个图像块取最大值，于是便可以得到一个4x4的特征图(即对每一个图像块做最大池化操作)，即图中spatial pyramid pooling layer最左边的16x256-d特征，然后将特征图分为4x4个图像块，然后对每一个图像块取最大值，于是便可以得到一个2x2的特征图，即图中spatial pyramid pooling layer中间的4x256-d特征，最后将特征图整体最大值，于是便可以得到一个1x1的特征图，即图中spatial pyramid pooling layer最右边的256-d特征，将以上的各个特征进行拼接，作为全连接层的输入，通过这种方式，不论输入的大小是多少，输出向量的尺寸都是(16+4+1)x256-d，其中256-d对应着Conv 5输出的特征图即输入SPP结构的特征图的数量</p><p>这种处理方式具有以下的优点：</p><blockquote><p>SPP is able to generate a ﬁxed-length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot</p></blockquote><blockquote><p>这种方法由于不是像传统的词袋模型那样，直接将图像中所有的图像块作为特征向量放入词袋模型中，所以不会破坏特征图的空间信息</p></blockquote><blockquote><p>SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size. Multi-level pooling has been shown to be robust to object deformations</p></blockquote><blockquote><p>SPP can pool features extracted at variable scales thanks to the ﬂexibility of input scales.</p></blockquote><blockquote><p>SPP-net not only makes it possible to generate representations from arbitrarily sized images/windows for testing, but also allows us to feed images with varying sizes or scales during training. Training with variable-size images increases scale-invariance and reduces over-ﬁtting.</p></blockquote><blockquote><p>在检测中，SPP-Net不用像R-CNN那样对每张图片中的上千个变形后的区域的像素反复调用CNN，只需要在整张图片上运行一次卷积网络层（不关心窗口的数量），然后再使用SPP-net在特征图上抽取特征，这里用一张图可以形象的说明：<br><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g1m51iiou3j30o40c4n2h.jpg" alt=""></p></blockquote><h3 id="SPP-Net的训练"><a href="#SPP-Net的训练" class="headerlink" title="SPP-Net的训练"></a>SPP-Net的训练</h3><p>由于GPU的实现(如cuda-convnet和Caffe)更适合运行在固定输入图像上，因此SPP-Net采用下面两种训练方式：</p><h4 id="单一尺寸训练"><a href="#单一尺寸训练" class="headerlink" title="单一尺寸训练"></a>单一尺寸训练</h4><p>首先考虑接收裁剪成224×224图像的网络。裁剪的目的是数据增强。对于一个给定尺寸的图像，我们先计算空间金字塔池化所需要的块（bins）的大小。试想一个尺寸是axa（也就是13×13）的conv5之后特征图。对于nxn块的金字塔级，我们实现一个滑窗池化过程，窗口大小为win = 上取整[a/n]，步幅str = 下取整[a/n]. 对于l层金字塔，我们实现l个这样的层。然后将l个层的输出进行连接输出给全连接层。</p><h4 id="多尺寸训练"><a href="#多尺寸训练" class="headerlink" title="多尺寸训练"></a>多尺寸训练</h4><p>携带SPP的网络可以应用于任意尺寸，为了解决不同图像尺寸的训练问题，我们考虑一些预设好的尺寸。现在考虑这两个尺寸：180×180,224×224。我们使用缩放而不是裁剪，将前述的224的区域图像变成180大小。这样，不同尺度的区域仅仅是分辨率上的不同，而不是内容和布局上的不同。对于接受180输入的网络，我们实现另一个固定尺寸的网络。本例中，conv5输出的特征图尺寸是axa=10×10。我们仍然使用win = 上取整[a/n]，str = 下取整[a/n]，实现每个金字塔池化层。这个180网络的空间金字塔层的输出的大小就和224网络的一样了。<br>这样，这个180网络就和224网络拥有一样的参数了。换句话说，训练过程中，我们通过使用共享参数的两个固定尺寸的网络实现了不同输入尺寸的SPP-net。<br>为了降低从一个网络（比如224）向另一个网络（比如180）切换的开销，我们在每个网络上训练一个完整的epoch，然后在下一个完成的epoch再切换到另一个网络（权重保留）。依此往复。实验中我们发现多尺寸训练的收敛速度和单尺寸差不多。</p><h3 id="SPP-Net用于物体检测"><a href="#SPP-Net用于物体检测" class="headerlink" title="SPP-Net用于物体检测"></a>SPP-Net用于物体检测</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g1maoa60lrj30ug0sgq8p.jpg" alt=""></p><p>将SPP-Net应用于检测的步骤如下：</p><ol><li>使用主干网络提取特征，取conv5产生的特征图</li><li>根据Selective Search方法在原始图像中选取到的候选框的位置映射到conv5产生的特征图中，利用该区域的特征图使用SPP产生定长的特征</li><li>SVM分类器根据上面的特征进行分类</li><li>利用Bounding Box回归收紧检测框 </li></ol><h3 id="SPP-Net的测试结果总结"><a href="#SPP-Net的测试结果总结" class="headerlink" title="SPP-Net的测试结果总结"></a>SPP-Net的测试结果总结</h3><ol><li>多尺度训练的效果比单尺度训练的好</li><li>采用整幅图进行训练的效果比只采用图像正中间裁切后的图像块的效果好</li><li>给定两个模型，我们首先使用每个模型对测试图像的候选框进行打分。然后对并联的两个候选框集合上应用最大化抑制。一个方法比较置信的窗口就会压制另一个方法不太置信的窗口。通过这样的结合，mAP得到有效的提升，这意味着双模型是互补的。并且这种互补性主要是因为卷积层，结合卷积模型完全相同的两个模型，则没有任何效果。</li></ol><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><blockquote><p>《Fast R-CNN》</p></blockquote><p>作为改进之前方法的论文，第一步，当然是水一下之前的论文了：<br>这里主要针对R-CNN及其改进版本SPP-Net：</p><h3 id="R-CNN和SPP-Net的缺点以及Fast-R-CNN的贡献"><a href="#R-CNN和SPP-Net的缺点以及Fast-R-CNN的贡献" class="headerlink" title="R-CNN和SPP-Net的缺点以及Fast R-CNN的贡献"></a>R-CNN和SPP-Net的缺点以及Fast R-CNN的贡献</h3><ul><li><p><strong>R-CNN缺点</strong>：</p><ol><li>训练过程是多级pipline：即先训练CNN网络，然后根据CNN生成的特征训练SVM分类器</li><li>训练时空间和时间开销大：R-CNN训练时是将每一张图片中的所有RoI区域作为输入网络的图片，而同一张图片中的RoI区域之间存在着很大程度上的重叠，因此对每一个RoI区域进行网络传播会造成大量的冗余计算，同时由于SVM分类器的训练需要CNN提取的特征，因此前面的冗余计算同样会导致存储这些CNN产生的特征会造成空间上的冗余</li><li>测试速度慢：对于每一个候选框都进行一次CNN的正向传播，因此目标检测速度很慢</li></ol></li><li><p><strong>SPP-Net的缺点</strong></p><ol><li>训练过程是多级pipline</li><li>提取特征使用共享计算，但是仍需要将特征文件写入内存，供后面分类器的训练</li></ol></li><li><p><strong>Fast R-CNN的贡献</strong></p><ol><li>训练是使用多任务损失的单阶段训练，可以更新所有网络层参数</li><li>训练时将一站个图片及其对应的RoI区域送入网络中，利用RoI pooling层实现特征的共享，大大的减少了R-CNN中训练时空间和时间上的冗余</li><li>不需要磁盘空间缓存特征，来提供给分类器的训练</li><li>比R-CNN和SPPnet具有更高的目标检测精度</li></ol></li></ul><h3 id="Fast-R-CNN的结构"><a href="#Fast-R-CNN的结构" class="headerlink" title="Fast R-CNN的结构"></a>Fast R-CNN的结构</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g1m9p4cel4j30ru0juals.jpg" alt=""></p><p>网络的整体处理步骤为：<br>Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出K个类别加上1个背景类别的Softmax概率估计，另一个为K个类别的每一个类别输出四个实数值。每组4个值表示K个类别的一个类别的检测框位置的修正。</p><p>其中RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×W（例如，7×7）的固定空间范围的小特征图，其中H和W是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)及其高度和宽度(h,w)的四元组(r,c,h,w)定义。RoI最大池化通过将大小为h×w的RoI窗口分割成H×W个网格，子窗口大小约为h/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。</p><p>这里采用博客<a href="https://blog.csdn.net/u014380165/article/details/72851319" target="_blank" rel="noopener">Fast RCNN算法详解</a>中的两幅图来说明Fast R-CNN在上面网络结构下的训练和测试过程：<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1g214ve8jg1j30oq09odlw.jpg" alt=""><br>下面就具体根据训练和测试两个过程来分别说明Fast R-CNN算法的详细情况</p><h3 id="Fast-R-CNN的训练"><a href="#Fast-R-CNN的训练" class="headerlink" title="Fast R-CNN的训练"></a>Fast R-CNN的训练</h3><h4 id="从预训练网络初始化"><a href="#从预训练网络初始化" class="headerlink" title="从预训练网络初始化"></a>从预训练网络初始化</h4><p>Fast R-CNN采用了三个ImageNet预训练的网络：AlexNet，VGG_CNN_M_1024以及VGG16，对这些网络进行下面的结构替换：</p><ol><li><p>将网络的输入改为：图像的列表和这些图像中RoI的列表</p></li><li><p>将最后的池化层替换成Rol池化，假设预训练网络的输入图片大小为h和w，池化层之后的全连接层输出为：$batchsize\times c\times 1\times 1$，那么Rol池化层中H和W两个超参数设置为(这里假设网络中的特征图长和宽相等)：<br>$$H=h/\sqrt{c}, W=w/\sqrt{c}$$</p></li><li><p>将网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1个类别的Softmax以及类别特定的检测框回归）</p></li></ol><h4 id="训练配置"><a href="#训练配置" class="headerlink" title="训练配置"></a>训练配置</h4><p><strong>训练数据采样方式</strong></p><p>不同于R-CNN和SPPNet将图像中每一个RoI区域作为输入来进行反向传播训练，Fast R-CNN采用一种更有效的训练方式：<br>在Fast R-CNN网络训练中，随机梯度下降的小批量是被分层采样的，首先采样N个图像，然后从每个图像采样R/N个RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小N，就减少了小批量的计算。例如，当N=2和R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。在微调期间，每个SGD的小批量由N=2个图像构成，均匀地随机选择。我们使用大小为R=128的小批量，从每个图像采样64个RoI。并且这N张图片以0.5的概率水平翻转，其中正例和负例如下表选择方式：</p><table><thead><tr><th>类别</th><th>比例</th><th>方式</th></tr></thead><tbody><tr><td>正例</td><td>25%</td><td>与GT bounding box的IoU在[0.5,1)</td></tr><tr><td>负例</td><td>75%</td><td>与GT bounding box的IoU在[0.1,0.5)</td></tr></tbody></table><p>从训练配置中训练数据采样方式的表中可以看出，Fast R-CNN中训练CNN还是采用R-CNN中1:3的正例和负例比例以及训练CNN中区分正例和负例的IoU阈值，因此这里实际上还是引入了R-CNN中讨论的一个问题，采用上面这种IoU值来区分正例和负例会引入“jittered example”，即大量的正例与GT的bounding box之间还是存在一些偏差，不是精确的定位位置，导致CNN学到的分类边界不明显，可是Fast R-CNN采用这种方式来进行训练却比R-CNN和SPPNet中的效果好很多，里面可能存在的原因是什么？论文中给出的答案是：</p><blockquote><p>“one-shot” ﬁne-tuning is sufﬁcient compared to previous multi-stage training approaches. We note that softmax, unlike one-vs-rest SVMs, introduces competition between classes when scoring a RoI.</p></blockquote><blockquote><p>上面的解释主要说明了直接端到端训练的有效性，并且由于softmax分类器不同于SVM是直接对21个类别进行分类，引入了不同类别之间的竞争(为什么引入不同类别之间的竞争会对最后的分类结果有帮助？)。</p></blockquote><p><strong>多任务损失</strong></p><p>Fast R-CNN中将分类和回归损失合并为一个损失函数：<br>$$L(p,u,t^u,v)=L_{cls}(p,u)+\lambda[u\ge1]L_{loc}(t^u,v)$$<br>其中分类损失函数如下，即为softmax预测到的第$u$类物体的概率值，负号为了使该概率值越大，$L_{cls}$值越小<br>$$L_{cls}(p,u)=-log(p_u)$$<br>回归损失函数如下，其中$(t_i^u-v_i)$部分主要参考R-CNN中的检测框回归，主要的不同在于Fast R-CNN中采用的是Smooth L1损失，而R-CNN中采用的是L2损失：<br>$$L_{loc}(t^u,v)=\sum_{i\in{x,y,w,h}}smooth_{L_1}(t_i^u-v_i)$$</p><p>$$smooth_{L_{1}}(x)=<br>\begin{cases}<br>0.5x^{2} &amp; |x|\leq 1 \cr<br>|x|-0.5 &amp; otherwise<br>\end{cases}$$</p><p>Smooth L1损失相较于L1 loss和L2 loss而言具有以下优点：<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1g21f4m2w5xj30ii0x07ab.jpg" alt=""></p><p>个人觉得这里采用Smooth L1损失主要是因为：根据上面关于R-CNN中loss函数的分析博客<a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="noopener">https://blog.csdn.net/zijin0802034/article/details/77685438</a>可知，由于R-CNN只挑选出IoU&gt;0.6的检测框来训练回归模型，所以可以控制预测的检测框与GT之间的差值较小，保证其为线性回归问题，但是在Fast R-CNN中由于采用多任务损失进行联合训练，并且从损失函数可以看到，是分类的结果决定了检测的结果(即只有当分类结果$u\ge1$时，检测部分的损失才会大于0)，因此如果分类出现了错误导致预测的检测框与实际的检测框相距很远时，使用L2损失很容易导致梯度爆炸，因此采用Smooth L1损失。同时由于采用Smooth L1损失可以避免预测的检测框与实际的检测框相距很远造成的梯度爆炸，便可以省略R-CNN检测框回归中的正则项</p><p><strong>RoI池化层的反向传播</strong>和<strong>SVD分解</strong>见博客<a href="https://blog.csdn.net/shenxiaolu1984/article/details/51036677" target="_blank" rel="noopener">Fast RCNN算法详解</a></p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ul><li>使用多尺度的图像金字塔，性能几乎没有提高</li><li>倍增训练数据，能够有2%-3%的准确度提升</li><li>网络直接输出各类概率(softmax)，比SVM分类器性能略好</li><li>更多候选窗不能提升性能</li></ul><h3 id="Fast-R-CNN测试"><a href="#Fast-R-CNN测试" class="headerlink" title="Fast R-CNN测试"></a>Fast R-CNN测试</h3><p>Fast RCNN测试中region proposal的提取仍然使用selective search，其他过程与训练中前向传播的过程一致，同时目标检测时间大多消耗在这上面（提region proposal 2~3s，而提特征分类只需0.32s）</p><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;计算机视觉的三大任务&quot;&gt;&lt;a href=&quot;#计算机视觉的三大任务&quot; class=&quot;headerlink&quot; title=&quot;计算机视觉的三大任务&quot;&gt;&lt;/a&gt;计算机视觉的三大任务&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;分类（是什么）：给定一张图片，为每张图片打一个标签，说出图
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习算法" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="目标检测" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://camlinzhang.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="two-stage" scheme="http://camlinzhang.com/tags/two-stage/"/>
    
      <category term="OverFeat" scheme="http://camlinzhang.com/tags/OverFeat/"/>
    
      <category term="SPP-Net" scheme="http://camlinzhang.com/tags/SPP-Net/"/>
    
      <category term="R-CNN" scheme="http://camlinzhang.com/tags/R-CNN/"/>
    
      <category term="Fast-RCNN" scheme="http://camlinzhang.com/tags/Fast-RCNN/"/>
    
      <category term="Faster-RCNN" scheme="http://camlinzhang.com/tags/Faster-RCNN/"/>
    
  </entry>
  
  <entry>
    <title>Caffe转NCNN并移植Android配置记录</title>
    <link href="http://camlinzhang.com/2018/11/01/Caffe%E8%BD%ACNCNN%E5%B9%B6%E7%A7%BB%E6%A4%8DAndroid%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/"/>
    <id>http://camlinzhang.com/2018/11/01/Caffe转NCNN并移植Android配置记录/</id>
    <published>2018-11-01T03:48:53.000Z</published>
    <updated>2019-03-25T05:13:07.551Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实验目的："><a href="#实验目的：" class="headerlink" title="实验目的："></a>实验目的：</h2><p>将caffe模型转成ncnn可以实现在移动端运行深度学习模型，主要使用：<br><a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">https://github.com/Tencent/ncnn</a></p><h2 id="实验环境："><a href="#实验环境：" class="headerlink" title="实验环境："></a>实验环境：</h2><h3 id="1、系统环境"><a href="#1、系统环境" class="headerlink" title="1、系统环境"></a>1、系统环境</h3><ul><li>Mac OS Mojave系统</li><li>编译好的caffe源码（可以参考我之前的博客：<a href="https://blog.csdn.net/sinat_28731575/article/details/78958348）" target="_blank" rel="noopener">https://blog.csdn.net/sinat_28731575/article/details/78958348）</a><h3 id="2、软件"><a href="#2、软件" class="headerlink" title="2、软件"></a>2、软件</h3></li><li>Android Studio 3.2</li><li>Genymotion虚拟机<br>（参考：<a href="http://www.open-open.com/lib/view/open1466430392743.html）" target="_blank" rel="noopener">http://www.open-open.com/lib/view/open1466430392743.html）</a></li></ul><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><h3 id="1、实验准备"><a href="#1、实验准备" class="headerlink" title="1、实验准备"></a>1、实验准备</h3><p>1、将 <a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">https://github.com/Tencent/ncnn</a> clone到本地后解压，可以看到下面的组织结构：<br><img src="https://img-blog.csdnimg.cn/20181031214109797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中</p><ul><li>examples是简单的在安卓上使用NCNN的例子，有一个根据这个例子编译好的Android Studio工程： <a href="https://github.com/dangbo/ncnn-mobile" target="_blank" rel="noopener">https://github.com/dangbo/ncnn-mobile</a></li><li>tools是后面需要用到的一些工具代码，包含了将各种网络转换到NCNN的代码</li></ul><p>2、编译好的caffe源码用于后面转换模型使用</p><h3 id="2、编译NCNN"><a href="#2、编译NCNN" class="headerlink" title="2、编译NCNN"></a>2、编译NCNN</h3><p>（1）参照：<a href="https://github.com/Tencent/ncnn/wiki/how-to-build" target="_blank" rel="noopener">https://github.com/Tencent/ncnn/wiki/how-to-build</a><br>中选择一个需要的环境编译，因为我需要在Android上面使用，所以选择了“Build for Android”：<br>这里首先需要安装NDK来编译Android项目，配置NDK环境有以下两种方式：</p><ul><li><p>使用Android Studio来直接安装：<br><img src="https://img-blog.csdnimg.cn/2018103122011249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在偏好设置中进行如上图所示的配置，就可以配置NDK编译环境以及相关工具，安装好后NDK存放在上面的sdk目录下的ndk-bundle文件夹中</p></li><li><p>自己到网站上面下载的方式：<br>下载网址为：<a href="http://developer.android.com/ndk/downloads/index.html" target="_blank" rel="noopener">http://developer.android.com/ndk/downloads/index.html</a><br>选择合适的版本下载(因为上面的第一种方法虽然简单，但是默认下载最新的NDK，在编译的时候可能会出现后面我会讲到的一些问题，所以这种方式可以根据实际需要选择合适的版本)<br>解压上面下载的NDK压缩包<br>使用下面的命令配置环境变量：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"></span><br><span class="line"># 在.bash_profile文件的最后添加上（路径根据自己的进行修改）：</span><br><span class="line">export PATH=$PATH:/Users/camlin_z/Data/Project/AndroidStudioProjects/android-ndk-r10e</span><br><span class="line"># 或者想把环境变量添加成Android Studio配置的NDK的话：</span><br><span class="line">export ANDROID_SDK=&quot;/Users/camlin_z/Library/Android/sdk&quot;</span><br><span class="line">export ANDROID_NDK=&quot;/Users/camlin_z/Library/Android/sdk/ndk-bundle&quot;</span><br><span class="line">export PATH=&quot;$PATH:$ANDROID_SDK/tools:$ANDROID_SDK/platform-tools:$ANDROID_NDK&quot;</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>或者想要替换Android Studio中的NDK环境为自己下的版本的话将上面下载的NDK压缩包重命名为ndk-bundle后放到sdk目录下即可</p><p>（2）编译libncnn.a<br>根据上面ncnn的github下的教程有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cd &lt;ncnn-root-dir&gt;</span><br><span class="line">$ mkdir -p build-android-armv7</span><br><span class="line">$ cd build-android-armv7</span><br><span class="line">$ cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \</span><br><span class="line">    -DANDROID_ABI=&quot;armeabi-v7a&quot; -DANDROID_ARM_NEON=ON \</span><br><span class="line">    -DANDROID_PLATFORM=android-14 ..</span><br><span class="line">$ make -j4</span><br><span class="line">$ make install</span><br></pre></td></tr></table></figure><p>即可“build armv7 library”，之后便会在build-android-armv7/install/lib目录下生成libncnn.a，这样ncnn的编译工作就完成了</p><h3 id="3、使用NCNN将caffemodel转换成NCNN中需要的格式"><a href="#3、使用NCNN将caffemodel转换成NCNN中需要的格式" class="headerlink" title="3、使用NCNN将caffemodel转换成NCNN中需要的格式"></a>3、使用NCNN将caffemodel转换成NCNN中需要的格式</h3><p>参照上面ncnn的github下第二个教程：<br><a href="https://github.com/Tencent/ncnn/wiki/how-to-use-ncnn-with-alexnet" target="_blank" rel="noopener">https://github.com/Tencent/ncnn/wiki/how-to-use-ncnn-with-alexnet</a><br>首先是下载模型以及权重文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.prototxt</span><br><span class="line">deploy.prototxt</span><br><span class="line">snapshot_10000.caffemodel</span><br></pre></td></tr></table></figure><p>然后使用之前编译好的caffe中build/tools文件夹下的upgrade_net_proto_text和upgrade_net_proto_binary两个文件分别处理模型以及权重文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upgrade_net_proto_text [old prototxt] [new prototxt]</span><br><span class="line">upgrade_net_proto_binary [old caffemodel] [new caffemodel]</span><br></pre></td></tr></table></figure><p>同时要更改数据层的batchsize大小为1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Input&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 227 dim: 227 &#125; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>经过上面的步骤就准备好了需要转换的模型和权重文件。</p><p>接下来进入之前clone的 <a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">ncnn工程文件</a>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd tools/caffe</span><br><span class="line">mkdir build</span><br><span class="line">cmake ..</span><br><span class="line">make -j4</span><br></pre></td></tr></table></figure><p>就可以在build文件夹中生成caffe2ncnn.cpp对应的可执行文件caffe2ncnn，最后执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">caffe2ncnn deploy.prototxt bvlc_alexnet.caffemodel alexnet.param alexnet.bin</span><br></pre></td></tr></table></figure><p>就可以得到最后转化的权重以及模型文件：alexnet.param alexnet.bin</p><h3 id="4、编译jni生成了-so库文件"><a href="#4、编译jni生成了-so库文件" class="headerlink" title="4、编译jni生成了.so库文件"></a>4、编译jni生成了.so库文件</h3><p>进入刚刚ncnn工程下的examples中，这是一个用squeeze net作为例子来生成动态链接库的例子，可以看到examples下面有已经按照3中步骤生成好的squeeze net对应的权重和模型文件，</p><p>进入的squeezencnn/jni文件夹中，可以看到如下文件架结构：<br><img src="https://img-blog.csdnimg.cn/2018110110422615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>其中的cpp和h就是我们需要编写的C++文件和头文件，其中包含以下几个部分：</p><ul><li>我们需要的C++功能函数以及对应的头文件</li><li>C++和java之间的jni接口函数，用于两者之间的信息互通</li></ul><p>然后在终端使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ndk-build</span><br></pre></td></tr></table></figure><p>命令就可以将上面的文件打包成一个 .so动态链接库供Android调用，可以参考：<br><a href="https://blog.csdn.net/CrazyMo_/article/details/52804896" target="_blank" rel="noopener">https://blog.csdn.net/CrazyMo_/article/details/52804896</a> 中的讲解，下面我以squeeze net这个例子简单说明一下安卓调用的过程：<br>首先是Android Studio工程中的结构为：<br><img src="https://img-blog.csdnimg.cn/20181101111713277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>实际上上图中的工程顺序也就是我们建立我们工程的顺序：</p><ul><li>按照上面3中的步骤转换的模型就放在assets目录下</li><li>然后我们除了MainActivity.java，就可以定义一个自己需要的函数接口类代码，比如这里的SqueezeNcnn.java，里面的内容为：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.tencent.squeezencnn;</span><br><span class="line"></span><br><span class="line">import android.graphics.Bitmap;</span><br><span class="line">import android.content.Context;</span><br><span class="line"></span><br><span class="line">public class SqueezeNcnn</span><br><span class="line">&#123;</span><br><span class="line">// 我们自己定义的类方法，用于实现我们自己的功能（这里可以看到是java）</span><br><span class="line">    public native boolean Init(byte[] param, byte[] bin, byte[] words);</span><br><span class="line"></span><br><span class="line">    public native String Detect(Bitmap bitmap);</span><br><span class="line"></span><br><span class="line">    static &#123;</span><br><span class="line">        System.loadLibrary(&quot;squeezencnn&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后可以参考：<a href="https://blog.csdn.net/createchance/article/details/53783490" target="_blank" rel="noopener">https://blog.csdn.net/createchance/article/details/53783490</a><br>来自动生成jni文件夹下的squeezenet_v1.1.id.h和squeezencnn_jni.cpp，然后在其中进一步编写我们需要实现的功能函数</p><ul><li>接着就是JNI代码了，这个部分实际上包含了实现功能的C/C++代码以及jni接口函数两部分，通过上面的生成，我们得到了squeezenet_v1.1.id.h和squeezencnn_jni.cpp，对应于上面SqueezeNcnn.java中的类方法，squeezencnn_jni.cpp中有对应的JNI接口函数：<br><img src="https://img-blog.csdnimg.cn/20181101113648484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（函数具体内容大家可以到ncnn工程中查看，这里为了说明方便隐去内容）<br>可以看到jni接口函数是在java类函数的前面加上了</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Java_com_tencent_squeezencnn_SqueezeNcnn_</span><br></pre></td></tr></table></figure><p>部分，将java的native方法转换成C函数声明的规则是这样的：Java_{package_and_classname}_{function_name}(JNI arguments)。包名中的点换成单下划线。需要说明的是生成函数中的两个参数：<br>JNIEnv *：这是一个指向JNI运行环境的指针，后面我们会看到，我们通过这个指针访问JNI函数<br> jobject：这里指代java中的this对象</p><p>而对于一些不是接口的功能函数，我们就可以使用C++或者C来编写，而不需要考虑jni</p><ul><li>最后就是将上面的代码编译成libsqueezencnn.so动态库<br>这里我们首先需要编写jni目录下的编译配置文件 Android.mk 和 Application.mk ，类似于C++编译中的CMakeLists.txt：</li></ul><p>Android. mk ：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">LOCAL_PATH := $(call my-dir)</span><br><span class="line"></span><br><span class="line"># change this folder path to yours</span><br><span class="line">NCNN_INSTALL_PATH := /Users/camlin_z/Data/Project/AndroidStudioProjects/ncnn-master/build-android-armv7/install</span><br><span class="line"></span><br><span class="line">include $(CLEAR_VARS)</span><br><span class="line">LOCAL_MODULE := ncnn</span><br><span class="line"># LOCAL_SRC_FILES := $(NCNN_INSTALL_PATH)/$(TARGET_ARCH_ABI)/libncnn.a</span><br><span class="line">LOCAL_SRC_FILES := $(NCNN_INSTALL_PATH)/lib/libncnn.a</span><br><span class="line">include $(PREBUILT_STATIC_LIBRARY)</span><br><span class="line"></span><br><span class="line">include $(CLEAR_VARS)</span><br><span class="line"></span><br><span class="line">LOCAL_MODULE := squeezencnn</span><br><span class="line">LOCAL_SRC_FILES := squeezencnn_jni.cpp</span><br><span class="line"></span><br><span class="line">LOCAL_C_INCLUDES := $(NCNN_INSTALL_PATH)/include</span><br><span class="line"></span><br><span class="line">LOCAL_STATIC_LIBRARIES := ncnn</span><br><span class="line"></span><br><span class="line">LOCAL_CFLAGS := -O2 -fvisibility=hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math</span><br><span class="line">LOCAL_CPPFLAGS := -O2 -fvisibility=hidden -fvisibility-inlines-hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math</span><br><span class="line">LOCAL_LDFLAGS += -Wl,--gc-sections</span><br><span class="line"></span><br><span class="line">LOCAL_CFLAGS += -fopenmp</span><br><span class="line">LOCAL_CPPFLAGS += -fopenmp</span><br><span class="line">LOCAL_LDFLAGS += -fopenmp</span><br><span class="line"></span><br><span class="line">LOCAL_LDLIBS := -lz -llog -ljnigraphics</span><br><span class="line"></span><br><span class="line">include $(BUILD_SHARED_LIBRARY)</span><br></pre></td></tr></table></figure></p><p>具体里面的配置方法可以参考：<br><a href="http://www.cnblogs.com/wainiwann/p/3837936.html" target="_blank" rel="noopener">http://www.cnblogs.com/wainiwann/p/3837936.html</a></p><p>Application. mk：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># APP_STL := stlport_static</span><br><span class="line">APP_STL := gnustl_static</span><br><span class="line"># APP_ABI := armeabi armeabi-v7a</span><br><span class="line"></span><br><span class="line"># 注意此处哟啊对应你之前编译ncnn时的版本，比如我之前用的就是armeabi-v7a</span><br><span class="line"># 下面就要指定为armeabi-v7a，不能再有后面的arm64-v8a</span><br><span class="line">APP_ABI := armeabi-v7a #arm64-v8a</span><br><span class="line"></span><br><span class="line">APP_PLATFORM := android-14</span><br><span class="line"># NDK_TOOLCHAIN_VERSION := 4.9</span><br></pre></td></tr></table></figure><p>写好上面的各个配置文件之后就可以在终端进入jni文件夹输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ndk-build</span><br></pre></td></tr></table></figure><p>命令进行编译生成 libsqueezencnn. so动态链接库，经过了以上的所有步骤得到最后的动态链接库，Android中的函数就可以直接调用来实现对应的功能了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;实验目的：&quot;&gt;&lt;a href=&quot;#实验目的：&quot; class=&quot;headerlink&quot; title=&quot;实验目的：&quot;&gt;&lt;/a&gt;实验目的：&lt;/h2&gt;&lt;p&gt;将caffe模型转成ncnn可以实现在移动端运行深度学习模型，主要使用：&lt;br&gt;&lt;a href=&quot;https://g
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习环境" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    
      <category term="caffe学习" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/caffe%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Caffe" scheme="http://camlinzhang.com/tags/Caffe/"/>
    
      <category term="NCNN" scheme="http://camlinzhang.com/tags/NCNN/"/>
    
      <category term="Android" scheme="http://camlinzhang.com/tags/Android/"/>
    
  </entry>
  
  <entry>
    <title>caffe中solver配置文档</title>
    <link href="http://camlinzhang.com/2018/10/05/caffe%E4%B8%ADsolver%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3/"/>
    <id>http://camlinzhang.com/2018/10/05/caffe中solver配置文档/</id>
    <published>2018-10-05T09:21:25.000Z</published>
    <updated>2019-03-25T05:11:47.556Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要参考：<br><a href="https://www.cnblogs.com/denny402/p/5074049.html" target="_blank" rel="noopener">https://www.cnblogs.com/denny402/p/5074049.html</a> 以及caffe官方文档<br><strong>Solver的流程：</strong></p><ol><li>设计好需要优化的对象，以及用于学习的训练网络和用于评估的测试网络。（通过调用另外一个网络配置文件prototxt来进行）</li><li>通过forward和backward迭代的进行优化来跟新参数。</li><li>定期的评价测试网络。（可设定多少次训练后，进行一次测试）</li><li>在优化过程中显示模型和solver的状态</li></ol><p><strong>在每一次的迭代过程中，solver做了这几步工作：</strong></p><ol><li>调用forward算法来计算最终的输出值，以及对应的loss</li><li>调用backward算法来计算每层的梯度</li><li>根据选用的slover方法，利用梯度进行参数更新</li><li>记录并保存每次迭代的学习率、快照，以及对应的状态。</li></ol><p>下面通过从上到下的分类说明来看看使用各中优化算法以及学习率计算方法的solver.prototxt配置方法：</p><h3 id="1-设置深度网络模型"><a href="#1-设置深度网络模型" class="headerlink" title="1. 设置深度网络模型"></a>1. 设置深度网络模型</h3><ul><li>训练网络和测试网络是一个网络：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</span><br></pre></td></tr></table></figure><ul><li>训练和测试的网络不同，或者只需要训练的网络也可以只写train_net：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_net: &quot;examples/mnist/lenet_train.prototxt&quot;</span><br><span class="line">test_net: &quot;examples/mnist/lenet_test.prototxt&quot;</span><br></pre></td></tr></table></figure><h3 id="2-设置测试网络的测试间隔-只在需要测试的时候需要"><a href="#2-设置测试网络的测试间隔-只在需要测试的时候需要" class="headerlink" title="2. 设置测试网络的测试间隔(只在需要测试的时候需要)"></a>2. 设置测试网络的测试间隔(只在需要测试的时候需要)</h3><ul><li>设置测试完一次所有样本的迭代轮数test_iter：<br>$$test_iter = \frac{测试样本数量}{测试层batch_size大小}$$以及设置测试间隔test_interval(每训练多少次进行一次测试)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_iter: 100</span><br><span class="line">test_interval: 500</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-设置学习率计算策略"><a href="#3-设置学习率计算策略" class="headerlink" title="3. 设置学习率计算策略"></a>3. 设置学习率计算策略</h3><p>对于所有的学习率计算策略都需要设置基础学习率以及最大迭代次数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">base_lr: 1e-2</span><br><span class="line">max_iter: 20000</span><br></pre></td></tr></table></figure></p><ul><li><p><strong>fixed 学习率计算策略</strong><br>该学习率计算策略保持base_lr不变，所以不需要其他参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: &quot;fixed&quot;</span><br></pre></td></tr></table></figure></li><li><p><strong>step 学习率计算策略</strong><br>该学习率下更新base_lr的公式为：<br>$$当前学习率 = base_lr * gamma^{floor(\frac{iter}{stepsize})}$$其中iter为当前迭代次数，gamma, stepsize为需要设置的超参数</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: &quot;step&quot;</span><br><span class="line">gamma: 0.1# 学习速率变化因子</span><br><span class="line">stepsize: 1000# 每1000次迭代，降低学习速率</span><br></pre></td></tr></table></figure><ul><li><p><strong>multistep 学习率计算策略</strong><br>step是均匀等间隔变化，而multistep则是根据stepvalue值变化，当迭代次数到了其中一个stepvalue，那么就根据下面的公式进行计算，在到达下一个stepvalue之前，其学习率维持不变<br>$$当前学习率 = base_lr * gamma^{floor(\frac{iter}{stepvalue})}$$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: &quot;multistep&quot;</span><br><span class="line">gamma: 0.9</span><br><span class="line">stepvalue: 5000</span><br><span class="line">stepvalue: 7000</span><br><span class="line">stepvalue: 8000</span><br><span class="line">stepvalue: 9000</span><br><span class="line">stepvalue: 9500</span><br></pre></td></tr></table></figure></li><li><p><strong>poly 学习率计算策略</strong><br>该学习率计算策略是进行多项式误差，更新公式为：<br>$$当前学习率 = base_lr *(1 -  {(\frac{iter}{max_iter})}^{power})$$</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: &quot;poly&quot;</span><br><span class="line">power: 0.9</span><br></pre></td></tr></table></figure><h3 id="4-设置优化算法类型"><a href="#4-设置优化算法类型" class="headerlink" title="4.设置优化算法类型"></a>4.设置优化算法类型</h3><p>caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择，各种优化算法之间的关系可以参考下面的图片<br><img src="https://img-blog.csdn.net/20181005153617841?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p><ul><li><p><strong>SGD</strong><br>caffe中默认的SGD优化算法是SGDM即SGD with momentum，所以配置方法为，此时 type: “SGD” 可以省略：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">momentum: 0.9</span><br><span class="line">type: &quot;SGD&quot;</span><br></pre></td></tr></table></figure></li><li><p><strong>Nesterov</strong><br>Nesterov优化算法是在SGDM的基础上结合未来的梯度信息来进行更新，没有引入新的超参数</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">momentum: 0.9</span><br><span class="line">type: &quot;Nesterov&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>AdaGrad</strong><br>AdaGrad优化算法在SGD的基础上，根据自变量在每个维度上梯度值大小来调整各个维度上的学习率，只在二阶动量上进行累加，没有使用一阶动量，所以不需要超参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type: &quot;AdaGrad&quot;</span><br><span class="line">delta: 1e-6#建议为1e-6，默认值为1e-8</span><br></pre></td></tr></table></figure><ul><li><strong>RMSProp</strong><br>RMSProp优化方法在二阶动量上使用了EMA，caffe使用rms_decay来作为其超参数，而将momentum设为0.0</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">momentum: 0.0</span><br><span class="line">type: &quot;RMSProp&quot;</span><br><span class="line">rms_decay: 0.98</span><br><span class="line">delta: 1e-6#建议为1e-6，默认值为1e-8</span><br></pre></td></tr></table></figure><ul><li><strong>AdaDelta</strong><br>AdaDelta是针对AdaGrad后期学习率过小找不到最优解的问题，与RMSProp一样在二阶动量中引入EMA，但同时引入一个状态变量Δx来代替基础学习率，所以其可以不需要设置base_lr，但caffe中通常可以将其设置为1.0</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">base_lr: 1.0</span><br><span class="line">lr_policy: &quot;fixed&quot;</span><br><span class="line">momentum: 0.95</span><br><span class="line">delta: 1e-5#建议为1e-5，默认值为1e-8</span><br><span class="line">type: &quot;AdaDelta&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>Adam</strong><br>Adam优化方法在一阶动量以及二阶动量上均进行了EMA以及偏差修正，所以需要两个动量超参数值，并且由于Adam优化算法会自动调整学习率，所以我们设置基础学习率的调整策略为fixed：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 下面是Adam作者建议的两个动量超参数取值</span><br><span class="line">momentum: 0.9</span><br><span class="line">momentum2: 0.999</span><br><span class="line">lr_policy: &quot;fixed&quot;</span><br><span class="line">type: &quot;Adam&quot;</span><br><span class="line">delta = 1e-8 #默认值为1e-8</span><br></pre></td></tr></table></figure><h3 id="5-设置权重衰减值以及正则化类型"><a href="#5-设置权重衰减值以及正则化类型" class="headerlink" title="5. 设置权重衰减值以及正则化类型"></a>5. 设置权重衰减值以及正则化类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight_decay: 0.0005</span><br><span class="line">regularization_type: &quot;L2&quot;# 默认是&quot;L2&quot;，也可以设为&quot;L1&quot;</span><br></pre></td></tr></table></figure><h3 id="6-其他常用参数"><a href="#6-其他常用参数" class="headerlink" title="6. 其他常用参数"></a>6. 其他常用参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 每间隔多少轮迭代在终端打印loss结果</span><br><span class="line">display: 100</span><br><span class="line"># Display the loss averaged over the last average_loss iterations</span><br><span class="line">average_loss: 20</span><br><span class="line"># accumulate gradients over `iter_size` * `batch_size` instances</span><br><span class="line">iter_size = 64</span><br><span class="line"># 每间隔多少轮迭代保存中间模型结果，一个权重结果caffemodel和一个用于恢复训练状态的solverstate</span><br><span class="line">snapshot: 1000</span><br><span class="line"># snapshot保存路径，其中最后的voc2012是保存的模型名字的开头部分</span><br><span class="line">snapshot_prefix: &quot;evaluation/snapshot/voc2012&quot;</span><br><span class="line"># 在GPU还是CPU下面运行</span><br><span class="line">solver_mode: GPU# default = GPU</span><br></pre></td></tr></table></figure><h3 id="7-其他不常用参数"><a href="#7-其他不常用参数" class="headerlink" title="7.其他不常用参数"></a>7.其他不常用参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 下面这种写法表示在不同的网络上进行测试</span><br><span class="line">test_state: &#123; stage: &apos;test-on-train&apos; &#125;</span><br><span class="line">test_iter: 500</span><br><span class="line">test_state: &#123; stage: &apos;test-on-test&apos; &#125;</span><br><span class="line">test_iter: 100</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># test时是否计算loss，默认为false</span><br><span class="line">test_compute_loss: true</span><br><span class="line"></span><br><span class="line"># If true, run an initial test pass before the first iteration, ensuring</span><br><span class="line"># memory availability and printing the starting value of the loss.</span><br><span class="line">test_initialization: true</span><br><span class="line"></span><br><span class="line"># Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm,</span><br><span class="line"># whenever their actual L2 norm is larger.</span><br><span class="line">clip_gradients = -1# default = -1</span><br><span class="line"></span><br><span class="line"># whether to snapshot diff in the results or not. Snapshotting diff will help</span><br><span class="line"># debugging but the final protocol buffer size will be much larger.</span><br><span class="line">snapshot_diff: false # default = false</span><br><span class="line"></span><br><span class="line"># snapshot的保存</span><br><span class="line">snapshot_format: HDF5# default = BINARYPROTO</span><br><span class="line"></span><br><span class="line"># the device_id will that be used in GPU mode. Use device_id = 0 in default</span><br><span class="line">device_id: 0# default = 0</span><br><span class="line"></span><br><span class="line"># If non-negative, the seed with which the Solver will initialize the Caffe</span><br><span class="line"># random number generator -- useful for reproducible results. Otherwise,</span><br><span class="line"># (and by default) initialize using a seed derived from the system clock.</span><br><span class="line">random_seed: -1# default = -1</span><br><span class="line"></span><br><span class="line"># If true, print information about the state of the net that may help with</span><br><span class="line"># debugging learning problems.</span><br><span class="line">debug_info: true# default = false</span><br><span class="line"></span><br><span class="line"># If false, don&apos;t save a snapshot after training finishes.</span><br><span class="line">snapshot_after_train: true# default = true</span><br><span class="line"></span><br><span class="line"># Overlap compute and communication for data parallel training</span><br><span class="line">layer_wise_reduce: false# default = true</span><br><span class="line"></span><br><span class="line">// Path to caffemodel file(s) with pretrained weights to initialize finetuning.</span><br><span class="line">// Tha same as command line --weights parameter for caffe train command.</span><br><span class="line">// If command line --weights parameter is specified, it has higher priority</span><br><span class="line">// and overwrites this one(s).</span><br><span class="line">// If --snapshot command line parameter is specified, this one(s) are ignored.</span><br><span class="line">// If several model files are expected, they can be listed in a one</span><br><span class="line">// weights parameter separated by &apos;,&apos; (like in a command string) or</span><br><span class="line">// in repeated weights parameters separately.</span><br><span class="line">weights: &apos;,&apos;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要参考：&lt;br&gt;&lt;a href=&quot;https://www.cnblogs.com/denny402/p/5074049.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/denny402/p/50
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习环境" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    
      <category term="caffe学习" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/caffe%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Caffe" scheme="http://camlinzhang.com/tags/Caffe/"/>
    
      <category term="solver.prototxt" scheme="http://camlinzhang.com/tags/solver-prototxt/"/>
    
  </entry>
  
  <entry>
    <title>PSPNet运行及训练</title>
    <link href="http://camlinzhang.com/2018/09/25/PSPNet%E8%BF%90%E8%A1%8C%E5%8F%8A%E8%AE%AD%E7%BB%83/"/>
    <id>http://camlinzhang.com/2018/09/25/PSPNet运行及训练/</id>
    <published>2018-09-25T05:02:46.000Z</published>
    <updated>2019-03-25T05:09:37.777Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近实习完回学校，继续开始我的图像语义分割研究了，为毕业论文做准备，首先还是看了一下最近的一些最新的分割论文，在ICNet论文中找到一个很好的图表，显示各个方法在CItyscapes数据集的测试集上的时间和精度指标情况：</p><center><br><img src="https://img-blog.csdn.net/20180924091329249?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="80%" alt="图片还在路上，稍等..."><br></center><br>由上图可以看到精度上很高的方法有Resnet38，PSPNet，DUC三种方法，速度上较快的只有ICNet（考虑到精度因素）；除了上图中的方法之外，2018年最新的模型中deeplab v3+在这个测试集上精度达到了最好的82.1（时间没有考虑），所以本次主要实验的baseline使用PSPNet，下面就开始漫漫征程的第一步——跑通PSPNet<br><br># 配置环境<br>## 准备工作<br><br>深度学习的第一步当然是配置环境了，好的环境简直就是成功的一半了。不过这里就不详细介绍了，得益于出去实习的四个月我之前配置好环境的服务器还是完好的保存着没有人用，所以可以有一套完好的caffe训练环境，需要配置的童鞋可以参考我之前的：<a href="https://blog.csdn.net/sinat_28731575/article/details/78958348" target="_blank" rel="noopener">Unbuntu配置Caffe以及调试DeepLab记录</a><br><br>我自己配置的环境是Ubuntu 16.04+NVIDIA-Linux-x86_64-384.98驱动<br>+CUDA8.0+Anaconda2+cuDNN5.0.5+OpenCV2.4.13.4+caffe<br><br>在此环境下首先还需要下载的就是：<br><br> 1. <a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="noopener">PSPNet论文</a><br> 2. <a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener">PSPNet caffe源码</a><br> 3. MATLAB 2015b（链接:<a href="https://pan.baidu.com/s/10q16mB_62EZL_aVCdo545w" target="_blank" rel="noopener">https://pan.baidu.com/s/10q16mB_62EZL_aVCdo545w</a>  密码:1ggm），这里需要稍微注释一下，我本来下的2016b，但是看到<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener">caffe的官网</a>上：<img src="https://img-blog.csdn.net/20180924093503141?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br> 所以我还是猥琐的换回2015a算了，也是因为之前实习的时候配置用的2016b结果没有弄好(好尴尬，写这篇博客的时候才发现自己还是看错了，人家支持的是2015a，我下的是2015b，哎，这粗心的毛病啥时候能改的好啊。。。)，不过最终还是配置成功了，不放心的童鞋还是可以自己去下2015a。为什么需要下MATLAB是因为作者的评价代码是用的MATLAB，为了和作者保持一致，还是配置一下matcaffe吧<br><br>## 开始配置<br><br>1、首先我们来安装MATLAB2015b，我主要参考的是<br><a href="https://blog.csdn.net/hejunqing14/article/details/50265049" target="_blank" rel="noopener">https://blog.csdn.net/hejunqing14/article/details/50265049</a><br><br>2、安装好了之后就是非常艰难的源码编译过程了，主要可以参考：<br><a href="https://blog.csdn.net/WZZ18191171661/article/details/70149070" target="_blank" rel="noopener">https://blog.csdn.net/WZZ18191171661/article/details/70149070</a> ，首先还是修<br>改Makefile.config，我是直接把之前编译成功的caffe的Makefile.config复制到PSPNet源码里面进行编译的，刚开始问题就来了：<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In file included from ./include/caffe/util/device_alternate.hpp:40:0,</span><br><span class="line">                 from ./include/caffe/common.hpp:19,</span><br><span class="line">                 from src/caffe/common.cpp:7:</span><br><span class="line">./include/caffe/util/cudnn.hpp: In function ‘void caffe::cudnn::createPoolingDesc(cudnnPoolingStruct**, caffe::PoolingParameter_PoolMethod, cudnnPoolingMode_t*, int, int, int, int, int, int)’:</span><br><span class="line">./include/caffe/util/cudnn.hpp:127:41: error: too few arguments to function ‘cudnnStatus_t cudnnSetPooling2dDescriptor(cudnnPoolingDescriptor_t, cudnnPoolingMode_t, cudnnNanPropagation_t, int, int, int, int, int, int)’</span><br><span class="line">         pad_h, pad_w, stride_h, stride_w));</span><br><span class="line">                                         ^</span><br><span class="line">./include/caffe/util/cudnn.hpp:15:28: note: in definition of macro ‘CUDNN_CHECK’</span><br><span class="line">     cudnnStatus_t status = condition; \</span><br><span class="line">                            ^</span><br><span class="line">In file included from ./include/caffe/util/cudnn.hpp:5:0,</span><br><span class="line">                 from ./include/caffe/util/device_alternate.hpp:40,</span><br><span class="line">                 from ./include/caffe/common.hpp:19,</span><br><span class="line">                 from src/caffe/common.cpp:7:</span><br><span class="line">/usr/local/cuda-7.5//include/cudnn.h:803:27: note: declared here</span><br><span class="line"> cudnnStatus_t CUDNNWINAPI cudnnSetPooling2dDescriptor(</span><br><span class="line">                           ^</span><br><span class="line">make: *** [.build_release/src/caffe/common.o] Error 1</span><br></pre></td></tr></table></figure><br><br>这个问题是我之前编译成功的caffe的cuDNN版本和PSPNet的版本不一致导致的，参考<br><a href="https://blog.csdn.net/u011070171/article/details/52292680" target="_blank" rel="noopener">https://blog.csdn.net/u011070171/article/details/52292680</a> 中的方法顺利解决<br><br>3、在make runtest的时候我出现了下面的错误<br><br><br><br><br><br><br>网上一般的解答都是网络中feature map大小不一致造成的，可是我又不是在训练自己的网络的时候遇到的，我也很无奈，所以就放着了，毕竟这个也并没有影响到后面的测试以及训练，不过有大牛知道还是希望告知一下的。<br><br>4、然后就是编译MATLAB的caffe接口了，make matcaffe还好没有报错，但是make mattest的时候：<br><img src="https://img-blog.csdn.net/2018092410021144?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>主要参考：<a href="http://www.cnblogs.com/laiqun/p/6031925.html" target="_blank" rel="noopener">http://www.cnblogs.com/laiqun/p/6031925.html</a> 以及 <a href="http://caffe.berkeleyvision.org/tutorial/interfaces.html" target="_blank" rel="noopener">caffe官网</a>，但是我用<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ldd ./matlab/+caffe/private/caffe_.mexa64</span><br></pre></td></tr></table></figure><br><br>查询C++的动态链接库的时候，发现了两个没有链接到的运行时库，尝试了各种方法将运行时库链接上了，可是还是编译不成功，最后我把终端重启了一次之后就好了，我也很迷。。。<br><br>至此基本上就完成了所有的编译过程，可以进行到下面的步骤了。<br><br># PSPNet运行<br>## 数据集准备<br>PSPNet论文中用到的数据集为：<br><br> - <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" rel="noopener">ADE20K</a><br> - <a href="https://www.cityscapes-dataset.com/downloads/" target="_blank" rel="noopener">Cityscapes</a><br> - <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">PASCAL VOC2012</a>以及<a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz" target="_blank" rel="noopener">PASCAL VOC2012增强数据集</a><br><br>## 数据集处理<br>作者在使用这些数据之前都进行了一系列的预处理，下面就来介绍一下：<br><br><strong>1、ADE20K</strong><br>下载完成后解压数据集可以发现它分为training和validation两个部分，每个部分都由很多子文件夹组成，具体的图片以及标签为：<br><img src="https://img-blog.csdn.net/20180924102920582?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>官方对于以上的图片解释如下：<br><img src="https://img-blog.csdn.net/20180924103119235?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><br>作者对于以上的数据集主要使用的了原始图像以及分割的标记图像(上图中的ADE_val_00000001.jpg和ADE_val_00000001_seg.png)，同时作者对分割的标记图像进行了预处理，转化为代表类别的灰度图像，转化工具是ADE20K官方提供的<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/code.zip" target="_blank" rel="noopener">MATLAB转换代码</a>，对于代码的介绍可以参考官网：<br><img src="https://img-blog.csdn.net/2018092410391444?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>将demo.m进行修改后可以进行转换（我自己也没有写出来，有大牛写出来了希望@我一下，万分感谢），官网代码的github上也有对应的issues：<a href="https://github.com/hszhao/PSPNet/issues/76" target="_blank" rel="noopener">https://github.com/hszhao/PSPNet/issues/76</a><br><br><strong>2、Cityscapes</strong><br>作者使用的是下面这四个部分：<br><img src="https://img-blog.csdn.net/20180924104159539?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>对于这个数据集的介绍可以参考：<a href="https://blog.csdn.net/Cxiazaiyu/article/details/81866173" target="_blank" rel="noopener">https://blog.csdn.net/Cxiazaiyu/article/details/81866173</a><br><br>上面的数据集同样要进行预处理，首先按照上面博文的介绍运行cityscapesscripts/helpers/labels.py脚本可以看到下面的各个类别的情况：<br><img src="https://img-blog.csdn.net/20180924105303647?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>而论文中使用的类别情况也和这个默认的trainId是一致的，所以我们直接使用cityscapesscripts/preparation/createTrainIdLabelImgs.py脚本来对数据集进行转换即可，只需要将文件夹放成如下结构：<br><center><br><img src="https://img-blog.csdn.net/20180925101135380?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="60%" alt="图片还在路上，稍等..."><br></center><br><center><br><img src="https://img-blog.csdn.net/20180925101407569?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="30%" alt="图片还在路上，稍等..."><br></center><p>然后修改脚本中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    # Where to look for Cityscapes</span><br><span class="line">    if &apos;CITYSCAPES_DATASET&apos; in os.environ:</span><br><span class="line">        cityscapesPath = os.environ[&apos;CITYSCAPES_DATASET&apos;]</span><br><span class="line">    else:</span><br><span class="line">        cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),&apos;..&apos;,&apos;..&apos;)</span><br></pre></td></tr></table></figure><p>部分的路径即可将原始的标签图片转换到上表所示的类别情况中</p><p><strong>3、PSCAL VOC2012</strong><br>该数据集的处理主要参考：<a href="https://blog.csdn.net/Xmo_jiao/article/details/77897109" target="_blank" rel="noopener">https://blog.csdn.net/Xmo_jiao/article/details/77897109</a></p><h2 id="运行及测试"><a href="#运行及测试" class="headerlink" title="运行及测试"></a>运行及测试</h2><p>在PSPNet官方代码中evaluation文件夹下的.m文件以及evaluationCode文件夹下的文件即为整个测试的所有脚本，其中最主要需要注意的分别为，可以参考：<a href="https://blog.csdn.net/qq_21368481/article/details/81068514" target="_blank" rel="noopener">PSPNet测试代码解读</a>：</p><ul><li>eval_all.m：测试的主函数</li><li>eval_sub.m：运行caffemodel产生预测图片</li><li>eval_acc.m：将预测的图片和标签图片进行对比生成测试指标</li></ul><p>根据自己的实际情况修改脚本中的路径并按照代码将各个测试的图片放入相应的位置就可以进行测试了<br>主要修改的脚本为eval_all.m：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 修改路径部（这里以ADE20K为例）</span><br><span class="line">isVal = true; %evaluation on valset</span><br><span class="line">step = 2000; %equals to number of images divide num of GPUs in testing e.g. 500=2000/4</span><br><span class="line">data_root = &apos;/home/t7810/data/ADE20K&apos;; %root path of dataset</span><br><span class="line">eval_list = &apos;list/ADE20K_val.txt&apos;; %evaluation list, refer to lists in folder &apos;samplelist&apos;</span><br><span class="line">save_root = &apos;/home/t7810/data/ADE20K/mc_result/pspnet50_473/&apos;; %root path to store the result image</span><br><span class="line">model_weights = &apos;/home/t7810/project/PSPNet-master/evaluation/model/pspnet50_ADE20K.caffemodel&apos;;</span><br><span class="line">model_deploy = &apos;/home/t7810/project/PSPNet-master/evaluation/prototxt/pspnet50_ADE20K_473.prototxt&apos;;</span><br><span class="line">fea_cha = 150; %number of classes</span><br><span class="line">base_size = 512; %based size for scaling</span><br><span class="line">crop_size = 473; %crop size fed into network</span><br><span class="line">data_class = &apos;objectName150.mat&apos;; %class name</span><br><span class="line">data_colormap = &apos;color150.mat&apos;; %color map</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"># 修改GPU部分（因为我的电脑只有一个GPU）</span><br><span class="line">gpu_id_array = [0:3]; %multi-GPUs for parfor testing, if number of GPUs is changed, remember to change the variable &apos;step&apos;</span><br><span class="line">runID = 1;</span><br><span class="line">%gpu_num = size(gpu_id_array,2);</span><br><span class="line">gpu_num = 1</span><br><span class="line">index_array = [(runID-1)*gpu_num+1:runID*gpu_num];</span><br><span class="line"></span><br><span class="line">for i = 1:gpu_num %change &apos;parfor&apos; to &apos;for&apos; if singe GPU testing is used</span><br><span class="line">  eval_sub(data_name,data_root,eval_list,model_weights,model_deploy,fea_cha,base_size,crop_size,data_class,data_colormap, ...</span><br><span class="line">           is_save_feat,save_gray_folder,save_color_folder,save_feat_folder,gpu_id_array(i),index_array(i),step,skipsize,scale_array,mean_r,mean_g,mean_b);</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>测试图片文件夹结构为：</p><center><br><img src="https://img-blog.csdn.net/20180925101942448?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="60%" alt="图片还在路上，稍等..."><br></center><p>测试情况如下：<br>1、PASCAL VOC2012:</p><left><br><img src="https://img-blog.csdn.net/20180925130207533?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="40%" alt="图片还在路上，稍等..."><br></left><p>2、Cityscapes</p><left><br><img src="https://img-blog.csdn.net/20180925103456547?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="40%" alt="图片还在路上，稍等..."><br></left><p>3、ADE20K<br>由于上面的转换代码没有写出来，所以测试的结果很低（不能讲预测的类别标签和真实的标签进行匹配）</p><h1 id="PSPNet训练"><a href="#PSPNet训练" class="headerlink" title="PSPNet训练"></a>PSPNet训练</h1><p>本次训练首先使用PASCAL VOC数据集进行训练，该数据集对应的label标签以及数据上面的：<a href="https://blog.csdn.net/Xmo_jiao/article/details/77897109" target="_blank" rel="noopener">https://blog.csdn.net/Xmo_jiao/article/details/77897109</a> 都有对应的下载链接，所以主要涉及到的是配置文档：</p><h2 id="配置文档"><a href="#配置文档" class="headerlink" title="配置文档"></a>配置文档</h2><p><strong>solver.prototxt</strong><br>参考论文中的数据：<br><img src="https://img-blog.csdn.net/20180924112443836?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdn.net/20180924112502852?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>可有得到下面的配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">train_net: &quot;prototxt/VOC2012_train.prototxt&quot;</span><br><span class="line"></span><br><span class="line"># 由于实验室只有一个GPU，没有办法将batchsize设为16，所以用下面的参数来代替</span><br><span class="line">iter_size: 16</span><br><span class="line"></span><br><span class="line">lr_policy: &quot;poly&quot;</span><br><span class="line">power: 0.9</span><br><span class="line"># 实际运行中直接将学习率设为论文中的0.01并且不采用初始化模型时会导致loss爆炸，使用下面的学习率并使用作者训好的模型来finetune时loss正常</span><br><span class="line">base_lr: 1e-3</span><br><span class="line"></span><br><span class="line">average_loss: 20</span><br><span class="line">display: 20</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0001</span><br><span class="line"></span><br><span class="line"># imagenet</span><br><span class="line"># max_iter: 150000</span><br><span class="line"># PASCAL VOC</span><br><span class="line">max_iter: 30000</span><br><span class="line"># Cityscape</span><br><span class="line">#max_iter: 90000</span><br><span class="line"></span><br><span class="line">snapshot: 1000</span><br><span class="line">snapshot_prefix: &quot;/evaluation/snapshot/voc2012&quot;</span><br><span class="line">solver_mode: GPU</span><br></pre></td></tr></table></figure><p><strong>train.prototxt</strong><br>该训练网络要考虑下面这些问题：<br><img src="https://img-blog.csdn.net/20180924113652188?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p><p>主要参考的是：<br><a href="https://github.com/SoonminHwang/caffe-segmentation/tree/master/pspnet/models" target="_blank" rel="noopener">https://github.com/SoonminHwang/caffe-segmentation/tree/master/pspnet/models</a><br>里面分享的网络</p><p><strong>run.sh</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">cd ../</span><br><span class="line">## MODIFY PATH for YOUR SETTING</span><br><span class="line">CAFFE_DIR=/home/t7810/project/PSPNet-master</span><br><span class="line">CONFIG_DIR=$&#123;CAFFE_DIR&#125;/evaluation/prototxt</span><br><span class="line">MODEL_DIR=$&#123;CAFFE_DIR&#125;/evaluation/model</span><br><span class="line">CAFFE_BIN=$&#123;CAFFE_DIR&#125;/build/tools/caffe</span><br><span class="line">DEV_ID=0</span><br><span class="line"></span><br><span class="line">sudo $&#123;CAFFE_BIN&#125; train \</span><br><span class="line">-solver=$&#123;CONFIG_DIR&#125;/VOC2012_solver.prototxt \</span><br><span class="line">-weights=$&#123;MODEL_DIR&#125;/pspnet101_VOC2012.caffemodel \</span><br><span class="line">-gpu=$&#123;DEV_ID&#125; \</span><br><span class="line">2&gt;&amp;1 | tee $&#123;CAFFE_DIR&#125;/evaluation/snapshot/train.log</span><br></pre></td></tr></table></figure><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>按照上面论文中的截图可以看到作者使用了以下数据增强：</p><ul><li>随机镜像</li><li>随机在[0.5, 2]之间进行resize</li><li>随机在[-10, 10]之间随机旋转</li><li>随机高斯模糊</li></ul><p>下面是进行数据增强的脚本：<br>其中主要是旋转部分考虑到旋转后的图像会产生黑边，而label图像每一个像素点都是类别特征，所以黑边可能会影响到训练，因此参考：<br><a href="https://blog.csdn.net/YhL_Leo/article/details/51510432" target="_blank" rel="noopener">https://blog.csdn.net/YhL_Leo/article/details/51510432</a> 这篇博文求每次旋转后图片的内接最大矩形，然后将博文中的C++代码用python改写成下面旋转处理部分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This script shows how to relize data augmentation in PSPNet.</span><br><span class="line"> 1. random mirror for all datasets</span><br><span class="line"> 2. random resize between 0.5 and 2 for all datasets</span><br><span class="line"> 3. random rotation between -10 and 10 degrees for ImageNet(ADE20K) and PASCAL VOC</span><br><span class="line"> 4. random Gaussian blur for ImageNet(ADE20K) and PASCAL VOC</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from __future__ import division</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import re</span><br><span class="line">import cv2</span><br><span class="line">import math</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">dataset_list = [&apos;ADE20K&apos;, &apos;cityscapes&apos;, &apos;VOC2012&apos;]</span><br><span class="line">dataset_name = dataset_list[0]</span><br><span class="line">DATA_ROOT = &quot;/Users/camlin_z/Data/data&quot;    # change to your data root directory</span><br><span class="line">img_data_dir_name = &quot;images&quot;    # original images directory name</span><br><span class="line">anno_data_dir_name = &quot;annotation&quot;   # label images directory name</span><br><span class="line"></span><br><span class="line">img_data_dir = os.path.join(DATA_ROOT, img_data_dir_name)</span><br><span class="line">anno_data_dir = os.path.join(DATA_ROOT, anno_data_dir_name)</span><br><span class="line"># rotate image global param: judge the cols is bigger than rows or not(image is vertical or horizon)</span><br><span class="line">isColBigger = True</span><br><span class="line"></span><br><span class="line">def mkr(dr):</span><br><span class="line">    if not os.path.exists(dr):</span><br><span class="line">        os.mkdir(dr)</span><br><span class="line"></span><br><span class="line"># 1. random mirror</span><br><span class="line">def mirror_process(img_name, anno_name, mirror_img_data_dir, mirror_anno_data_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    mirror process</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    img = cv2.imread(os.path.join(img_data_dir, img_name))</span><br><span class="line">    anno = cv2.imread(os.path.join(anno_data_dir ,anno_name))</span><br><span class="line"></span><br><span class="line">    flip_flag = np.random.randint(-1, 2)</span><br><span class="line">    img_mirror = cv2.flip(img, flip_flag)</span><br><span class="line">    cv2.imwrite(os.path.join(mirror_img_data_dir, img_name), img_mirror)</span><br><span class="line">    anno_mirror = cv2.flip(anno, flip_flag)</span><br><span class="line">    cv2.imwrite(os.path.join(mirror_anno_data_dir, anno_name), anno_mirror)</span><br><span class="line"></span><br><span class="line"># 2. random resize between 0.5 and 2 for all datasets</span><br><span class="line">def resize_process(img_name, anno_name, resize_img_data_dir, resize_anno_data_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    resize process</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    img = cv2.imread(os.path.join(img_data_dir, img_name))</span><br><span class="line">    anno = cv2.imread(os.path.join(anno_data_dir, anno_name))</span><br><span class="line">    height, width = img.shape[:2]</span><br><span class="line"></span><br><span class="line">    resize_flag = np.random.uniform(0.5, 2)</span><br><span class="line">    new_height = int(height * resize_flag)</span><br><span class="line">    new_width = int(width * resize_flag)</span><br><span class="line">    img_resize = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">    cv2.imwrite(os.path.join(resize_img_data_dir, img_name), img_resize)</span><br><span class="line">    anno_resize = cv2.resize(anno, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">    cv2.imwrite(os.path.join(resize_anno_data_dir, anno_name), anno_resize)</span><br><span class="line"></span><br><span class="line"># 3. random rotation between -10 and 10 degrees</span><br><span class="line"># reference material: https://blog.csdn.net/YhL_Leo/article/details/51510432</span><br><span class="line">def rotate_process(img_name, anno_name, rotate_img_data_dir, rotate_anno_data_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rotate process</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    img = cv2.imread(os.path.join(img_data_dir, img_name))</span><br><span class="line">    anno = cv2.imread(os.path.join(anno_data_dir, anno_name))</span><br><span class="line"></span><br><span class="line">    rows, cols = img.shape[:2]</span><br><span class="line">    if cols &lt; rows:</span><br><span class="line">        isColBigger = False</span><br><span class="line"></span><br><span class="line">    # Notes: clockwise rotation is negative and anticlockwise rotation is positive</span><br><span class="line">    theta = np.random.randint(-10, 10)</span><br><span class="line">    if theta &lt; 0:</span><br><span class="line">        theta += 360</span><br><span class="line"></span><br><span class="line">    # rotate image</span><br><span class="line">    # Notes: param &quot;-theta&quot; for accord with our intuition, make clockwise rotation positive</span><br><span class="line">    M_rotation = cv2.getRotationMatrix2D((cols / 2, rows / 2), -theta, 1)</span><br><span class="line">    img_rotated = cv2.warpAffine(img, M_rotation, (cols, rows))</span><br><span class="line">    anno_rotated = cv2.warpAffine(anno, M_rotation, (cols, rows))</span><br><span class="line"></span><br><span class="line">    # compute max rect</span><br><span class="line">    theta -= int(theta / 180) * 180</span><br><span class="line">    vertex = np.array([[-(cols-1)/2, (rows-1)/2], [(cols-1)/2, (rows-1)/2],</span><br><span class="line">                        [(cols-1)/2, -(rows-1)/2], [-(cols-1)/2,-(rows-1)/2]])</span><br><span class="line"></span><br><span class="line">    if theta &gt; 0 and theta &lt; 90:</span><br><span class="line">        rMat = rotateMat(theta)</span><br><span class="line">        r_vertext = rotateVertex(vertex, rMat)</span><br><span class="line">        maxp = getCrossPoint(r_vertext)</span><br><span class="line"></span><br><span class="line">        if maxp[0] &gt; cols/2 or maxp[1] &gt; rows/2:</span><br><span class="line">            maxp = getSpecialCrossPoint(r_vertext)</span><br><span class="line"></span><br><span class="line">        maxp[0] = maxp[0] if maxp[0] &lt; cols/2 else cols/2</span><br><span class="line">        maxp[1] = maxp[1] if maxp[1] &lt; rows/2 else rows/2</span><br><span class="line"></span><br><span class="line">        # Notes: first slice param is rows(height) scale, second is cols(width) scale</span><br><span class="line">        img_rot = img_rotated[int(rows / 2 - maxp[1]): int(rows / 2 - maxp[1] + 2 * abs(maxp[1])),</span><br><span class="line">                  int(cols / 2 - maxp[0]): int(cols / 2 - maxp[0] + 2 * abs(maxp[0]))]</span><br><span class="line">        anno_rot = anno_rotated[int(rows / 2 - maxp[1]): int(rows / 2 - maxp[1] + 2 * abs(maxp[1])),</span><br><span class="line">                   int(cols / 2 - maxp[0]): int(cols / 2 - maxp[0] + 2 * abs(maxp[0]))]</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_img_data_dir, img_name), img_rot)</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_anno_data_dir, anno_name), anno_rot)</span><br><span class="line"></span><br><span class="line">    elif theta == 90:</span><br><span class="line">        if cols &gt; rows:</span><br><span class="line">            img_rot = img_rotated[0:rows, int((cols-rows)/2):int((cols-rows)/2 + rows)]</span><br><span class="line">            anno_rot = anno_rotated[0:rows, int((cols-rows)/2):int((cols-rows)/2 + rows)]</span><br><span class="line">        else:</span><br><span class="line">            img_rot = img_rotated[0:cols, int((rows-cols)/2):int((rows-cols)/2+cols)]</span><br><span class="line">            anno_rot = anno_rotated[0:cols, int((rows-cols)/2):int((rows-cols)/2+cols)]</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_img_data_dir, img_name), img_rot)</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_anno_data_dir, anno_name), anno_rot)</span><br><span class="line"></span><br><span class="line">    elif theta &gt; 90:</span><br><span class="line">        theta2 = 180 - theta</span><br><span class="line">        rMat = rotateMat(theta2)</span><br><span class="line">        r_vertext = rotateVertex(vertex, rMat)</span><br><span class="line">        maxp = getCrossPoint(r_vertext)</span><br><span class="line"></span><br><span class="line">        if maxp[0] &gt; cols/2 or maxp[1] &gt; rows/2:</span><br><span class="line">            maxp = getSpecialCrossPoint(r_vertext)</span><br><span class="line"></span><br><span class="line">        maxp[0] = maxp[0] if maxp[0] &lt; cols / 2 else cols / 2</span><br><span class="line">        maxp[1] = maxp[1] if maxp[1] &lt; rows / 2 else rows / 2</span><br><span class="line"></span><br><span class="line">        img_rot = img_rotated[int(rows / 2 - maxp[1]): int(rows / 2 - maxp[1] + 2 * abs(maxp[1])),</span><br><span class="line">                  int(cols / 2 - maxp[0]): int(cols / 2 - maxp[0] + 2 * abs(maxp[0]))]</span><br><span class="line">        anno_rot = anno_rotated[int(rows / 2 - maxp[1]): int(rows / 2 - maxp[1] + 2 * abs(maxp[1])),</span><br><span class="line">                  int(cols / 2 - maxp[0]): int(cols / 2 - maxp[0] + 2 * abs(maxp[0]))]</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_img_data_dir, img_name), img_rot)</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_anno_data_dir, anno_name), anno_rot)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_img_data_dir, img_name), img_rotated)</span><br><span class="line">        cv2.imwrite(os.path.join(rotate_anno_data_dir, anno_name), anno_rotated)</span><br><span class="line"></span><br><span class="line">def rotateMat(radian):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    compute rotate matrix</span><br><span class="line">    :param radian: radian system angle</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    alpha = radian</span><br><span class="line">    alpha *= np.pi / 180</span><br><span class="line">    # below rotate matrix is different from Wikipedia</span><br><span class="line">    # ([[math.cos(alpha), -math.sin(alpha)], [math.sin(alpha), math.cos(alpha)]])</span><br><span class="line">    # like the above &quot;Notes: param &quot;-theta&quot; &quot;, make clockwise rotation is positive</span><br><span class="line">    return np.array([[math.cos(alpha), math.sin(alpha)], [-math.sin(alpha), math.cos(alpha)]])</span><br><span class="line"></span><br><span class="line">def rotateVertex(vertexs, rMat):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    compute rectangle coordinate after rotated &quot;rt&quot; by rotate matrix &quot;rMat&quot;</span><br><span class="line">    :param vertexs: original rectangle coordinate</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rt = vertexs</span><br><span class="line">    for i in range(vertexs.shape[0]):</span><br><span class="line">        v_i = np.array([[vertexs[i][0]], [vertexs[i][1]]])</span><br><span class="line">        v_r = np.matmul(rMat, v_i)</span><br><span class="line">        rt[i] = (v_r[0][0], v_r[1][0])</span><br><span class="line">    return rt</span><br><span class="line"></span><br><span class="line">def getCrossPoint(vertexs):</span><br><span class="line">    ln_ab = lineFunction(vertexs)</span><br><span class="line">    return getMaxRectRegion(ln_ab)</span><br><span class="line"></span><br><span class="line">def getSpecialCrossPoint(vertexs):</span><br><span class="line">    line0_1 = lineFunction(vertexs[0], vertexs[1])</span><br><span class="line">    line1_2 = lineFunction(vertexs[1], vertexs[2])</span><br><span class="line"></span><br><span class="line">    (a1, b1, c1) = line0_1</span><br><span class="line">    (a2, b2, c2) = line1_2</span><br><span class="line">    x = -(a1*c2 + a2*c1) / (a2*b1 + a1*b2)</span><br><span class="line">    y = -(b1*x+c1) / a1</span><br><span class="line">    return np.array([x, y])</span><br><span class="line"></span><br><span class="line">def lineFunction(v1, v2=np.array([])):</span><br><span class="line">    if v2.shape[0] == 0:</span><br><span class="line">        pa = v1[0]</span><br><span class="line">        pb = v1[1]</span><br><span class="line">        if not isColBigger:</span><br><span class="line">           pb = v1[3]</span><br><span class="line">    else:</span><br><span class="line">        pa = v1</span><br><span class="line">        pb = v2</span><br><span class="line"></span><br><span class="line">    delta_x = pa[0] - pb[0]</span><br><span class="line">    delta_y = pa[1] - pb[1]</span><br><span class="line"></span><br><span class="line">    line = np.array([delta_x, -delta_y, -pb[1] * delta_x + pb[0] * delta_y])</span><br><span class="line"></span><br><span class="line">    # normalization param</span><br><span class="line">    m_line = np.sqrt(line[0] * line[0] + line[1] * line[1])</span><br><span class="line">    # compute the param a, b, s in the blog</span><br><span class="line">    line *= 1 / m_line</span><br><span class="line"></span><br><span class="line">    # assume a &gt;= 0</span><br><span class="line">    if line[0] &lt; 0:</span><br><span class="line">        line = -line</span><br><span class="line"></span><br><span class="line">    return line</span><br><span class="line"></span><br><span class="line">def getMaxRectRegion(line):</span><br><span class="line">    if line[0] != 0 and line[1] != 0:</span><br><span class="line">        (a, b, c) = line</span><br><span class="line"></span><br><span class="line">        if not isColBigger:</span><br><span class="line">            b *= -1</span><br><span class="line">        return np.array([-c/(2*b), -c/(2*a)])</span><br><span class="line">    else:</span><br><span class="line">        return np.array([0, 0])</span><br><span class="line"></span><br><span class="line">def getImageRange(vertexs):</span><br><span class="line">    pMin = np.array(0, 0)</span><br><span class="line">    pMax = np.array(0, 0)</span><br><span class="line">    for i in range(vertexs.shape[0]):</span><br><span class="line">        pMin[0] = pMin[0] if pMin[0] &lt; vertexs[i][0] else vertexs[i][0]</span><br><span class="line">        pMin[1] = pMin[1] if pMin[1] &lt; vertexs[i][1] else vertexs[i][1]</span><br><span class="line">        pMax[0] = pMax[0] if pMax[0] &lt; vertexs[i][0] else vertexs[i][0]</span><br><span class="line">        pMax[1] = pMax[1] if pMax[1] &lt; vertexs[i][1] else vertexs[i][1]</span><br><span class="line">    return ([pMax[0] - pMin[0] + 1, pMax[1] - pMin[1] + 1])</span><br><span class="line"></span><br><span class="line"># 4. random Gaussian blur for ImageNet(ADE20K) and PASCAL VOC</span><br><span class="line">def gaussian_process(img_name, anno_name, gaussian_img_data_dir, gaussian_anno_data_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">        gaussian blur process</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    img = cv2.imread(os.path.join(img_data_dir, img_name))</span><br><span class="line">    anno = cv2.imread(os.path.join(anno_data_dir, anno_name))</span><br><span class="line"></span><br><span class="line">    # gaussian blur param</span><br><span class="line">    kernel_size_list = [3, 5, 7]</span><br><span class="line">    kernel_size = kernel_size_list[np.random.randint(0, 3)]</span><br><span class="line">    sigma = np.random.uniform(0, 10)</span><br><span class="line"></span><br><span class="line">    img_gaussian = cv2.GaussianBlur(img, (kernel_size, kernel_size), sigma)</span><br><span class="line">    anno_gaussian = cv2.GaussianBlur(anno, (kernel_size, kernel_size), sigma)</span><br><span class="line"></span><br><span class="line">    cv2.imwrite(os.path.join(gaussian_img_data_dir, img_name), img_gaussian)</span><br><span class="line">    cv2.imwrite(os.path.join(gaussian_anno_data_dir, anno_name), anno_gaussian)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The main entrance</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    ######################### make new director ######################</span><br><span class="line">    # mirrior data director</span><br><span class="line">    mirror_img_data_dir = os.path.join(DATA_ROOT, img_data_dir_name + &quot;_mirror&quot;)</span><br><span class="line">    mirror_anno_data_dir = os.path.join(DATA_ROOT, anno_data_dir_name + &quot;_mirror&quot;)</span><br><span class="line">    mkr(mirror_img_data_dir)</span><br><span class="line">    mkr(mirror_anno_data_dir)</span><br><span class="line"></span><br><span class="line">    # resize data director</span><br><span class="line">    resize_img_data_dir = os.path.join(DATA_ROOT, img_data_dir_name + &quot;_resize&quot;)</span><br><span class="line">    resize_anno_data_dir = os.path.join(DATA_ROOT, anno_data_dir_name + &quot;_resize&quot;)</span><br><span class="line">    mkr(resize_img_data_dir)</span><br><span class="line">    mkr(resize_anno_data_dir)</span><br><span class="line"></span><br><span class="line">    # rotate data director</span><br><span class="line">    rotate_img_data_dir = os.path.join(DATA_ROOT, img_data_dir_name + &quot;_rotate&quot;)</span><br><span class="line">    rotate_anno_data_dir = os.path.join(DATA_ROOT, anno_data_dir_name + &quot;_rotate&quot;)</span><br><span class="line">    mkr(rotate_img_data_dir)</span><br><span class="line">    mkr(rotate_anno_data_dir)</span><br><span class="line"></span><br><span class="line">    # gaussian data director</span><br><span class="line">    gaussian_img_data_dir = os.path.join(DATA_ROOT, img_data_dir_name + &quot;_gaussian&quot;)</span><br><span class="line">    gaussian_anno_data_dir = os.path.join(DATA_ROOT, anno_data_dir_name + &quot;_gaussian&quot;)</span><br><span class="line">    mkr(gaussian_img_data_dir)</span><br><span class="line">    mkr(gaussian_anno_data_dir)</span><br><span class="line"></span><br><span class="line">    if dataset_name == &apos;cityscapes&apos;:</span><br><span class="line">        shutil.rmtree(rotate_img_data_dir)</span><br><span class="line">        shutil.rmtree(rotate_anno_data_dir)</span><br><span class="line">        shutil.rmtree(gaussian_img_data_dir)</span><br><span class="line">        shutil.rmtree(gaussian_anno_data_dir)</span><br><span class="line"></span><br><span class="line">    ####################### main process loop ######################</span><br><span class="line">    for _, _, anno_name_list in os.walk(anno_data_dir):</span><br><span class="line">        for anno_name in anno_name_list:</span><br><span class="line"></span><br><span class="line">            # generate image name by annotation name</span><br><span class="line">            if dataset_name == &apos;ADE20K&apos;:</span><br><span class="line">                img_name = re.split(r&apos;.seg&apos;, anno_name)[0] + &quot;.jpg&quot;</span><br><span class="line">            elif dataset_name == &apos;cityscapes&apos;:</span><br><span class="line">                img_name = re.split(r&apos;.leftImg8bit&apos;, anno_name)[0] + &quot;_gtFine_labelTrainIds.png&quot;</span><br><span class="line">            elif dataset_name == &apos;VOC2012&apos;:</span><br><span class="line">                img_name = anno_name.split(&apos;.&apos;)[0] + &quot;.jpg&quot;</span><br><span class="line">            else:</span><br><span class="line">                print &quot;wrong dataset name!&quot;</span><br><span class="line">                return</span><br><span class="line"></span><br><span class="line">            print &quot;Processing: &quot;, img_name</span><br><span class="line">            if not (os.path.exists(os.path.join(img_data_dir, img_name)) and os.path.join(mirror_anno_data_dir, anno_name)):</span><br><span class="line">                print &quot;file not exists!&quot;</span><br><span class="line">                continue</span><br><span class="line"></span><br><span class="line">            # process image and annotation</span><br><span class="line">            mirror_process(img_name, anno_name, mirror_img_data_dir, mirror_anno_data_dir)</span><br><span class="line">            resize_process(img_name, anno_name, resize_img_data_dir, resize_anno_data_dir)</span><br><span class="line">            if dataset_name == &apos;ADE20K&apos; or dataset_name == &apos;VOC2012&apos;:</span><br><span class="line">                rotate_process(img_name, anno_name, rotate_img_data_dir, rotate_anno_data_dir)</span><br><span class="line">                gaussian_process(img_name, anno_name, gaussian_img_data_dir, gaussian_anno_data_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>对应的标签处理脚本如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">root = &quot;/home/t7810/data/VOC2012_train&quot;</span><br><span class="line">path_old_file = &quot;VOC2012_train.txt&quot;</span><br><span class="line">path_new_file = &quot;VOC2012_train_aug.txt&quot;</span><br><span class="line">dir_suffix_list = [&apos;_mirror&apos;, &apos;_resize&apos;, &apos;_rotate&apos;, &apos;_gaussian&apos;]</span><br><span class="line"></span><br><span class="line">file_new = open(os.path.join(root, path_new_file), &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">with open(os.path.join(root, path_old_file)) as file:</span><br><span class="line">    for line in file:</span><br><span class="line">        # for PASCAL VOC 2012</span><br><span class="line">        file_new.write(line)</span><br><span class="line">        line_split = line.strip().split()</span><br><span class="line">        (_, img_dir_name, img_name) = line_split[0].strip().split(&apos;/&apos;)</span><br><span class="line">        (_, anno_dir_name, anno_name) = line_split[1].strip().split(&apos;/&apos;)</span><br><span class="line"></span><br><span class="line">        for dir_suffix in dir_suffix_list:</span><br><span class="line">            img_dir_name_new = img_dir_name + dir_suffix</span><br><span class="line">            anno_dir_name_new = anno_dir_name + dir_suffix</span><br><span class="line">            file_new.write(&quot;/&quot; + img_dir_name_new + &quot;/&quot; + img_name + &quot; &quot;</span><br><span class="line">                           + &quot;/&quot; + anno_dir_name_new + &quot;/&quot; + anno_name + &quot;\n&quot;)</span><br></pre></td></tr></table></figure><p>进行数据增强后的图片文件夹结构为：</p><center><br><img src="https://img-blog.csdn.net/20180926164817340?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="60%" alt="图片还在路上，稍等..."><br></center><p>按照上面的配置可以进行训练：<br><img src="https://img-blog.csdn.net/2018092510225348?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>这是我用原始模型进行finetune一天之后的情况，很明显没有收敛，其中第一个精度是整个网络的精度，后面的两个loss分别是总体的和拉出一个分支的loss，下面的精度分别对应21类的精度，由于每一轮的batchsize为1，所以每一次只有1-2个类的精度会显示出来，明显很低，而且用中间保存的caffemodel进行测试结果也很不好，如果有哪位大神完美复现了，可以一起交流学习一下，后续自己也会继续探索，感恩！！！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;最近实习完回学校，继续开始我的图像语义分割研究了，为毕业论文做准备，首先还是看了一下最近的一些最新的分割论文，在ICNet论文中找到一个很好
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习算法" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="图像语义分割" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
    
      <category term="PSPNet" scheme="http://camlinzhang.com/tags/PSPNet/"/>
    
  </entry>
  
  <entry>
    <title>随笔2018_09_03</title>
    <link href="http://camlinzhang.com/2018/09/03/%E9%9A%8F%E7%AC%942018-09-03/"/>
    <id>http://camlinzhang.com/2018/09/03/随笔2018-09-03/</id>
    <published>2018-09-03T11:20:39.000Z</published>
    <updated>2019-03-31T10:44:43.857Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1f9j6026qj31pq0u0qv6.jpg" alt=""></p><p>终于还是坐上了回程的火车，四个月，在一辈子的时间里，说长不长，说短不短，但是回忆里深夜中一个人昏黄的街道，枕边绝望的泪水，看到希望后内心的喜悦，还有那短暂的幸福与绝望的交织，这一切让这短短的四个月有时像四年那么漫长，有时又像一转瞬，但无论长短，无论起伏，终究还是画上了句号，在徽州的山水与古镇渐渐远去的背影中完完整整的结束……</p><p>徽州，作为一个意外的终点，我曾坐在摇椅上远远的看着你，舟行画中，画卧心底，脚踩着大地的温热，头顶着依稀的光亮，每一个毛孔都因此打开，妄图伸向更远的远方，还有触碰不到的你。云与山薄如蝉翼般划过天际，抚慰着我的过往，我的曾经。</p><p>我们一直有一双可以看到外物的眼睛，却没有一双可以看到自己的双眸，我们将一切尽收入眼底，可是不曾知道万事万物也在静静地观察着我们自己。曾经眼中的风景，终究有一刻也将我们纳入其中。当自以为的超脱相外不过是沉浸其中，我再一次在生命的漫漫旅途中迷茫。我们无时不刻被各种周遭环境裹挟，在承受与被承受的循环中艰难而又幸福的前行。我们看到世事，却依然受缚于世事，我们祈求超越人性，却依然在人性中挣扎。一切寂静如无物，却又激流暗涌。天地玄黄，云海苍茫，当你以为看透，却又回到原点，当你以为超越，实则只是逃避。万事万物存在皆有意义，可是意义的终点确是虚无。</p><p>我向往完整的人生应该是五味杂陈，且不排除遍体鳞伤，可是体验的幸福感终究不免在失望与痛苦中陷入意义的怪圈中。规则，大义，普世价值观，我们在环境中生长，在人性中成熟，在束缚中一点点适应，一点点冷漠，一点点变成一个大人。终于有一天，我们可以宠辱不惊，闲看庭前花开花落；去留无意，漫观天外云卷云舒，那是我们将一切人世的复杂化为了太阳东升西落的简单。可是一世时间，我们选择简单亦或复杂，选择幸福亦或痛苦，或者别无选择的前行，终究在一切的裹挟中走到生命的尽头。</p><p>可是我们不曾因为害怕结束而放弃开始，也许茫茫天地间你的生死无足轻重，但是你在你自己的生命和环境中却是全世界。每个人都是一座孤岛，在浩瀚大海中漂泊，也许游船曾将两者相连，也许我们曾因碰撞而在某一处相互联结，但我们身下的激流依然涌动，从来不曾停止。</p><p>很喜欢一句话：如果你觉得这世态炎凉，那是因为你还没有经历过真正的地狱。一个人本性若善，纵然是烈狱归来，其赤子之心亦可永生不死。虽然一切可知与不可知都在决定着我们的本心本性，虽然一切束缚都在挣脱与捆绑中相生相伴，虽然美丑善恶都在时代与种族间变幻莫测，可是当我们看到初生的朝阳与落日的余晖时，看到壮丽的山河与清澈的溪水时，看到烟雨的古镇和缥缈的游船时，我们知道，钟灵毓秀，大美河山，此为人赞自然，而自然即为本心，不因人的褒奖而美丽，不因人的厌恶而丑陋，只是生而为此，本心为此。</p><p>万事万物只是存在，本无关意义与价值，为人者，在一个个无关因果的果中总结着因，在一个个意义和偶然中为其赋予价值，却最终在意义的尽头找到了虚无，衍生出万能的主。万事万物本没有意义，其存在即是他的本性，而他的本性也即是他的本心，在裹挟中自然生长，无论善恶美丑，不管意义价值，他只是存在，仅此而已。</p><p>而为人者，我们毕生追求，我们终生信仰，在初生朝阳中绚烂繁华，在落日余晖中沉寂凋零，无关于残忍和现实，终将如沙粒落入大海，美好且安静的存在着，不再消逝……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tKfTcly1g1f9j6026qj31pq0u0qv6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;终于还是坐上了回程的火车，四个月，在一辈子的时间里，说长不长，说短不短，但是回忆里深夜中一
      
    
    </summary>
    
      <category term="随笔" scheme="http://camlinzhang.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>人脸检测和人脸对齐C++工程</title>
    <link href="http://camlinzhang.com/2018/08/25/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90C++%E5%B7%A5%E7%A8%8B/"/>
    <id>http://camlinzhang.com/2018/08/25/人脸检测和人脸对齐C++工程/</id>
    <published>2018-08-25T11:39:58.000Z</published>
    <updated>2019-03-25T12:21:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>代码链接为：<a href="https://github.com/CamlinZ/face_landmark_detector_project.git" target="_blank" rel="noopener">https://github.com/CamlinZ/face_landmark_detector_project.git</a></p><h2 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h2><p>该工程分为人脸检测，人脸对齐和特征点平滑处理三个部分。</p><p><strong>人脸检测</strong></p><p>采用FaceBoxes作为解决方案，参考论文《FaceBoxes: A CPU Real-time Face Detector with High Accuracy》。并且在该论文提出的网络结构基础上，进行了网络压缩，使其达到更快的速度，详情见model文件夹中的README.md</p><p><strong>人脸对齐</strong></p><p>采用dlib库中的ERT作为解决方案，参考论文《One Millisecond Face Alignment with an Ensemble of Regression Trees》</p><p><strong>特征点平滑处理</strong></p><p>由于使用ERT进行视频帧的特征点检测时，特征点会出现明显的抖动，所以采用卡尔曼滤波进行平滑处理，参考论文《Automatic facial landmark tracking in video sequences using kalman filter assisted active shape models》</p><h2 id="2、流程说明"><a href="#2、流程说明" class="headerlink" title="2、流程说明"></a>2、流程说明</h2><p><strong>总体流程</strong></p><p><img src="/2018/08/25/人脸检测和人脸对齐C++工程/flow_diagram.jpg" alt="flow_diagram"></p><p><strong>调整人脸检测框流程</strong></p><p>在上述流程中，由于检测算法得到的人脸框普遍无法将所有的68个人脸特征点包含进来，因此引入调整人脸检测框位置这一步，具体步骤如下图所示：</p><p><img src="/2018/08/25/人脸检测和人脸对齐C++工程/adjust_flow_diagram.jpg" alt="adjust_flow_diagram"></p><p><strong>卡尔曼滤波处理流程</strong></p><p>卡尔曼滤波分别针对68个特征点中嘴部、眼睛、面部轮廓和其他区域4个部分进行滤波，这四个部分的原理相同，只是参数的设置不同。以其中一个为例，如下图所示：</p><p><img src="/2018/08/25/人脸检测和人脸对齐C++工程/kalman_filter_flow_diagram.jpg" alt="kalman_filter_flow_diagram"></p><h2 id="3、依赖环境"><a href="#3、依赖环境" class="headerlink" title="3、依赖环境"></a>3、依赖环境</h2><ul><li>FaceBoxes caffe环境</li><li>dlib 19.15</li><li>Opencv 3.1.0</li><li>CentOS Linux release 7.2</li></ul><h2 id="4、API"><a href="#4、API" class="headerlink" title="4、API"></a>4、API</h2><p><strong>类说明</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FacenetCaffe</span><br></pre></td></tr></table></figure><ul><li>该类用于封装人脸检测的功能函数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FaceLandmarkDetector</span><br></pre></td></tr></table></figure><ul><li>该类用于封装人脸特征点检测的功能函数</li></ul><p><strong>函数说明</strong></p><p><strong>class FacenetCaffe</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int initModel(const string &amp; model_path, const string &amp; weights_path, const string &amp; mean_value)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>model_path —— 网络模型路径</li><li>weights_path —— 网络权重路径</li><li>mean_value —— 图像均值</li></ul><p>返回值</p><ul><li>int类型，用于返回错误码，可根据实际情况进行修改</li></ul><p>作用</p><ul><li>初始化人脸检测模型</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;vector&lt;float&gt;&gt; detectFace(const cv::Mat &amp; img)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>img —— 待检测图片</li></ul><p>返回值</p><ul><li>检测到人脸的坐标值</li></ul><p>作用</p><ul><li>检测img中人脸坐标值</li></ul><p><strong>class FaceLandmarkDetector</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void init(const string model_path)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>model_path —— ERT模型路径</li></ul><p>返回值</p><ul><li>空</li></ul><p>作用</p><ul><li>初始化ERT模型和卡尔曼滤波参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int GetFaceBox(FacenetCaffe fc_box, cv::Mat image, std::vector&lt;std::vector&lt;int&gt;&gt; &amp; face_box, float confidence_threshold, double &amp; time_detec)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>fc_box —— FacenetCaffe类的对象</li><li>image —— 待检测人脸框的图像</li><li>face_box —— 检测出来的人脸框</li><li>confidence_threshold —— 置信度阈值</li><li>time_detec —— 检测时间</li></ul><p>返回值</p><ul><li>int类型，用于返回错误码，可根据实际情况进行修改</li></ul><p>作用</p><ul><li>利用类FacenetCaffe中的detectFace方法检测图片的人脸框位置，并计算检测时间</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int AdjustFaceBox(cv::Mat image, std::vector&lt;std::vector&lt;int&gt;&gt; &amp; face_box)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>image —— 待检测人脸框的图像</li><li>face_box —— 由GetFaceBox检测出来的人脸框</li></ul><p>返回值</p><ul><li>int类型，用于返回错误码，可根据实际情况进行修改</li></ul><p>作用</p><ul><li>根据上面第二部分的流程说明中”调整人脸检测框流程“的步骤对GetFaceBox检测出来的人脸框进行调整，使其可以包含所有的人脸特征点</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int ImageStandard(float &amp; rate_w, float &amp; rate_h, cv::Mat frame, std::vector&lt;std::vector&lt;int&gt;&gt; face_box, std::vector&lt;cv::Mat&gt; &amp; img_resize_vector)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>rate_w —— 缩放人脸框的x轴方向比例</li><li>rate_h —— 缩放人脸框的y轴方向比例</li><li>frame —— 待检测人脸框的图像</li><li>face_box —— 经过AdjustFaceBox调整过后的人脸检测框</li><li>img_resize_vector —— 存储根据上面人脸检测框crop出来的人脸图片</li></ul><p>返回值</p><ul><li>int类型，用于返回错误码，可根据实际情况进行修改</li></ul><p>作用</p><ul><li>为了达到更高的精度，ERT检测人脸特征点需要224x224的纯人脸图像，所以要根据上面得到的人脸框将人脸从原图中crop出来，并将其resize到224x224大小。而由于后面需要将这些点重新映射到原图上，所以需要rate_w和rate_h来保存缩放比例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int detectFaceLandmark(float &amp; rate_w, float &amp; rate_h, std::vector&lt;int&gt; face_box, cv::Mat image, std::vector&lt;std::vector&lt;int&gt;&gt; &amp; landmark, bool flag, double &amp; time)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>rate_w —— 缩放人脸框的x轴方向比例</li><li>rate_h —— 缩放人脸框的y轴方向比例</li><li>face_box —— 经过AdjustFaceBox调整过后的人脸检测框</li><li>image —— 根据人脸检测框crop出来的人脸图片</li><li>landmark —— 检测出来的特征点，是一个68x2的二维vector</li><li>flag —— 是否需要使用滤波处理</li><li>time —— 检测特征点所需时间</li></ul><p>返回值</p><ul><li>int类型，用于返回错误码，可根据实际情况进行修改</li></ul><p>作用</p><ul><li>检测根据人脸检测框crop出来的人脸图片中对应的人脸特征点，并将其坐标映射回原图中。如果flag为True，则将该预测出来的特征点进行滤波平滑处理。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void GetNewPoints(float rate_w, float rate_h, std::vector&lt;int&gt; face_box, std::vector&lt;std::vector&lt;int&gt;&gt; &amp; landmark_pre, std::vector&lt;std::vector&lt;int&gt;&gt; &amp; landmark_pre_ori)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>rate_w —— 缩放人脸框的x轴方向比例</li><li>rate_h —— 缩放人脸框的y轴方向比例</li><li>face_box —— 经过AdjustFaceBox调整过后的人脸检测框</li><li>landmark_pre —— 利用ERT检测crop之后的人脸所得到的特征点</li><li>landmark_pre_ori —— 将landmark_pre映射到原始输入图像中得到的特征点</li></ul><p>返回值</p><ul><li>空</li></ul><p>作用</p><ul><li>实现detectFaceLandmark函数中将ERT检测crop之后的人脸所得到的特征点映射到原始输入图像的功能</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;std::vector&lt;int&gt;&gt; KalmanFilter(cv::Mat landmark_pre, float height)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>landmark_pre —— 对于detectFaceLandmark中ERT方法得到的原始图像中的特征点坐标值，将其转换为cv::Mat形式，并将其进行转置为一个68x2的矩阵</li><li>height —— 检测出来的人脸框的高</li></ul><p>返回值</p><ul><li>landmark_pre经过卡尔曼滤波处理后得到的新的特征点坐标值</li></ul><p>作用</p><ul><li>根据上面第二部分的流程说明中”卡尔曼滤波处理流程“对ERT检测到的特征点进行平滑处理</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void drawLandmarks(cv::Mat &amp;image, std::vector&lt;std::vector&lt;int&gt;&gt; landmark, cv::Scalar color = cv::Scalar(255, 0, 0), int radius = 3)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>image —— 输入的原始待检测图片</li><li>landmark —— 特征点坐标值</li><li>color —— 在输入图片上画特征点的颜色</li><li>radius —— 在输入图片上画特征点的半径值</li></ul><p>返回值</p><ul><li>空</li></ul><p>作用</p><ul><li>将检测到的特征点画到输入图片上</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void TestVedioLandmark(FacenetCaffe fc_box, const string root)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>fc_box —— FacenetCaffe类的对象</li><li>root —— 存放示例视频及其对应标签的根目录</li></ul><p>返回值</p><ul><li>空</li></ul><p>作用</p><ul><li>检测示例视频中的人脸位置以及人脸特征点，主函数可以仿照这个函数来写</li></ul><h2 id="5、Quick-Start"><a href="#5、Quick-Start" class="headerlink" title="5、Quick Start"></a>5、Quick Start</h2><ul><li>拷贝整个工程文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone http://git.code.com/camlinzhang/face_detection_and_alignment.git</span><br></pre></td></tr></table></figure><ul><li>修改CMakeList.txt</li></ul><p>（1）编译动态库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set(SOURCE_FILES src/face_landmark_detection.cpp include/face_landmark_detection.h src/facenet_caffe.cpp include/facenet_caffe.h include/caffe_register.h)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">#add_executable(face_landmark $&#123;SOURCE_FILES&#125;)</span><br><span class="line"></span><br><span class="line">ADD_LIBRARY(face_landmark SHARED $&#123;SOURCE_FILES&#125;)</span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;)</span><br><span class="line"></span><br><span class="line">target_link_libraries(face_landmark $&#123;olibs_point&#125; $&#123;bLIBS&#125; $&#123;oLIBS&#125; $&#123;hLIBS&#125; dlib caffe glog gflags protobuf openblas opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)</span><br></pre></td></tr></table></figure><p>（2）编译静态库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">set(SOURCE_FILES src/face_landmark_detection.cpp include/face_landmark_detection.h src/facenet_caffe.cpp include/facenet_caffe.h include/caffe_register.h)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#add_executable(face_landmark $&#123;SOURCE_FILES&#125;)</span><br><span class="line"></span><br><span class="line">ADD_LIBRARY(face_landmark $&#123;SOURCE_FILES&#125;)</span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;)</span><br><span class="line"></span><br><span class="line">target_link_libraries(face_landmark $&#123;olibs_point&#125; $&#123;bLIBS&#125; $&#123;oLIBS&#125; $&#123;hLIBS&#125; dlib caffe glog gflags protobuf openblas opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)</span><br></pre></td></tr></table></figure><p>（3）编译可执行文件，用于测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set(SOURCE_FILES src/main.cpp src/face_landmark_detection.cpp include/face_landmark_detection.h src/facenet_caffe.cpp include/facenet_caffe.h include/caffe_register.h)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">add_executable(face_landmark $&#123;SOURCE_FILES&#125;)</span><br><span class="line"></span><br><span class="line">#ADD_LIBRARY(face_landmark SHARED $&#123;SOURCE_FILES&#125;)</span><br><span class="line">#SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;)</span><br><span class="line"></span><br><span class="line">target_link_libraries(face_landmark $&#123;olibs_point&#125; $&#123;bLIBS&#125; $&#123;oLIBS&#125; $&#123;hLIBS&#125; dlib caffe glog gflags protobuf openblas opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)</span><br></pre></td></tr></table></figure><ul><li>进入工程文件并编译</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd face_detection_and_alignment</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure><ul><li>最后</li></ul><p>（1）执行可执行文件进行测试(当修改CMakeList.txt用于编译可执行文件进行测试时)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./face_landmark</span><br></pre></td></tr></table></figure><p>（2）在工程的根目录下生成库文件(当修改CMakeList.txt用于生成库文件时)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;代码链接为：&lt;a href=&quot;https://github.com/CamlinZ/face_landmark_detector_project.git&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/CamlinZ/f
      
    
    </summary>
    
      <category term="总结文章" scheme="http://camlinzhang.com/categories/%E6%80%BB%E7%BB%93%E6%96%87%E7%AB%A0/"/>
    
      <category term="工作总结" scheme="http://camlinzhang.com/categories/%E6%80%BB%E7%BB%93%E6%96%87%E7%AB%A0/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="人脸检测" scheme="http://camlinzhang.com/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    
      <category term="人脸对齐" scheme="http://camlinzhang.com/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/"/>
    
      <category term="C++" scheme="http://camlinzhang.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>一种人脸68特征点检测的深度学习方法</title>
    <link href="http://camlinzhang.com/2018/07/25/%E4%B8%80%E7%A7%8D%E4%BA%BA%E8%84%B868%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>http://camlinzhang.com/2018/07/25/一种人脸68特征点检测的深度学习方法/</id>
    <published>2018-07-25T13:39:17.000Z</published>
    <updated>2019-03-25T05:07:15.474Z</updated>
    
    <content type="html"><![CDATA[<p>该人脸68特征点检测的深度学习方法采用VGG作为原型进行改造(以下简称mini VGG)，从数据集的准备，网络模型的构造以及最终的训练过程三个方面进行介绍，工程源码详见：<a href="https://github.com/CamlinZ/face_alignment" target="_blank" rel="noopener">Github链接</a></p><h1 id="一、数据集的准备"><a href="#一、数据集的准备" class="headerlink" title="一、数据集的准备"></a>一、数据集的准备</h1><h2 id="1、数据集的采集"><a href="#1、数据集的采集" class="headerlink" title="1、数据集的采集"></a>1、数据集的采集</h2><p>第一类是公共数据集：<br>人脸68特征点检测的数据集通常采用ibug数据集，官网地址为：<br><a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/" target="_blank" rel="noopener">https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</a><br>其中同时包含图像和标注的有(有的数据集免费下载的只有标注没有图像)：<br>300W，AFW，HELEN，LFPW，IBUG五个数据集。<br>如果需要做对于视频类图像的68特征点检测可以用下面300-VM数据集：<br><a href="https://ibug.doc.ic.ac.uk/resources/300-VW/" target="_blank" rel="noopener">https://ibug.doc.ic.ac.uk/resources/300-VW/</a><br>上面数据集的介绍可以参考：<a href="https://yinguobing.com/facial-landmark-localization-by-deep-learning-data-and-algorithm/" target="_blank" rel="noopener">https://yinguobing.com/facial-landmark-localization-by-deep-learning-data-and-algorithm/</a></p><p>第二类是自己标注的数据集：<br>这部分主要是用标注工具对自己收集到的图片进行标注，我采用自己的标注工具进行标注后，生成的是一个包含68点坐标位置的txt文档，之后要需要通过以下脚本将其转换成公共数据集中类似的pts文件的形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import division</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">from compiler.ast import flatten</span><br><span class="line"></span><br><span class="line">txt_dir = &quot;/Users/camlin_z/Data/68landmark/txt/&quot;</span><br><span class="line">txt_new_dir = &quot;/Users/camlin_z/Data/68landmark/landmark/&quot;</span><br><span class="line"></span><br><span class="line">def trans_label():</span><br><span class="line">    files = os.listdir(txt_dir)</span><br><span class="line">    for file in files:</span><br><span class="line">        flag = file.find(&quot;.&quot;)</span><br><span class="line">        if flag &gt; 0:</span><br><span class="line">            txt_name = file[:flag] + &quot;.pts&quot;</span><br><span class="line">            print txt_name</span><br><span class="line">            line = open(txt_dir + file, &apos;r&apos;)</span><br><span class="line">            for label in line:</span><br><span class="line">                label = label.strip().split()</span><br><span class="line">                label = map(float, label)</span><br><span class="line">                file_new = open(txt_new_dir + txt_name, &apos;w+&apos;)</span><br><span class="line">                file_new.write(&quot;version: 1&quot; + &quot;\n&quot;)</span><br><span class="line">                file_new.write(&quot;n_points: 68&quot; + &quot;\n&quot;)</span><br><span class="line">                file_new.write(&quot;&#123;&quot; + &quot;\n&quot;)</span><br><span class="line">                for i in range(0, 135, 2):</span><br><span class="line">                    file_new.write(str(label[i]) + &quot; &quot; + str(label[i+1]) + &quot;\n&quot;)</span><br><span class="line">                file_new.write(&quot;&#125;&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print file, &quot; not exist!&quot;</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    trans_label()</span><br></pre></td></tr></table></figure><p>通过以上的整理过程，就可以将数据集整理成以下形式：<br><img src="https://img-blog.csdn.net/20180623141024430?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>同时需要将以上数据集分成训练集和测试集两个部分。</p><h2 id="2、数据集的预处理"><a href="#2、数据集的预处理" class="headerlink" title="2、数据集的预处理"></a>2、数据集的预处理</h2><p>准备好上面的五个数据集后，接下来就是对于数据集的一系列处理了，由于特征点的检测是基于检测框检测出来之后，将图像crop出只有人脸的部分，然后再进行特征点的检测过程(因为这样可以大量的减少图像中其他因素的干扰，将神经网络的功能聚焦到特征点检测的任务上面来)，所以需要根据以上数据集中标注的特征点位置来裁剪出一个只有人脸的区域，用于神经网络的训练。</p><p>处理过程主要参考：<br><a href="https://yinguobing.com/facial-landmark-localization-by-deep-learning-data-collate/" target="_blank" rel="noopener">https://yinguobing.com/facial-landmark-localization-by-deep-learning-data-collate/</a><br>但是在图像进行预处理之后，特征点的位置同样也会发生变化，上面作者分享的代码在对图像进行处理之后没有将对应的特征点坐标进行处理，所以我将原始的代码进行改进，同时对特征点坐标和图像进行处理，并生成最终我们网络训练需要的label形式，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">This script shows how to read iBUG pts file and draw all the landmark points on image.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from __future__ import division</span><br><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">from compiler.ast import flatten</span><br><span class="line">import face_detector_image as fd</span><br><span class="line">from lxml import etree, objectify</span><br><span class="line">from compiler.ast import flatten</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line"># 0: test the pts of crop image</span><br><span class="line"># 1: output the crop image</span><br><span class="line">test_flag = 0</span><br><span class="line"># List all the files</span><br><span class="line">filelist_train = [&quot;300W/trainset&quot;, &quot;afw&quot;, &quot;data2&quot;, &quot;data3&quot;, &quot;data4/trainset&quot;,</span><br><span class="line">             &quot;helen/trainset&quot;, &quot;landmark/trainset&quot;, &quot;lfpw/trainset&quot;]</span><br><span class="line">filelist_test = [&quot;300W/testset&quot;, &quot;data4/testset&quot;, &quot;helen/testset&quot;,</span><br><span class="line">             &quot;landmark/testset&quot;, &quot;lfpw/testset&quot;]</span><br><span class="line"></span><br><span class="line">filelist = filelist_train</span><br><span class="line"></span><br><span class="line">def mkr(dr):</span><br><span class="line">    if not os.path.exists(dr):</span><br><span class="line">        os.mkdir(dr)</span><br><span class="line"></span><br><span class="line">def read_points(file_name=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Read points from .pts file.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    points = []</span><br><span class="line">    with open(file_name) as file:</span><br><span class="line">        line_count = 0</span><br><span class="line">        for line in file:</span><br><span class="line">            if &quot;version&quot; in line or &quot;points&quot; in line or &quot;&#123;&quot; in line or &quot;&#125;&quot; in line:</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                loc_x, loc_y = line.strip().split()</span><br><span class="line">                points.append([float(loc_x), float(loc_y)])</span><br><span class="line">                line_count += 1</span><br><span class="line">    return points</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_landmark_point(image, points):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Draw landmark point on image.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for point in points:</span><br><span class="line">        cv2.circle(image, (int(point[0]), int(</span><br><span class="line">            point[1])), 2, (0, 255, 0), -1, cv2.LINE_AA)</span><br><span class="line">    return image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def points_are_valid(points, image):</span><br><span class="line">    &quot;&quot;&quot;Check if all points are in image&quot;&quot;&quot;</span><br><span class="line">    min_box = get_minimal_box(points)</span><br><span class="line">    if box_in_image(min_box, image):</span><br><span class="line">        return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_square_box(box):</span><br><span class="line">    &quot;&quot;&quot;Get the square boxes which are ready for CNN from the boxes&quot;&quot;&quot;</span><br><span class="line">    left_x = box[0]</span><br><span class="line">    top_y = box[1]</span><br><span class="line">    right_x = box[2]</span><br><span class="line">    bottom_y = box[3]</span><br><span class="line"></span><br><span class="line">    box_width = right_x - left_x</span><br><span class="line">    box_height = bottom_y - top_y</span><br><span class="line"></span><br><span class="line">    # Check if box is already a square. If not, make it a square.</span><br><span class="line">    diff = box_height - box_width</span><br><span class="line">    delta = int(abs(diff) / 2)</span><br><span class="line"></span><br><span class="line">    if diff == 0:                   # Already a square.</span><br><span class="line">        return box</span><br><span class="line">    elif diff &gt; 0:                  # Height &gt; width, a slim box.</span><br><span class="line">        left_x -= delta</span><br><span class="line">        right_x += delta</span><br><span class="line">        if diff % 2 == 1:</span><br><span class="line">            right_x += 1</span><br><span class="line">    else:                           # Width &gt; height, a short box.</span><br><span class="line">        top_y -= delta</span><br><span class="line">        bottom_y += delta</span><br><span class="line">        if diff % 2 == 1:</span><br><span class="line">            bottom_y += 1</span><br><span class="line"></span><br><span class="line">    # Make sure box is always square.</span><br><span class="line">    assert ((right_x - left_x) == (bottom_y - top_y)), &apos;Box is not square.&apos;</span><br><span class="line"></span><br><span class="line">    return [left_x, top_y, right_x, bottom_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_minimal_box(points):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Get the minimal bounding box of a group of points.</span><br><span class="line">    The coordinates are also converted to int numbers.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    min_x = int(min([point[0] for point in points]))</span><br><span class="line">    max_x = int(max([point[0] for point in points]))</span><br><span class="line">    min_y = int(min([point[1] for point in points]))</span><br><span class="line">    max_y = int(max([point[1] for point in points]))</span><br><span class="line">    return [min_x, min_y, max_x, max_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def move_box(box, offset):</span><br><span class="line">    &quot;&quot;&quot;Move the box to direction specified by offset&quot;&quot;&quot;</span><br><span class="line">    left_x = box[0] + offset[0]</span><br><span class="line">    top_y = box[1] + offset[1]</span><br><span class="line">    right_x = box[2] + offset[0]</span><br><span class="line">    bottom_y = box[3] + offset[1]</span><br><span class="line">    return [left_x, top_y, right_x, bottom_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def expand_box(square_box, scale_ratio=1.2):</span><br><span class="line">    &quot;&quot;&quot;Scale up the box&quot;&quot;&quot;</span><br><span class="line">    assert (scale_ratio &gt;= 1), &quot;Scale ratio should be greater than 1.&quot;</span><br><span class="line">    delta = int((square_box[2] - square_box[0]) * (scale_ratio - 1) / 2)</span><br><span class="line">    left_x = square_box[0] - delta</span><br><span class="line">    left_y = square_box[1] - delta</span><br><span class="line">    right_x = square_box[2] + delta</span><br><span class="line">    right_y = square_box[3] + delta</span><br><span class="line">    return [left_x, left_y, right_x, right_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def points_in_box(points, box):</span><br><span class="line">    &quot;&quot;&quot;Check if box contains all the points&quot;&quot;&quot;</span><br><span class="line">    minimal_box = get_minimal_box(points)</span><br><span class="line">    return box[0] &lt;= minimal_box[0] and \</span><br><span class="line">        box[1] &lt;= minimal_box[1] and \</span><br><span class="line">        box[2] &gt;= minimal_box[2] and \</span><br><span class="line">        box[3] &gt;= minimal_box[3]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def box_in_image(box, image):</span><br><span class="line">    &quot;&quot;&quot;Check if the box is in image&quot;&quot;&quot;</span><br><span class="line">    rows = image.shape[0]</span><br><span class="line">    cols = image.shape[1]</span><br><span class="line">    return box[0] &gt;= 0 and box[1] &gt;= 0 and box[2] &lt;= cols and box[3] &lt;= rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def box_is_valid(image, points, box):</span><br><span class="line">    &quot;&quot;&quot;Check if box is valid.&quot;&quot;&quot;</span><br><span class="line">    # Box contains all the points.</span><br><span class="line">    points_is_in_box = points_in_box(points, box)</span><br><span class="line"></span><br><span class="line">    # Box is in image.</span><br><span class="line">    box_is_in_image = box_in_image(box, image)</span><br><span class="line"></span><br><span class="line">    # Box is square.</span><br><span class="line">    w_equal_h = (box[2] - box[0]) == (box[3] - box[1])</span><br><span class="line"></span><br><span class="line">    # Return the result.</span><br><span class="line">    return box_is_in_image and points_is_in_box and w_equal_h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_by_shifting(box, rows, cols):</span><br><span class="line">    &quot;&quot;&quot;Method 1: Try to move the box.&quot;&quot;&quot;</span><br><span class="line">    # Face box points.</span><br><span class="line">    left_x = box[0]</span><br><span class="line">    top_y = box[1]</span><br><span class="line">    right_x = box[2]</span><br><span class="line">    bottom_y = box[3]</span><br><span class="line"></span><br><span class="line">    # Check if moving is possible.</span><br><span class="line">    if right_x - left_x &lt;= cols and bottom_y - top_y &lt;= rows:</span><br><span class="line">        if left_x &lt; 0:                  # left edge crossed, move right.</span><br><span class="line">            right_x += abs(left_x)</span><br><span class="line">            left_x = 0</span><br><span class="line">        if right_x &gt; cols:              # right edge crossed, move left.</span><br><span class="line">            left_x -= (right_x - cols)</span><br><span class="line">            right_x = cols</span><br><span class="line">        if top_y &lt; 0:                   # top edge crossed, move down.</span><br><span class="line">            bottom_y += abs(top_y)</span><br><span class="line">            top_y = 0</span><br><span class="line">        if bottom_y &gt; rows:             # bottom edge crossed, move up.</span><br><span class="line">            top_y -= (bottom_y - rows)</span><br><span class="line">            bottom_y = rows</span><br><span class="line"></span><br><span class="line">    return [left_x, top_y, right_x, bottom_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_by_shrinking(box, rows, cols):</span><br><span class="line">    &quot;&quot;&quot;Method 2: Try to shrink the box.&quot;&quot;&quot;</span><br><span class="line">    # Face box points.</span><br><span class="line">    left_x = box[0]</span><br><span class="line">    top_y = box[1]</span><br><span class="line">    right_x = box[2]</span><br><span class="line">    bottom_y = box[3]</span><br><span class="line"></span><br><span class="line">    # The first step would be get the interlaced area.</span><br><span class="line">    if left_x &lt; 0:                  # left edge crossed, set zero.</span><br><span class="line">        left_x = 0</span><br><span class="line">    if right_x &gt; cols:              # right edge crossed, set max.</span><br><span class="line">        right_x = cols</span><br><span class="line">    if top_y &lt; 0:                   # top edge crossed, set zero.</span><br><span class="line">        top_y = 0</span><br><span class="line">    if bottom_y &gt; rows:             # bottom edge crossed, set max.</span><br><span class="line">        bottom_y = rows</span><br><span class="line"></span><br><span class="line">    # Then found out which is larger: the width or height. This will</span><br><span class="line">    # be used to decide in which dimention the size would be shrinked.</span><br><span class="line">    width = right_x - left_x</span><br><span class="line">    height = bottom_y - top_y</span><br><span class="line">    delta = abs(width - height)</span><br><span class="line">    # Find out which dimention should be altered.</span><br><span class="line">    if width &gt; height:                  # x should be altered.</span><br><span class="line">        if left_x != 0 and right_x != cols:     # shrink from center.</span><br><span class="line">            left_x += int(delta / 2)</span><br><span class="line">            right_x -= int(delta / 2) + delta % 2</span><br><span class="line">        elif left_x == 0:                       # shrink from right.</span><br><span class="line">            right_x -= delta</span><br><span class="line">        else:                                   # shrink from left.</span><br><span class="line">            left_x += delta</span><br><span class="line">    else:                               # y should be altered.</span><br><span class="line">        if top_y != 0 and bottom_y != rows:     # shrink from center.</span><br><span class="line">            top_y += int(delta / 2) + delta % 2</span><br><span class="line">            bottom_y -= int(delta / 2)</span><br><span class="line">        elif top_y == 0:                        # shrink from bottom.</span><br><span class="line">            bottom_y -= delta</span><br><span class="line">        else:                                   # shrink from top.</span><br><span class="line">            top_y += delta</span><br><span class="line"></span><br><span class="line">    return [left_x, top_y, right_x, bottom_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_box(box, image, points):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Try to fit the box, make sure it satisfy following conditions:</span><br><span class="line">    - A square.</span><br><span class="line">    - Inside the image.</span><br><span class="line">    - Contains all the points.</span><br><span class="line">    If all above failed, return None.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rows = image.shape[0]</span><br><span class="line">    cols = image.shape[1]</span><br><span class="line"></span><br><span class="line">    # First try to move the box.</span><br><span class="line">    box_moved = fit_by_shifting(box, rows, cols)</span><br><span class="line"></span><br><span class="line">    # If moving faild ,try to shrink.</span><br><span class="line">    if box_is_valid(image, points, box_moved):</span><br><span class="line">        return box_moved</span><br><span class="line">    else:</span><br><span class="line">        box_shrinked = fit_by_shrinking(box, rows, cols)</span><br><span class="line"></span><br><span class="line">    # If shrink failed, return None</span><br><span class="line">    if box_is_valid(image, points, box_shrinked):</span><br><span class="line">        return box_shrinked</span><br><span class="line"></span><br><span class="line">    # Finally, Worst situation.</span><br><span class="line">    print(&quot;Fitting failed!&quot;)</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_valid_box(image, points):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Try to get a valid face box which meets the requirments.</span><br><span class="line">    The function follows these steps:</span><br><span class="line">        1. Try method 1, if failed:</span><br><span class="line">        2. Try method 0, if failed:</span><br><span class="line">        3. Return None</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Try method 1 first.</span><br><span class="line">    def _get_postive_box(raw_boxes, points):</span><br><span class="line">        for box in raw_boxes:</span><br><span class="line">            # Move box down.</span><br><span class="line">            diff_height_width = (box[3] - box[1]) - (box[2] - box[0])</span><br><span class="line">            offset_y = int(abs(diff_height_width / 2))</span><br><span class="line">            box_moved = move_box(box, [0, offset_y])</span><br><span class="line"></span><br><span class="line">            # Make box square.</span><br><span class="line">            square_box = get_square_box(box_moved)</span><br><span class="line"></span><br><span class="line">            # Remove false positive boxes.</span><br><span class="line">            if points_in_box(points, square_box):</span><br><span class="line">                return square_box</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line">    # Try to get a positive box from face detection results.</span><br><span class="line">    _, raw_boxes = fd.get_facebox(image, threshold=0.5)</span><br><span class="line">    positive_box = _get_postive_box(raw_boxes, points)</span><br><span class="line">    if positive_box is not None:</span><br><span class="line">        if box_in_image(positive_box, image) is True:</span><br><span class="line">            return positive_box</span><br><span class="line">        return fit_box(positive_box, image, points)</span><br><span class="line"></span><br><span class="line">    # Method 1 failed, Method 0</span><br><span class="line">    min_box = get_minimal_box(points)</span><br><span class="line">    sqr_box = get_square_box(min_box)</span><br><span class="line">    epd_box = expand_box(sqr_box)</span><br><span class="line">    if box_in_image(epd_box, image) is True:</span><br><span class="line">        return epd_box</span><br><span class="line">    return fit_box(epd_box, image, points)</span><br><span class="line"></span><br><span class="line">def get_new_pts(facebox, raw_points, label_txt, image_file, flag, ratio_w, ratio_h):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    generate a new pts file according to face box</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = facebox[0]</span><br><span class="line">    y = facebox[1]</span><br><span class="line">    # print x, y</span><br><span class="line">    new_point = []</span><br><span class="line">    label_pts = flatten(raw_points)</span><br><span class="line">    # print label_pts</span><br><span class="line"></span><br><span class="line">    label_txt.write(flag + image_file + &quot;.jpg &quot;)</span><br><span class="line">    for i in range(0, 135, 2):</span><br><span class="line">        if i != 134:</span><br><span class="line">            x_temp = int((label_pts[i] - x) * ratio_w )</span><br><span class="line">            y_temp = int((label_pts[i + 1] - y) * ratio_h)</span><br><span class="line">            new_point.append([x_temp, y_temp])</span><br><span class="line">            label_txt.write(str(x_temp) + &quot; &quot; + str(y_temp) + &quot; &quot;)</span><br><span class="line">        else:</span><br><span class="line">            x_temp = int((label_pts[i] - x) * ratio_w)</span><br><span class="line">            y_temp = int((label_pts[i + 1] - y) * ratio_h)</span><br><span class="line">            new_point.append([x_temp, y_temp])</span><br><span class="line">            label_txt.write(str(x_temp) + &quot; &quot; + str(y_temp))</span><br><span class="line">    label_txt.write(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    # print new_point</span><br><span class="line">    return new_point</span><br><span class="line"></span><br><span class="line">def preview(point_file, test_flag, bbox_new_file):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Preview points on image.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Read the points from file.</span><br><span class="line">    raw_points = read_points(point_file)</span><br><span class="line"></span><br><span class="line">    # Safe guard, make sure point importing goes well.</span><br><span class="line">    assert len(raw_points) == 68, &quot;The landmarks should contain 68 points.&quot;</span><br><span class="line"></span><br><span class="line">    # Read the image.</span><br><span class="line">    head, tail = os.path.split(point_file)</span><br><span class="line">    image_file = tail.split(&apos;.&apos;)[-2]</span><br><span class="line">    img_jpeg = os.path.join(head, image_file + &quot;.jpeg&quot;)</span><br><span class="line">    img_jpg = os.path.join(head, image_file + &quot;.jpg&quot;)</span><br><span class="line">    img_png = os.path.join(head, image_file + &quot;.png&quot;)</span><br><span class="line">    if os.path.exists(img_jpg):</span><br><span class="line">        img = cv2.imread(img_jpg)</span><br><span class="line">        img_file = img_jpg</span><br><span class="line">    elif os.path.exists(img_jpeg):</span><br><span class="line">        img = cv2.imread(img_jpeg)</span><br><span class="line">        img_file = img_jpeg</span><br><span class="line">    else:</span><br><span class="line">        img = cv2.imread(img_png)</span><br><span class="line">        img_file = img_png</span><br><span class="line">    print image_file</span><br><span class="line">    # Fast check: all points are in image.</span><br><span class="line">    if points_are_valid(raw_points, img) is False:</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line">    # Get the valid facebox.</span><br><span class="line">    facebox = get_valid_box(img, raw_points)</span><br><span class="line">    if facebox is None:</span><br><span class="line">        print(&quot;Using minimal box.&quot;)</span><br><span class="line">        facebox = get_minimal_box(raw_points)</span><br><span class="line"></span><br><span class="line">    # Extract valid image area.</span><br><span class="line">    face_area = img[facebox[1]:facebox[3],</span><br><span class="line">                    facebox[0]: facebox[2]]</span><br><span class="line"></span><br><span class="line">    rw = 1</span><br><span class="line">    rh = 1</span><br><span class="line">    # Check if resize is needed.</span><br><span class="line">    width = facebox[2] - facebox[0]</span><br><span class="line">    height = facebox[3] - facebox[1]</span><br><span class="line">    print width,height</span><br><span class="line">    if width != height:</span><br><span class="line">        print(&apos;opps!&apos;, width, height)</span><br><span class="line">    if (width != 224) or (height != 224):</span><br><span class="line">        face_area = cv2.resize(face_area, (224, 224))</span><br><span class="line">        rw = 224 / width</span><br><span class="line">        rh = 224 / height</span><br><span class="line"></span><br><span class="line">    # generate a new pts file according to facebox</span><br><span class="line">    new_point = get_new_pts(facebox, raw_points, label_txt,</span><br><span class="line">                            image_file, flag, rw, rh)</span><br><span class="line"></span><br><span class="line">    if test_flag == 0:</span><br><span class="line">        # verify the crop image whether match to 68 point or not</span><br><span class="line">        face_area = draw_landmark_point(face_area, new_point)</span><br><span class="line">        cv2.imwrite(DATA_TEST_DST + image_file + &quot;.jpg&quot;, face_area)</span><br><span class="line">    else:</span><br><span class="line">        cv2.imwrite(DATA_DST + image_file + &quot;.jpg&quot;, face_area)</span><br><span class="line"></span><br><span class="line">    # Show the result.</span><br><span class="line">    cv2.imshow(&quot;Crop face&quot;, face_area)</span><br><span class="line">    if cv2.waitKey(10) == 27:</span><br><span class="line">        cv2.waitKey()</span><br><span class="line"></span><br><span class="line">    # # Show whole image in window.</span><br><span class="line">    # width, height = img.shape[:2]</span><br><span class="line">    # max_height = 640</span><br><span class="line">    # if height &gt; max_height:</span><br><span class="line">    #     img = cv2.resize(</span><br><span class="line">    #         img, (max_height, int(width * max_height / height)))</span><br><span class="line">    # cv2.imshow(&quot;preview&quot;, img)</span><br><span class="line">    # cv2.waitKey()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The main entrance</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for file_string in filelist:</span><br><span class="line"></span><br><span class="line">        root = &quot;/Users/camlin_z/Data/data/&quot;</span><br><span class="line">        # 图像存储的路劲</span><br><span class="line">        DATA_DIR = root + file_string + &quot;/&quot;</span><br><span class="line">        # crop之后图像存储的路劲</span><br><span class="line">        DATA_DST = root + file_string + &quot;_crop/&quot;</span><br><span class="line">        # 存储将转换后的坐标画在crop之后的图像的路径，用于验证坐标的转换是否出现错误</span><br><span class="line">        DATA_TEST_DST = root + file_string + &quot;_pts/&quot;</span><br><span class="line">        # 最终生成网络训练需要的label的txt文件的路径</span><br><span class="line">        point_new_file = root + file_string + &quot;.txt&quot;</span><br><span class="line">        flag = file_string + &quot;/&quot;</span><br><span class="line"></span><br><span class="line">        pts_file_list = []</span><br><span class="line">        for file_path, _, file_names in os.walk(DATA_DIR):</span><br><span class="line">            for file_name in file_names:</span><br><span class="line">                if file_name.split(&quot;.&quot;)[-1] in [&quot;pts&quot;]:</span><br><span class="line">                    pts_file_list.append(os.path.join(file_path, file_name))</span><br><span class="line"></span><br><span class="line">        label_txt = open(point_new_file, &apos;w&apos;)</span><br><span class="line">        mkr(DATA_DST)</span><br><span class="line">        mkr(DATA_TEST_DST)</span><br><span class="line"></span><br><span class="line">        # Show the image one by one.</span><br><span class="line">        for file_name in pts_file_list:</span><br><span class="line">            preview(file_name, test_flag, bbox_new_file)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="3、数据增强"><a href="#3、数据增强" class="headerlink" title="3、数据增强"></a>3、数据增强</h2><p>由于以上数据集总共加起来只有五千张左右，对于需要大数据训练的神经网络显然是不够的，所以这里考虑对上面的数据集进行数据增强的操作，由于项目的需要，所以主要是对原来的数据集进行旋转的数据增强。</p><p>以上的旋转主要可以分为两种策略：<br>1、将原始图像直接保持原始大小进行旋转<br>2、将原始图像旋转后将生成的图像的四个边向外扩充，使得生成的图像不会切掉原始图像的四个边。</p><p>主要分为±15°，±30°，±45°，±60°四种旋转类型，在进行数据增强的过程中，主要由三个问题需要解决：<br>（1）旋转后产生的黑色区域可能影响卷积学习特征<br>（2）利用以上产生的只有人脸的图像进行旋转可能将之前标注的特征点旋转到图像的外面，导致某些特征点损失<br>（3）旋转后特征点坐标的生成</p><p>针对第一个问题：<br>可以参考：<a href="https://blog.csdn.net/guyuealian/article/details/77993410中的方法，对图像的黑色区域利用其边缘值的二次插值来进行填充，但是上面的处理过程可能会产生一些奇怪的边缘效果，如下图所示：" target="_blank" rel="noopener">https://blog.csdn.net/guyuealian/article/details/77993410中的方法，对图像的黑色区域利用其边缘值的二次插值来进行填充，但是上面的处理过程可能会产生一些奇怪的边缘效果，如下图所示：</a><br><img src="https://img-blog.csdn.net/20180623154456210?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180623154509518?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>有担心上面这些奇怪的特征是不是会影响最终卷积网络的学习结果，但是暂时还没有找到合适的解决方法，有大牛知道，感谢留言。</p><p>针对第二个问题：<br>可以参考：<a href="https://www.oschina.net/translate/opencv-rotation中的代码，将图像旋转后根据其旋转后产生的新的长宽来存储图片，保证最终生成的旋转后的图片不会去掉原始图片的四个角，上面展示的图片就是利用这种方法进行旋转-60°之后的结果。" target="_blank" rel="noopener">https://www.oschina.net/translate/opencv-rotation中的代码，将图像旋转后根据其旋转后产生的新的长宽来存储图片，保证最终生成的旋转后的图片不会去掉原始图片的四个角，上面展示的图片就是利用这种方法进行旋转-60°之后的结果。</a></p><p>针对第三个问题：<br>可以参考：<a href="https://blog.csdn.net/songzitea/article/details/51043743中的解释来进行转换。" target="_blank" rel="noopener">https://blog.csdn.net/songzitea/article/details/51043743中的解释来进行转换。</a><br>还有一篇写的比较好的博文可以阅读：<a href="https://charlesnord.github.io/2017/04/01/rotation/" target="_blank" rel="noopener">https://charlesnord.github.io/2017/04/01/rotation/</a></p><p>将以上问题一一解决之后，由于采用策略二进行旋转时会产生上面图片所示的大块奇怪的特征，但是策略一则不会产生那么大块的奇怪的特征，所以我对于旋转的数据增强的整体逻辑如下：<br><img src="https://img-blog.csdn.net/2018062316063612?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>按照上面的处理过程，可以写出下面的代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line">#-*- coding: UTF-8 -*-</span><br><span class="line"></span><br><span class="line">from __future__ import division</span><br><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">filelist = [&quot;300W/trainset&quot;, &quot;afw/trainset&quot;, &quot;data2/trainset&quot;, &quot;data3/trainset&quot;,</span><br><span class="line">             &quot;data4/trainset&quot;, &quot;helen/trainset&quot;, &quot;landmark/trainset&quot;, &quot;lfpw/trainset&quot;,</span><br><span class="line">            &quot;300W/testset&quot;, &quot;afw/testset&quot;, &quot;data2/testset&quot;, &quot;data3/testset&quot;,</span><br><span class="line">            &quot;data4/testset&quot;, &quot;helen/testset&quot;, &quot;landmark/testset&quot;, &quot;lfpw/testset&quot;]</span><br><span class="line"></span><br><span class="line">img_dir = &quot;/Users/camlin_z/Data/data_output/&quot;</span><br><span class="line">angles = [15, 30, 45, 60]</span><br><span class="line"></span><br><span class="line">def mkr(dr):</span><br><span class="line">    if not os.path.exists(dr):</span><br><span class="line">        os.mkdir(dr)</span><br><span class="line"></span><br><span class="line">def read_points(file_name=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Read points from .pts file.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    points = []</span><br><span class="line">    with open(file_name) as file:</span><br><span class="line">        line_count = 0</span><br><span class="line">        for line in file:</span><br><span class="line">            if &quot;version&quot; in line or &quot;points&quot; in line or &quot;&#123;&quot; in line or &quot;&#125;&quot; in line:</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                loc_x, loc_y = line.strip().split()</span><br><span class="line">                points.append([float(loc_x), float(loc_y)])</span><br><span class="line">                line_count += 1</span><br><span class="line">    return points</span><br><span class="line"></span><br><span class="line">def draw_save_landmark(image, points, dst):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Draw landmark point on image.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for point in points:</span><br><span class="line">        cv2.circle(image, (int(point[0]), int(</span><br><span class="line">            point[1])), 2, (0, 255, 0), -1, cv2.LINE_AA)</span><br><span class="line">    cv2.imwrite(dst, image)</span><br><span class="line"></span><br><span class="line">def trans_label(txt, label):</span><br><span class="line">    file_new = open(txt, &apos;w+&apos;)</span><br><span class="line">    file_new.write(&quot;version: 1&quot; + &quot;\n&quot;)</span><br><span class="line">    file_new.write(&quot;n_points: 68&quot; + &quot;\n&quot;)</span><br><span class="line">    file_new.write(&quot;&#123;&quot; + &quot;\n&quot;)</span><br><span class="line">    for point in label:</span><br><span class="line">        file_new.write(str(point[0]) + &quot; &quot; + str(point[1]) + &quot;\n&quot;)</span><br><span class="line">    file_new.write(&quot;&#125;&quot;)</span><br><span class="line"></span><br><span class="line">def rotate_with_adjust_size(img, theta):</span><br><span class="line">    img_raw = cv2.imread(img)</span><br><span class="line">    height, width = img_raw.shape[:2]</span><br><span class="line">    center = (width / 2, height / 2)</span><br><span class="line">    scale = 1</span><br><span class="line">    rangle = np.deg2rad(theta)  # angle in radians</span><br><span class="line">    # now calculate new image width and height</span><br><span class="line">    nw = (abs(np.sin(rangle) * height) + abs(np.cos(rangle) * width)) * scale</span><br><span class="line">    nh = (abs(np.cos(rangle) * height) + abs(np.sin(rangle) * width)) * scale</span><br><span class="line">    # ask OpenCV for the rotation matrix</span><br><span class="line">    rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), theta, scale)</span><br><span class="line">    # calculate the move from the old center to the new center combined</span><br><span class="line">    # with the rotation</span><br><span class="line">    rot_move = np.dot(rot_mat, np.array([(nw - width) * 0.5, (nh - height) * 0.5, 0]))</span><br><span class="line">    # the move only affects the translation, so update the translation</span><br><span class="line">    # part of the transform</span><br><span class="line">    rot_mat[0, 2] += rot_move[0]</span><br><span class="line">    rot_mat[1, 2] += rot_move[1]</span><br><span class="line">    img_rotate = cv2.warpAffine(img_raw, rot_mat, (int(np.math.ceil(nw)), int(np.math.ceil(nh))), cv2.INTER_LANCZOS4,</span><br><span class="line">                          cv2.BORDER_REFLECT, 1)</span><br><span class="line">    offset_w = (nw - width) / 2</span><br><span class="line">    offset_h = (nh - height) / 2</span><br><span class="line"></span><br><span class="line">    img_rotate = cv2.resize(img_rotate, (224, 224))</span><br><span class="line">    rw = 224 / nw</span><br><span class="line">    rh = 224 / nh</span><br><span class="line">    return img_rotate, center, offset_w, offset_h, rw, rh</span><br><span class="line"></span><br><span class="line">def rotate_with_original_size(img, theta):</span><br><span class="line">    img_raw = cv2.imread(img)</span><br><span class="line">    height, width = img_raw.shape[:2]</span><br><span class="line">    center = (width / 2, height / 2)</span><br><span class="line">    rot_mat = cv2.getRotationMatrix2D(center, theta, 1)</span><br><span class="line">    img_rotate = cv2.warpAffine(img_raw, rot_mat, (width, height), cv2.INTER_LANCZOS4,</span><br><span class="line">                          cv2.BORDER_REFLECT, 1)</span><br><span class="line">    return img_rotate, center</span><br><span class="line"></span><br><span class="line">def rotate_pts_original_size(img, points, center, angle):</span><br><span class="line">    flag = 0</span><br><span class="line">    new_points = []</span><br><span class="line">    height, width = img.shape[:2]</span><br><span class="line">    theta = np.deg2rad(angle)</span><br><span class="line">    for i in range(len(points)):</span><br><span class="line">        [x_raw, y_raw] = points[i]</span><br><span class="line">        y_raw = height - y_raw</span><br><span class="line">        (center_x, center_y) = center</span><br><span class="line">        center_y = height - center_y</span><br><span class="line">        x = round((x_raw - center_x) * math.cos(theta) - (y_raw - center_y) * math.sin(theta) + center_x)</span><br><span class="line">        y = round((x_raw - center_x) * math.sin(theta) + (y_raw - center_y) * math.cos(theta) + center_y)</span><br><span class="line">        x = int(x)</span><br><span class="line">        y = int(height - y)</span><br><span class="line">        if x &lt;= 0 or y &lt;= 0:</span><br><span class="line">            flag = 1</span><br><span class="line">            break</span><br><span class="line">        new_points.append([x, y])</span><br><span class="line">    return new_points, flag</span><br><span class="line"></span><br><span class="line">def rotate_pts_adjust_size(img, points, center, angle, offset_w, offset_h, rate_w, rate_h):</span><br><span class="line">    new_points = []</span><br><span class="line">    height, width = img.shape[:2]</span><br><span class="line">    theta = np.deg2rad(angle)</span><br><span class="line">    for i in range(len(points)):</span><br><span class="line">        [x_raw, y_raw] = points[i]</span><br><span class="line">        y_raw = height - y_raw</span><br><span class="line">        (center_x, center_y) = center</span><br><span class="line">        center_y = height - center_y</span><br><span class="line">        x = round((x_raw - center_x) * math.cos(theta) - (y_raw - center_y) * math.sin(theta) + center_x)</span><br><span class="line">        y = round((x_raw - center_x) * math.sin(theta) + (y_raw - center_y) * math.cos(theta) + center_y)</span><br><span class="line">        x = int((x + offset_w) * rate_w)</span><br><span class="line">        y = int((height - y + offset_h) * rate_h)</span><br><span class="line">        new_points.append([x, y])</span><br><span class="line">    return new_points</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    for angle in angles:</span><br><span class="line">        for file_string in filelist:</span><br><span class="line">            out_dir = os.path.join(img_dir, file_string + &quot;_&quot; + str(abs(angle)))</span><br><span class="line">            out_verify_dir = out_dir + &quot;/out/&quot;</span><br><span class="line">            mkr(out_dir)</span><br><span class="line">            mkr(out_verify_dir)</span><br><span class="line">            for file_path, _, file_names in os.walk(os.path.join(img_dir, file_string)):</span><br><span class="line">                for file_name in file_names:</span><br><span class="line">                    if file_name.split(&quot;.&quot;)[-1] in [&quot;jpg&quot;, &quot;png&quot;, &quot;jpeg&quot;]:</span><br><span class="line">                        print file_name</span><br><span class="line">                        # 读取图像路径</span><br><span class="line">                        img_file_path = os.path.join(img_dir, file_string, file_name)</span><br><span class="line">                        # 读取pts文件路径</span><br><span class="line">                        pts_file_name = file_name.split(&quot;.&quot;)[0] + &quot;.pts&quot;</span><br><span class="line">                        pts_file_path = os.path.join(img_dir, file_string, pts_file_name)</span><br><span class="line">                        # 写入pts文件路径</span><br><span class="line">                        pts_new_dir = os.path.join(out_dir, pts_file_name)</span><br><span class="line"></span><br><span class="line">                        ############ 原始大小旋转图像和点 ############</span><br><span class="line">                        # 随机生成指定的旋转角度</span><br><span class="line">                        if angle == 15:</span><br><span class="line">                            theta_pos = np.random.randint(0, 15)</span><br><span class="line">                            theta_neg = np.random.randint(-15, 0)</span><br><span class="line">                        elif angle == 30:</span><br><span class="line">                            theta_pos = np.random.randint(15, 30)</span><br><span class="line">                            theta_neg = np.random.randint(-30, -15)</span><br><span class="line">                        elif angle == 45:</span><br><span class="line">                            theta_pos = np.random.randint(30, 45)</span><br><span class="line">                            theta_neg = np.random.randint(-45, -30)</span><br><span class="line">                        else:</span><br><span class="line">                            theta_pos = np.random.randint(45, 60)</span><br><span class="line">                            theta_neg = np.random.randint(-60, -45)</span><br><span class="line"></span><br><span class="line">                        arr = np.random.randint(0, 2)</span><br><span class="line">                        if arr == 0:</span><br><span class="line">                            theta = theta_pos</span><br><span class="line">                        else:</span><br><span class="line">                            theta = theta_neg</span><br><span class="line">                        print theta</span><br><span class="line">                        # 旋转图像</span><br><span class="line">                        img, center = rotate_with_original_size(img_file_path, theta)</span><br><span class="line">                        # 调整图像对应坐标点</span><br><span class="line">                        points = read_points(pts_file_path)</span><br><span class="line">                        new_points, flag = rotate_pts_original_size(img, points, center, theta)</span><br><span class="line"></span><br><span class="line">                        # 根据以上flag判断产生的点是否超出图像位置</span><br><span class="line">                        # 如果超出，则使用调整大小的方式旋转</span><br><span class="line">                        if flag == 1:</span><br><span class="line">                            print img_file_path, &quot;warning!!!&quot;</span><br><span class="line">                            img, center, offset_w, offset_h, rw, rh = rotate_with_adjust_size(img_file_path, theta)</span><br><span class="line">                            new_points = rotate_pts_adjust_size(img, points, center, theta, offset_w, offset_h,rw, rh)</span><br><span class="line"></span><br><span class="line">                        # 将图像写入输出文件夹</span><br><span class="line">                        cv2.imwrite(os.path.join(out_dir, file_name), img)</span><br><span class="line">                        # 将pts重新写入输出文件夹</span><br><span class="line">                        trans_label(pts_new_dir, new_points)</span><br><span class="line">                        # 将坐标点画到图像上验证位置是否正确</span><br><span class="line">                        out_img_path = os.path.join(out_verify_dir, file_name)</span><br><span class="line">                        draw_save_landmark(img, new_points, out_img_path)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>以上过程处理完成后，就完成了所有的数据预处理过程了。</p><h1 id="二、网络模型的构造"><a href="#二、网络模型的构造" class="headerlink" title="二、网络模型的构造"></a>二、网络模型的构造</h1><p>由于caffe的图片输入层只是支持一个标签的输入，所以本文中的caffe的iamge data layer经过了一定程度的修改，使其可以接受136个label值的输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br></pre></td><td class="code"><pre><span class="line">#ifdef USE_OPENCV</span><br><span class="line">#include &lt;opencv2/core/core.hpp&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;fstream&gt;  // NOLINT(readability/streams)</span><br><span class="line">#include &lt;iostream&gt;  // NOLINT(readability/streams)</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;utility&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line"></span><br><span class="line">#include &quot;caffe/data_transformer.hpp&quot;</span><br><span class="line">#include &quot;caffe/layers/base_data_layer.hpp&quot;</span><br><span class="line">#include &quot;caffe/layers/image_data_layer.hpp&quot;</span><br><span class="line">#include &quot;caffe/util/benchmark.hpp&quot;</span><br><span class="line">#include &quot;caffe/util/io.hpp&quot;</span><br><span class="line">#include &quot;caffe/util/math_functions.hpp&quot;</span><br><span class="line">#include &quot;caffe/util/rng.hpp&quot;</span><br><span class="line"></span><br><span class="line">namespace caffe &#123;</span><br><span class="line"></span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">ImageDataLayer&lt;Dtype&gt;::~ImageDataLayer&lt;Dtype&gt;() &#123;</span><br><span class="line">  this-&gt;StopInternalThread();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">int ImageDataLayer&lt;Dtype&gt;::Rand(int n) &#123;</span><br><span class="line">  if (n &lt; 1) return 1;</span><br><span class="line">  caffe::rng_t* rng =</span><br><span class="line">      static_cast&lt;caffe::rng_t*&gt;(prefetch_rng_-&gt;generator());</span><br><span class="line">  return ((*rng)() % n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void ImageDataLayer&lt;Dtype&gt;::DataLayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  const int new_height = this-&gt;layer_param_.image_data_param().new_height();</span><br><span class="line">  const int new_width  = this-&gt;layer_param_.image_data_param().new_width();</span><br><span class="line">  const bool is_color  = this-&gt;layer_param_.image_data_param().is_color();</span><br><span class="line">  const bool shuffleflag = this-&gt;layer_param_.image_data_param().shuffle();</span><br><span class="line">  string root_folder = this-&gt;layer_param_.image_data_param().root_folder();</span><br><span class="line"></span><br><span class="line">  CHECK((new_height == 0 &amp;&amp; new_width == 0) ||</span><br><span class="line">      (new_height &gt; 0 &amp;&amp; new_width &gt; 0)) &lt;&lt; &quot;Current implementation requires &quot;</span><br><span class="line">      &quot;new_height and new_width to be set at the same time.&quot;;</span><br><span class="line">  // Read the file with filenames and labels</span><br><span class="line">  const string&amp; source = this-&gt;layer_param_.image_data_param().source();</span><br><span class="line">  LOG(INFO) &lt;&lt; &quot;Opening file &quot; &lt;&lt; source;</span><br><span class="line">  std::ifstream infile(source.c_str());</span><br><span class="line">  string line;</span><br><span class="line">  int pos;</span><br><span class="line">  int label_dim = 0;</span><br><span class="line">  bool gfirst = true;</span><br><span class="line">  int rd = shuffleflag?4:0;</span><br><span class="line">  while (std::getline(infile, line)) &#123;</span><br><span class="line">if(line.find_last_of(&apos; &apos;)==line.size()-2) line.erase(line.find_last_not_of(&apos; &apos;)-1);</span><br><span class="line">        pos = line.find_first_of(&apos; &apos;);</span><br><span class="line">string str = line.substr(0, pos);</span><br><span class="line">int p0 = pos + 1;</span><br><span class="line">vector&lt;float&gt; vl;</span><br><span class="line">while (pos != -1)&#123;</span><br><span class="line">pos = line.find_first_of(&apos; &apos;, p0);</span><br><span class="line">vl.push_back(atof(line.substr(p0, pos).c_str()));</span><br><span class="line">p0 = pos + 1;</span><br><span class="line">&#125;</span><br><span class="line">if (shuffleflag) &#123;</span><br><span class="line">float minx = vl[0];</span><br><span class="line">float maxx = minx;</span><br><span class="line">float miny = vl[1];</span><br><span class="line">float maxy = miny;</span><br><span class="line">for (int i = 2; i &lt; vl.size(); i += 2)&#123;</span><br><span class="line">if (vl[i] &lt; minx) minx = vl[i];</span><br><span class="line">else if (vl[i] &gt; maxx) maxx = vl[i];</span><br><span class="line">if (vl[i + 1] &lt; miny) miny = vl[i + 1];</span><br><span class="line">else if (vl[i + 1] &gt; maxy) maxy = vl[i + 1];</span><br><span class="line">&#125;</span><br><span class="line">vl.push_back(minx);</span><br><span class="line">vl.push_back(maxx + 1);</span><br><span class="line">vl.push_back(miny);</span><br><span class="line">vl.push_back(maxy + 1);</span><br><span class="line">&#125;</span><br><span class="line">if (gfirst)&#123;</span><br><span class="line">label_dim = vl.size();</span><br><span class="line">gfirst = false;</span><br><span class="line">LOG(INFO) &lt;&lt; &quot;label dim: &quot; &lt;&lt; label_dim - rd;</span><br><span class="line">//LOG(INFO) &lt;&lt; line;</span><br><span class="line">&#125;</span><br><span class="line">CHECK_EQ(vl.size(), label_dim)  &lt;&lt; &quot;label dim not match in: &quot; &lt;&lt; lines_.size()&lt;&lt;&quot;, &quot;&lt;&lt;lines_[lines_.size()-1].first;</span><br><span class="line">lines_.push_back(std::make_pair(str, vl));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  CHECK(!lines_.empty()) &lt;&lt; &quot;File is empty&quot;;</span><br><span class="line"></span><br><span class="line">  if (shuffleflag) &#123;</span><br><span class="line">    // randomly shuffle data</span><br><span class="line">    LOG(INFO) &lt;&lt; &quot;Shuffling data &amp; randomly crop image&quot;;</span><br><span class="line">    const unsigned int prefetch_rng_seed = caffe_rng_rand();</span><br><span class="line">    prefetch_rng_.reset(new Caffe::RNG(prefetch_rng_seed));</span><br><span class="line">    ShuffleImages();</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    if (this-&gt;phase_ == TRAIN &amp;&amp; Caffe::solver_rank() &gt; 0 &amp;&amp;</span><br><span class="line">        this-&gt;layer_param_.image_data_param().rand_skip() == 0) &#123;</span><br><span class="line">      LOG(WARNING) &lt;&lt; &quot;Shuffling or skipping recommended for multi-GPU&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  LOG(INFO) &lt;&lt; &quot;A total of &quot; &lt;&lt; lines_.size() &lt;&lt; &quot; images.&quot;;</span><br><span class="line"></span><br><span class="line">  lines_id_ = 0;</span><br><span class="line">  // Check if we would need to randomly skip a few data points</span><br><span class="line">  if (this-&gt;layer_param_.image_data_param().rand_skip()) &#123;</span><br><span class="line">    unsigned int skip = caffe_rng_rand() %</span><br><span class="line">        this-&gt;layer_param_.image_data_param().rand_skip();</span><br><span class="line">    LOG(INFO) &lt;&lt; &quot;Skipping first &quot; &lt;&lt; skip &lt;&lt; &quot; data points.&quot;;</span><br><span class="line">    CHECK_GT(lines_.size(), skip) &lt;&lt; &quot;Not enough points to skip&quot;;</span><br><span class="line">    lines_id_ = skip;</span><br><span class="line">  &#125;</span><br><span class="line">  // Read an image, and use it to initialize the top blob.</span><br><span class="line">  cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</span><br><span class="line">                                    0, 0, is_color);</span><br><span class="line">  CHECK(cv_img.data) &lt;&lt; &quot;Could not load &quot; &lt;&lt; lines_[lines_id_].first;</span><br><span class="line">  // Use data_transformer to infer the expected blob shape from a cv_image.</span><br><span class="line">  vector&lt;int&gt; top_shape(4);</span><br><span class="line">  top_shape[0] = 1;</span><br><span class="line">  top_shape[1] = cv_img.channels();</span><br><span class="line">  top_shape[2] = shuffleflag ? new_height : cv_img.rows;</span><br><span class="line">  top_shape[3] = shuffleflag ? new_width : cv_img.cols;</span><br><span class="line">  this-&gt;transformed_data_.Reshape(top_shape);</span><br><span class="line">  // Reshape prefetch_data and top[0] according to the batch_size.</span><br><span class="line">  const int batch_size = this-&gt;layer_param_.image_data_param().batch_size();</span><br><span class="line">  CHECK_GT(batch_size, 0) &lt;&lt; &quot;Positive batch size required&quot;;</span><br><span class="line">  top_shape[0] = batch_size;</span><br><span class="line">  for (int i = 0; i &lt; this-&gt;prefetch_.size(); ++i) &#123;</span><br><span class="line">    this-&gt;prefetch_[i]-&gt;data_.Reshape(top_shape);</span><br><span class="line">  &#125;</span><br><span class="line">  top[0]-&gt;Reshape(top_shape);</span><br><span class="line"></span><br><span class="line">  LOG(INFO) &lt;&lt; &quot;output data size: &quot; &lt;&lt; top[0]-&gt;num() &lt;&lt; &quot;,&quot;</span><br><span class="line">      &lt;&lt; top[0]-&gt;channels() &lt;&lt; &quot;,&quot; &lt;&lt; top[0]-&gt;height() &lt;&lt; &quot;,&quot;</span><br><span class="line">      &lt;&lt; top[0]-&gt;width();</span><br><span class="line">  // label</span><br><span class="line">  vector&lt;int&gt; label_shape(2, batch_size);</span><br><span class="line">  label_shape[1] = label_dim-rd;</span><br><span class="line">  top[1]-&gt;Reshape(label_shape);</span><br><span class="line">  for (int i = 0; i &lt; this-&gt;prefetch_.size(); ++i) &#123;</span><br><span class="line">    this-&gt;prefetch_[i]-&gt;label_.Reshape(label_shape);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void ImageDataLayer&lt;Dtype&gt;::ShuffleImages() &#123;</span><br><span class="line">  caffe::rng_t* prefetch_rng =</span><br><span class="line">      static_cast&lt;caffe::rng_t*&gt;(prefetch_rng_-&gt;generator());</span><br><span class="line">  shuffle(lines_.begin(), lines_.end(), prefetch_rng);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// This function is called on prefetch thread</span><br><span class="line">template &lt;typename Dtype&gt;</span><br><span class="line">void ImageDataLayer&lt;Dtype&gt;::load_batch(Batch&lt;Dtype&gt;* batch) &#123;</span><br><span class="line">  CPUTimer batch_timer;</span><br><span class="line">  batch_timer.Start();</span><br><span class="line">  double read_time = 0;</span><br><span class="line">  double trans_time = 0;</span><br><span class="line">  CPUTimer timer;</span><br><span class="line">  CHECK(batch-&gt;data_.count());</span><br><span class="line">  CHECK(this-&gt;transformed_data_.count());</span><br><span class="line">  ImageDataParameter image_data_param = this-&gt;layer_param_.image_data_param();</span><br><span class="line">  const int batch_size = image_data_param.batch_size();</span><br><span class="line">  const float rate_height = this-&gt;layer_param_.image_data_param().rate_height();</span><br><span class="line">  const float rate_width = this-&gt;layer_param_.image_data_param().rate_width();</span><br><span class="line">  const bool is_color = image_data_param.is_color();</span><br><span class="line">  const bool shuffleflag = this-&gt;layer_param_.image_data_param().shuffle();</span><br><span class="line">  string root_folder = image_data_param.root_folder();</span><br><span class="line"></span><br><span class="line">  // Reshape according to the first image of each batch</span><br><span class="line">  // on single input batches allows for inputs of varying dimension.</span><br><span class="line">  cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</span><br><span class="line">      0, 0, is_color);</span><br><span class="line">  CHECK(cv_img.data) &lt;&lt; &quot;Could not load &quot; &lt;&lt; lines_[lines_id_].first;</span><br><span class="line">  const int new_height = shuffleflag ? image_data_param.new_height() : cv_img.rows;</span><br><span class="line">  const int new_width = shuffleflag ? image_data_param.new_width() : cv_img.cols;</span><br><span class="line">  // Use data_transformer to infer the expected blob shape from a cv_img.</span><br><span class="line">  vector&lt;int&gt; top_shape(4);</span><br><span class="line">  top_shape[0] = 1;</span><br><span class="line">  top_shape[1] = cv_img.channels();</span><br><span class="line">  top_shape[2] = new_height;</span><br><span class="line">  top_shape[3] = new_width;</span><br><span class="line">  this-&gt;transformed_data_.Reshape(top_shape);</span><br><span class="line">  // Reshape batch according to the batch_size.</span><br><span class="line">  top_shape[0] = batch_size;</span><br><span class="line">  batch-&gt;data_.Reshape(top_shape);</span><br><span class="line">  vector&lt;int&gt; top_shape1(4);</span><br><span class="line">  top_shape1[0] = batch_size;</span><br><span class="line">  top_shape1[1] = shuffleflag ? lines_[0].second.size() - 4 : lines_[0].second.size();</span><br><span class="line">  top_shape1[2] = 1;</span><br><span class="line">  top_shape1[3] = 1;</span><br><span class="line">  batch-&gt;label_.Reshape(top_shape1);</span><br><span class="line"></span><br><span class="line">  Dtype* prefetch_data = batch-&gt;data_.mutable_cpu_data();</span><br><span class="line">  Dtype* prefetch_label = batch-&gt;label_.mutable_cpu_data();</span><br><span class="line"></span><br><span class="line">  // datum scales</span><br><span class="line">  const int lines_size = lines_.size();</span><br><span class="line">  const float dh_2 = (new_height - 1)*0.5;</span><br><span class="line">  const float dw_2 = (new_width - 1)*0.5;</span><br><span class="line">  for (int item_id = 0; item_id &lt; batch_size; ++item_id) &#123;</span><br><span class="line">    // get a blob</span><br><span class="line">    timer.Start();</span><br><span class="line">    CHECK_GT(lines_size, lines_id_);</span><br><span class="line">    cv::Mat cv_img = ReadImageToCVMat(root_folder + lines_[lines_id_].first,</span><br><span class="line">        0, 0, is_color);</span><br><span class="line">    CHECK(cv_img.data) &lt;&lt; &quot;Could not load &quot; &lt;&lt; lines_[lines_id_].first;</span><br><span class="line">    read_time += timer.MicroSeconds();</span><br><span class="line">    timer.Start();</span><br><span class="line">    // Apply transformations (mirror, crop...) to the image</span><br><span class="line">int x1 = 0;</span><br><span class="line">int y1 = 0;</span><br><span class="line">int x2 = cv_img.cols;</span><br><span class="line">int y2 = cv_img.rows;</span><br><span class="line">if (shuffleflag)&#123;</span><br><span class="line">CHECK_GE(cv_img.rows, new_height) &lt;&lt; lines_[lines_id_].first;</span><br><span class="line">CHECK_GE(cv_img.cols, new_width) &lt;&lt; lines_[lines_id_].first;</span><br><span class="line">int minx = lines_[lines_id_].second[top_shape1[1]];</span><br><span class="line">int maxx = lines_[lines_id_].second[top_shape1[1] + 1];</span><br><span class="line">int miny = lines_[lines_id_].second[top_shape1[1] + 2];</span><br><span class="line">int maxy = lines_[lines_id_].second[top_shape1[1] + 3];</span><br><span class="line">x1 = Rand(2 * round(rate_width*cv_img.cols));</span><br><span class="line">y1 = Rand(2 * round(rate_height*cv_img.rows));</span><br><span class="line">x2 = x1 + new_width;</span><br><span class="line">y2 = y1 + new_height;</span><br><span class="line">if (x1 &gt; minx)&#123;</span><br><span class="line">x2 -= x1 - minx;</span><br><span class="line">x1 = minx;</span><br><span class="line">&#125;</span><br><span class="line">if (x2 &lt; maxx)&#123;</span><br><span class="line">x1 += maxx - x2;</span><br><span class="line">x2 = maxx;</span><br><span class="line">&#125;</span><br><span class="line">if (x1&lt;0)&#123;</span><br><span class="line">x2 += -x1;</span><br><span class="line">x1 = 0;</span><br><span class="line">&#125;</span><br><span class="line">if (x2 &gt; cv_img.cols)&#123;</span><br><span class="line">x1 -= x2 - cv_img.cols;</span><br><span class="line">x2 = cv_img.cols;</span><br><span class="line">&#125;</span><br><span class="line">if (y1 &gt; miny)&#123;</span><br><span class="line">y2 -= y1 - miny;</span><br><span class="line">y1 = miny;</span><br><span class="line">&#125;</span><br><span class="line">if (y2 &lt; maxy)&#123;</span><br><span class="line">y1 += maxy - y2;</span><br><span class="line">y2 = maxy;</span><br><span class="line">&#125;</span><br><span class="line">if (y1&lt;0)&#123;</span><br><span class="line">y2 += -y1;</span><br><span class="line">y1 = 0;</span><br><span class="line">&#125;</span><br><span class="line">if (y2&gt;cv_img.rows)&#123;</span><br><span class="line">y1 -= y2 - cv_img.rows;</span><br><span class="line">y2 = cv_img.rows;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">if (y2 - y1 != new_height || x2 - x1 != new_width)&#123;</span><br><span class="line">printf(&quot;%s y1:%d, y2:%d, x1:%d, x2:%d\n&quot;, lines_[lines_id_].first.c_str(),y1,y2,x1,x2);</span><br><span class="line">&#125;</span><br><span class="line">    //</span><br><span class="line">    int offset = batch-&gt;data_.offset(item_id);</span><br><span class="line">    this-&gt;transformed_data_.set_cpu_data(prefetch_data + offset);</span><br><span class="line">    this-&gt;data_transformer_-&gt;Transform(cv_img(cv::Range(y1, y2), cv::Range(x1, x2)), &amp;(this-&gt;transformed_data_));</span><br><span class="line">    trans_time += timer.MicroSeconds();</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i &lt; top_shape1[1]; i++)&#123;</span><br><span class="line">      if (i % 2 == 0) prefetch_label[item_id*top_shape1[1] + i] = (lines_[lines_id_].second[i] - x1 - dw_2) / dw_2;</span><br><span class="line">      else prefetch_label[item_id*top_shape1[1] + i] = (lines_[lines_id_].second[i] - y1 - dh_2) / dh_2;</span><br><span class="line">    &#125;</span><br><span class="line">    // go to the next iter</span><br><span class="line">    lines_id_++;</span><br><span class="line">    if (lines_id_ &gt;= lines_size) &#123;</span><br><span class="line">      // We have reached the end. Restart from the first.</span><br><span class="line">      DLOG(INFO) &lt;&lt; &quot;Restarting data prefetching from start.&quot;;</span><br><span class="line">      lines_id_ = 0;</span><br><span class="line">      if (shuffleflag) &#123;</span><br><span class="line">        ShuffleImages();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  batch_timer.Stop();</span><br><span class="line">  DLOG(INFO) &lt;&lt; &quot;Prefetch batch: &quot; &lt;&lt; batch_timer.MilliSeconds() &lt;&lt; &quot; ms.&quot;;</span><br><span class="line">  DLOG(INFO) &lt;&lt; &quot;     Read time: &quot; &lt;&lt; read_time / 1000 &lt;&lt; &quot; ms.&quot;;</span><br><span class="line">  DLOG(INFO) &lt;&lt; &quot;Transform time: &quot; &lt;&lt; trans_time / 1000 &lt;&lt; &quot; ms.&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">INSTANTIATE_CLASS(ImageDataLayer);</span><br><span class="line">REGISTER_LAYER_CLASS(ImageData);</span><br><span class="line"></span><br><span class="line">&#125;  // namespace caffe</span><br><span class="line">#endif  // USE_OPENCV</span><br></pre></td></tr></table></figure><p>经过上面的修改之后，就可以编译该caffe。<br>网络结构以及solver.ptototxt见github中landmark_detec文件夹中的内容。</p><h1 id="三、模型的训练"><a href="#三、模型的训练" class="headerlink" title="三、模型的训练"></a>三、模型的训练</h1><p>准备好了上面的所有数据以及文件之后，可以使用下面的shell脚本进行训练：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">cd ../</span><br><span class="line">## MODIFY PATH for YOUR SETTING</span><br><span class="line">CAFFE_DIR=/Users/camlin_z/Data/Project/caffe-68landmark</span><br><span class="line">CONFIG_DIR=$&#123;CAFFE_DIR&#125;/landmark_detec</span><br><span class="line">CAFFE_BIN=$&#123;CAFFE_DIR&#125;/build/tools/caffe</span><br><span class="line">DEV_ID=0</span><br><span class="line"></span><br><span class="line">sudo $&#123;CAFFE_BIN&#125; train \</span><br><span class="line">-solver=$&#123;CONFIG_DIR&#125;/solver.prototxt \</span><br><span class="line">-weights=$&#123;CONFIG_DIR&#125;/init.caffemodel \</span><br><span class="line">-gpu=$&#123;DEV_ID&#125; \</span><br><span class="line">2&gt;&amp;1 | tee $&#123;CONFIG_DIR&#125;/train.log</span><br></pre></td></tr></table></figure><p>训练过程中先开始使用”fixed”的策略进行训练，发现到20000次的迭代之后，loss不再下降了，所以改为”multistep”的策略进行训练，训练得到的模型效果还是很好的，时间在我的mac上面大概是80ms左右，可以使用下面的脚本进行测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">import caffe</span><br><span class="line">from PIL import Image, ImageDraw</span><br><span class="line">import time</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">blobname = &quot;68point&quot;</span><br><span class="line">feature_dim = 136</span><br><span class="line"></span><br><span class="line">root = &quot;/Users/camlin_z/Data/Project/caffe-master-multilabel-normalize-randcrop-newloss/landmark_detec/&quot;</span><br><span class="line">deploy = root + &quot;deploy.prototxt&quot;</span><br><span class="line"># caffe_model = root + &quot;snapshot_all1/snapshot_iter_250000.caffemodel&quot;</span><br><span class="line"># caffe_model = root + &quot;snapshot_part1/oldfinetune_iter_20000.caffemodel&quot;</span><br><span class="line">caffe_model = root + &quot;init.caffemodel&quot;</span><br><span class="line"># caffe_model = root + &quot;snapshot2/fine_iter_400000.caffemodel&quot;</span><br><span class="line"># caffe_model = root + &quot;snapshot_final/final_iter_350000.caffemodel&quot;</span><br><span class="line">img_dir = &quot;/Users/camlin_z/Data/data_fine/&quot;</span><br><span class="line">img_dir_out = &quot;/Users/camlin_z/Data/data_fine/out/&quot;</span><br><span class="line">label_file = &quot;/Users/camlin_z/Data/data_fine/label_test.txt&quot;</span><br><span class="line">img_path = &quot;/Users/camlin_z/Data/data_fine/data2/2415.jpeg&quot;</span><br><span class="line"></span><br><span class="line">net = caffe.Net(deploy, caffe_model, caffe.TEST)</span><br><span class="line">caffe.set_mode_cpu()</span><br><span class="line"></span><br><span class="line"># 测试一批数据，并显示所有数据的均方差</span><br><span class="line">def detec_whole(img_dir, img_dir_out, label_file):</span><br><span class="line">    time_sum = 0</span><br><span class="line">    mser_sum = 0</span><br><span class="line">    id_sum = 0</span><br><span class="line"></span><br><span class="line">    fid = open(label_file, &apos;r&apos;)</span><br><span class="line">    for id in fid:</span><br><span class="line">        id_sum += 1</span><br><span class="line">        flag = id.find(&apos; &apos;)</span><br><span class="line">        # 读取文件中的标签信息</span><br><span class="line">        image_name = id[:flag]</span><br><span class="line">        label_true = id[flag:]</span><br><span class="line">        label_true = label_true.strip().split()</span><br><span class="line">        label_true = map(float, label_true)</span><br><span class="line">        label_true = np.array(label_true, np.float32)</span><br><span class="line">        imgname = os.path.basename(image_name)</span><br><span class="line">        print imgname</span><br><span class="line">        # print label_true</span><br><span class="line"></span><br><span class="line">        img = cv2.imread(img_dir + image_name)</span><br><span class="line">        img_draw = img.copy()</span><br><span class="line">        sh = img.shape</span><br><span class="line">        h = sh[0]</span><br><span class="line">        w = sh[1]</span><br><span class="line">        rw = (w + 1) / 2</span><br><span class="line">        rh = (h + 1) / 2</span><br><span class="line"></span><br><span class="line">        # 以下网络输出了预测的68点坐标</span><br><span class="line">        # start = time.time()</span><br><span class="line">        img = np.array(img, np.float32)</span><br><span class="line">        transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;)  # 设定图片的shape格式(1,3,28,28)</span><br><span class="line">        transformer.set_transpose(&apos;data&apos;, (2, 0, 1))</span><br><span class="line">        transformer.set_mean(&apos;data&apos;, np.array([127.5, 127.5, 127.5]))</span><br><span class="line">        net.blobs[&apos;data&apos;].data[...] = transformer.preprocess(&apos;data&apos;, img)</span><br><span class="line">        start = time.time()</span><br><span class="line">        out = net.forward()</span><br><span class="line">        landmark = out[blobname]</span><br><span class="line">        elap = time.time() - start</span><br><span class="line">        landmark = np.array(landmark, np.float32)</span><br><span class="line">        landmark[0: 136: 2] = (landmark[0: 136: 2] * rh) + rh</span><br><span class="line">        landmark[1: 136: 2] = (landmark[1: 136: 2] * rw) + rw</span><br><span class="line">        # print landmark</span><br><span class="line">        time_sum += elap</span><br><span class="line">        print &quot;time:&quot;, elap</span><br><span class="line"></span><br><span class="line">        for i in range(0, 136, 2):</span><br><span class="line">            cv2.circle(img_draw, (int(landmark[0][i]), int(landmark[0][i + 1])), 2, (0, 255, 0), -1, cv2.LINE_AA)</span><br><span class="line">        cv2.imwrite(img_dir_out + imgname, img_draw)</span><br><span class="line"></span><br><span class="line">        v = label_true - landmark</span><br><span class="line">        v = v*v</span><br><span class="line">        v = v[0][0::2] + v[0][1:: 2]</span><br><span class="line">        sv = np.power(v, 0.5)</span><br><span class="line">        mser = sum(sv) / feature_dim</span><br><span class="line">        mser_sum += mser</span><br><span class="line">        print &quot;mser:&quot;, mser</span><br><span class="line"></span><br><span class="line">    print &quot;Average time:&quot;, time_sum/id_sum</span><br><span class="line">    print &quot;Average mser:&quot;, mser_sum/id_sum</span><br><span class="line"></span><br><span class="line"># 测试一张图片，并显示预测的特征点的位置</span><br><span class="line">def detec_single():</span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    # draw = ImageDraw.Draw(img1)</span><br><span class="line">    sh = img.shape</span><br><span class="line">    print sh</span><br><span class="line">    h = sh[0]</span><br><span class="line">    w = sh[1]</span><br><span class="line">    rw = (w + 1)/2</span><br><span class="line">    rh = (h + 1)/2</span><br><span class="line">    img = np.array(img, np.float32)</span><br><span class="line">    img_copy = img.copy()</span><br><span class="line"></span><br><span class="line">    transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;)</span><br><span class="line">    transformer.set_transpose(&apos;data&apos;, (2, 0, 1))</span><br><span class="line">    transformer.set_mean(&apos;data&apos;, np.array([127.5, 127.5, 127.5]))</span><br><span class="line">    net.blobs[&apos;data&apos;].data[...] = transformer.preprocess(&apos;data&apos;,img)</span><br><span class="line">    start = time.time()</span><br><span class="line">    out = net.forward()</span><br><span class="line">    elap = time.time() - start</span><br><span class="line">    print &quot;time:&quot;, elap</span><br><span class="line">    landmark = out[blobname]</span><br><span class="line">    landmark = np.array(landmark, np.float32)</span><br><span class="line">    landmark[0: 136: 2] = (landmark[0: 136: 2] * rh ) + rh</span><br><span class="line">    landmark[1: 136: 2] = (landmark[1: 136: 2] * rw ) + rw</span><br><span class="line">    # print landmark</span><br><span class="line"></span><br><span class="line">    for i in range(0, 136, 2):</span><br><span class="line">        cv2.circle(img_copy, (int(landmark[0][i]), int(landmark[0][i+1])), 2, (0, 255, 0), -1, cv2.LINE_AA)</span><br><span class="line">    # draw.point(landmark[0], (225, 225, 255))</span><br><span class="line">    # del draw</span><br><span class="line">    cv2.imwrite(root + &apos;test.jpg&apos;, img_copy)</span><br><span class="line">    # img1.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    detec_whole(img_dir, img_dir_out, label_file)</span><br><span class="line">    # detec_single()</span><br></pre></td></tr></table></figure><p>在写上面的测试代码的时候发现mac上由于caffe的包和cv2的包会有冲突，所以img.show()显示图片会出现问题，希望有知道的大牛可以告诉我怎么解决这个问题。</p><p>以上就是全部的过程，也是我实习里做的第二个完整的项目，上面如果有错误或者说的不对的地方，希望大家能够留言指出，万分感谢。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该人脸68特征点检测的深度学习方法采用VGG作为原型进行改造(以下简称mini VGG)，从数据集的准备，网络模型的构造以及最终的训练过程三个方面进行介绍，工程源码详见：&lt;a href=&quot;https://github.com/CamlinZ/face_alignment&quot; 
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习算法" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="人脸特征点" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E4%BA%BA%E8%84%B8%E7%89%B9%E5%BE%81%E7%82%B9/"/>
    
    
      <category term="人脸68特征点" scheme="http://camlinzhang.com/tags/%E4%BA%BA%E8%84%B868%E7%89%B9%E5%BE%81%E7%82%B9/"/>
    
      <category term="深度学习" scheme="http://camlinzhang.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>MTCNN配置及训练详细步骤</title>
    <link href="http://camlinzhang.com/2018/06/20/MTCNN%E9%85%8D%E7%BD%AE%E5%8F%8A%E8%AE%AD%E7%BB%83%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4/"/>
    <id>http://camlinzhang.com/2018/06/20/MTCNN配置及训练详细步骤/</id>
    <published>2018-06-20T07:11:38.000Z</published>
    <updated>2019-03-25T14:13:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置环境为win7 64位，主要完成的任务是用MTCNN完成人脸检测，即使用目标检测框将图像中的人脸框出来，配置过程如下：</p><h1 id="1、环境配置"><a href="#1、环境配置" class="headerlink" title="1、环境配置"></a>1、环境配置</h1><h2 id="安装anaconda"><a href="#安装anaconda" class="headerlink" title="安装anaconda"></a>安装anaconda</h2><p>进入官网：<br><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">https://www.anaconda.com/download/</a><br>根据python版本下载安装相应的anaconda即可</p><h2 id="安装Microsoft-Visual-Studio-2013"><a href="#安装Microsoft-Visual-Studio-2013" class="headerlink" title="安装Microsoft Visual Studio 2013"></a>安装Microsoft Visual Studio 2013</h2><p>注意此处一定要安装2013版方便后面caffe的编译，下载地址为：<br><a href="https://msdn.itellyou.cn/" target="_blank" rel="noopener">https://msdn.itellyou.cn/</a></p><h2 id="在编译好的VS环境下配置opencv和openblas"><a href="#在编译好的VS环境下配置opencv和openblas" class="headerlink" title="在编译好的VS环境下配置opencv和openblas"></a>在编译好的VS环境下配置opencv和openblas</h2><p>配置opencv参考：<br><a href="https://blog.csdn.net/SherryD/article/details/51734334" target="_blank" rel="noopener">https://blog.csdn.net/SherryD/article/details/51734334</a><br>配置openblas参考：<br><a href="https://blog.csdn.net/yangyangyang20092010/article/details/45156881" target="_blank" rel="noopener">https://blog.csdn.net/yangyangyang20092010/article/details/45156881</a></p><h2 id="在VS环境下编译caffe"><a href="#在VS环境下编译caffe" class="headerlink" title="在VS环境下编译caffe"></a>在VS环境下编译caffe</h2><p>下载caffe的windows官方编译版本：<br><a href="https://github.com/happynear/caffe-windows" target="_blank" rel="noopener">https://github.com/happynear/caffe-windows</a><br>然后按照：<br><a href="https://blog.csdn.net/xierhacker/article/details/51834563" target="_blank" rel="noopener">https://blog.csdn.net/xierhacker/article/details/51834563</a><br>安装即可</p><h2 id="安装pycharm，并在anaconda的python环境中配置opencv"><a href="#安装pycharm，并在anaconda的python环境中配置opencv" class="headerlink" title="安装pycharm，并在anaconda的python环境中配置opencv"></a>安装pycharm，并在anaconda的python环境中配置opencv</h2><p>1、安装pycharm参考：<br><a href="https://www.jianshu.com/p/042324342bf4" target="_blank" rel="noopener">https://www.jianshu.com/p/042324342bf4</a></p><p>2、在anaconda的python环境中配置opencv<br>首先在官网：<a href="https://opencv.org/releases.html" target="_blank" rel="noopener">https://opencv.org/releases.html</a><br>下载opencv的win pack包，然后直接点exe运行即可，安装完成后，将opencv的安装路径：<br>E:\OpenCV2\opencv\build\python\2.7\x86下的cv2.pyd移动到anaconda的安装路径：D:\Anaconda\anaconda2\Lib\site-packages下，然后可以在cmd命令进行测试</p><p>3、以上配置好后，在pycharm中一个常见的问题就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: No module named google.protobuf.internal</span><br></pre></td></tr></table></figure><p>这里需要首先到：<a href="https://github.com/google/protobuf将protobuf-maste拷贝下来，然后到：https://github.com/google/protobuf/releases中下载protoc-3.5.1-win32.zip" target="_blank" rel="noopener">https://github.com/google/protobuf将protobuf-maste拷贝下来，然后到：https://github.com/google/protobuf/releases中下载protoc-3.5.1-win32.zip</a><br>将protoc-3.5.1-win32\bin下的protoc.exe复制到protobuf-master\src文件夹下，按照：<br><a href="http://sharley.iteye.com/blog/2375044" target="_blank" rel="noopener">http://sharley.iteye.com/blog/2375044</a><br>中的方式进行安装</p><h1 id="2、MTCNN配置"><a href="#2、MTCNN配置" class="headerlink" title="2、MTCNN配置"></a>2、MTCNN配置</h1><p>github上MTCNN有很多版本，我以从数据集准备到最终的测试的顺序来介绍<br>训练主要参考：<a href="https://github.com/dlunion/mtcnn" target="_blank" rel="noopener">https://github.com/dlunion/mtcnn</a><br>测试主要参考：<a href="https://github.com/CongWeilin/mtcnn-caffe" target="_blank" rel="noopener">https://github.com/CongWeilin/mtcnn-caffe</a></p><h2 id="数据集的准备"><a href="#数据集的准备" class="headerlink" title="数据集的准备"></a>数据集的准备</h2><p>1、将采集好数据集放到一个文件夹中，命名为samples（也可以写成别的名字，但是注意与后面的步骤中需要该文件夹数据的路径要一致）<br>2、对数据集进行标注，网上有很多的标注工具可以使用：<br><a href="https://blog.csdn.net/chaipp0607/article/details/79036312" target="_blank" rel="noopener">https://blog.csdn.net/chaipp0607/article/details/79036312</a><br>可以使用上面的标注工具进行标注，标注完成后会生成一个txt文档或者是xml文档之类的文档，里面包含了图像检测框的左上角点的坐标和右下角点的坐标信息。<br>3、根据文档中提供的信息，我们需要将检测框的左上角点的坐标和右下角点的坐标提取出来，整理成以下形式：<br>samples/filename.jpg xmin ymin xmax ymax<br>（即：数据集文件夹/图片名 检测框左上角点的x坐标 检测框左上角点的y坐标 检测框右下角点的x坐标 检测框右下角点的y坐标）<br>我使用的数据标注工具生成的文档如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&apos;1.0&apos; encoding=&apos;GB2312&apos;?&gt;</span><br><span class="line">&lt;info&gt;</span><br><span class="line">&lt;src width=&quot;480&quot; height=&quot;640&quot; depth=&quot;3&quot;&gt;00ff0abc4818a309b51180264b830211.jpg&lt;/src&gt;</span><br><span class="line">&lt;object id=&quot;E68519DF-E8E1-4C55-9231-CB381DE1CC5A&quot;&gt;</span><br><span class="line">&lt;rect lefttopx=&quot;168&quot; lefttopy=&quot;168&quot; rightbottomx=&quot;313&quot; rightbottomy=&quot;340&quot;&gt;&lt;/rect&gt;</span><br><span class="line">&lt;type&gt;21&lt;/type&gt;</span><br><span class="line">&lt;descriinfo&gt;&lt;/descriinfo&gt;</span><br><span class="line">&lt;modifydate&gt;2018-05-08 17:04:07&lt;/modifydate&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;/info&gt;</span><br></pre></td></tr></table></figure><p>所以我需要将这个文档中的检测框坐标点提取出来，并整理成如上所述的标准形式，形成一个 label.txt 文档<br>根据以上xml的形式，转换的脚本如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">##################### 以下部分用于读取xml文件，返回检测框左上角和右下角的坐标 ###################</span><br><span class="line">def read_xml(in_path):</span><br><span class="line">    tree = etree.parse(in_path)</span><br><span class="line">    return tree</span><br><span class="line"></span><br><span class="line">def find_nodes(tree, path):</span><br><span class="line">    return tree.findall(path)</span><br><span class="line"></span><br><span class="line">def get_obj(xml_path):</span><br><span class="line">    tree = read_xml(xml_path)</span><br><span class="line">    nodes = find_nodes(tree, &quot;src&quot;)</span><br><span class="line">    objects = []</span><br><span class="line"></span><br><span class="line">    for node in nodes:</span><br><span class="line">        pic_struct = &#123;&#125;</span><br><span class="line">        pic_struct[&apos;width&apos;] = str(node.get(&apos;width&apos;))</span><br><span class="line">        pic_struct[&apos;height&apos;] = str(node.get(&apos;height&apos;))</span><br><span class="line">        pic_struct[&apos;depth&apos;] = str(node.get(&apos;depth&apos;))</span><br><span class="line">        # objects.append(pic_struct)</span><br><span class="line">    nodes = find_nodes(tree, &quot;object&quot;)</span><br><span class="line"></span><br><span class="line">    for i in range(len(nodes)):</span><br><span class="line">        # obj_struct = &#123;&#125;</span><br><span class="line">        # obj_struct[&apos;name&apos;] = str(find_nodes(nodes[i] , &apos;type&apos;)[0].text)</span><br><span class="line">        cl_box = find_nodes(nodes[i], &apos;rect&apos;)</span><br><span class="line">        for rec in cl_box:</span><br><span class="line">            objects = [int(rec.get(&apos;lefttopx&apos;)), int(rec.get(&apos;lefttopy&apos;)),</span><br><span class="line">                       int(rec.get(&apos;rightbottomx&apos;)), int(rec.get(&apos;rightbottomy&apos;))]</span><br><span class="line">    return objects</span><br><span class="line"></span><br><span class="line">################# 将xml的信息统一成标准形式 ################</span><br><span class="line">def listFile(data_dir, suffix):</span><br><span class="line">    fs = os.listdir(data_dir)</span><br><span class="line">    for i in range(len(fs)-1, -1, -1):</span><br><span class="line">        # 如果后缀不是.jpg就将该文件删除掉</span><br><span class="line">        if not fs[i].endswith(suffix):</span><br><span class="line">            del fs[i]</span><br><span class="line">    return fs</span><br><span class="line"></span><br><span class="line">def write_label(data_dir, xml_dir):</span><br><span class="line">    images = listFile(data_dir, &quot;.jpg&quot;)</span><br><span class="line">    with open(&quot;label.txt&quot;, &quot;w&quot;) as label:</span><br><span class="line">        for i in range(len(images)):</span><br><span class="line">            image_path = data_dir + &quot;/&quot; + images[i]</span><br><span class="line">            xml_path = xml_dir + &quot;/&quot; + images[i][:-4] + &quot;.txt&quot;</span><br><span class="line">            objects = get_obj(xml_path)</span><br><span class="line">            line = image_path + &quot; &quot; + str(objects[0]) + &quot; &quot; + str(objects[1]) \</span><br><span class="line">                   + &quot; &quot; + str(objects[2]) + &quot; &quot; + str(objects[3]) + &quot;\n&quot;</span><br><span class="line">            label.write(line)</span><br><span class="line"></span><br><span class="line">################ 主函数 ###################</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    data_dir = &quot;E:/MTCNN/Train/samples&quot;</span><br><span class="line">    xml_dir = &quot;E:/MTCNN/Train/samples/annotation&quot;</span><br><span class="line">    write_label(data_dir, xml_dir)</span><br></pre></td></tr></table></figure><p>整理好的 label.txt 形式为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">E:/MTCNN/Train/samples/0019c3f356ada6bcda0b695020e295e6.jpg 102 87 311 417</span><br><span class="line">E:/MTCNN/Train/samples/0043e38f303b247e50b9a07cb5887b39.jpg 156 75 335 295</span><br><span class="line">E:/MTCNN/Train/samples/004e26290d2290ca87e02b737a740aee.jpg 105 122 291 381</span><br><span class="line">E:/MTCNN/Train/samples/00ff0abc4818a309b51180264b830211.jpg 168 168 313 340</span><br><span class="line">E:/MTCNN/Train/samples/015a7137173f29e2cd4663c7cbcad1cb.jpg 127 60 332 398</span><br><span class="line">E:/MTCNN/Train/samples/0166ceba53a4bfc4360e1d12b33ecb61.jpg 149 82 353 378</span><br><span class="line">E:/MTCNN/Train/samples/01e6deccb55b377985d2c4d72006ee34.jpg 185 100 289 249</span><br><span class="line">E:/MTCNN/Train/samples/021e34448c0ed051db501156cf2b6552.jpg 204 91 359 289</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h1 id="3、MTCNN训练数据生成及训练"><a href="#3、MTCNN训练数据生成及训练" class="headerlink" title="3、MTCNN训练数据生成及训练"></a>3、MTCNN训练数据生成及训练</h1><h2 id="1-P-Net-的训练"><a href="#1-P-Net-的训练" class="headerlink" title="(1)   P_Net 的训练"></a>(1)   P_Net 的训练</h2><p>按照MTCNN论文中的说法：<br><img src="https://img-blog.csdn.net/20180516150305818?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>需要将原始数据集的数据分成Negative，Positive，Part faces，Landmark faces四个部分，由于本次主要是进行人脸检测的任务，所以只需要分成Negative，Positive，Part faces三个部分即可，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import numpy.random as npr</span><br><span class="line"></span><br><span class="line">stdsize = 12</span><br><span class="line">anno_file = &quot;label.txt&quot;</span><br><span class="line">im_dir = &quot;samples&quot;</span><br><span class="line">pos_save_dir = str(stdsize) + &quot;/positive&quot;</span><br><span class="line">part_save_dir = str(stdsize) + &quot;/part&quot;</span><br><span class="line">neg_save_dir = str(stdsize) + &apos;/negative&apos;</span><br><span class="line">save_dir = &quot;./&quot; + str(stdsize)</span><br><span class="line"></span><br><span class="line">def IoU(box, boxes):</span><br><span class="line">    &quot;&quot;&quot;Compute IoU between detect box and gt boxes</span><br><span class="line"></span><br><span class="line">    Parameters:</span><br><span class="line">    ----------</span><br><span class="line">    box: numpy array , shape (5, ): x1, y1, x2, y2, score</span><br><span class="line">        input box</span><br><span class="line">    boxes: numpy array, shape (n, 4): x1, y1, x2, y2</span><br><span class="line">        input ground truth boxes</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    -------</span><br><span class="line">    ovr: numpy.array, shape (n, )</span><br><span class="line">        IoU</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)</span><br><span class="line">    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)</span><br><span class="line">    # boxes[:, 0]代表取boxes这个nx4矩阵所有行的第一个数据</span><br><span class="line">    xx1 = np.maximum(box[0], boxes[:, 0])</span><br><span class="line">    yy1 = np.maximum(box[1], boxes[:, 1])</span><br><span class="line">    xx2 = np.minimum(box[2], boxes[:, 2])</span><br><span class="line">    yy2 = np.minimum(box[3], boxes[:, 3])</span><br><span class="line"></span><br><span class="line">    # compute the width and height of the bounding box</span><br><span class="line">    w = np.maximum(0, xx2 - xx1 + 1)</span><br><span class="line">    h = np.maximum(0, yy2 - yy1 + 1)</span><br><span class="line"></span><br><span class="line">    inter = w * h</span><br><span class="line">    ovr = inter / (box_area + area - inter)</span><br><span class="line">    return ovr</span><br><span class="line"></span><br><span class="line"># 生成一系列文件夹用于存储三类样本</span><br><span class="line">def mkr(dr):</span><br><span class="line">    if not os.path.exists(dr):</span><br><span class="line">        os.mkdir(dr)</span><br><span class="line"></span><br><span class="line">mkr(save_dir)</span><br><span class="line">mkr(pos_save_dir)</span><br><span class="line">mkr(part_save_dir)</span><br><span class="line">mkr(neg_save_dir)</span><br><span class="line"></span><br><span class="line"># 生成一系列txt文档用于存储Positive，Negative，Part三类数据的信息</span><br><span class="line">f1 = open(os.path.join(save_dir, &apos;pos_&apos; + str(stdsize) + &apos;.txt&apos;), &apos;w&apos;)</span><br><span class="line">f2 = open(os.path.join(save_dir, &apos;neg_&apos; + str(stdsize) + &apos;.txt&apos;), &apos;w&apos;)</span><br><span class="line">f3 = open(os.path.join(save_dir, &apos;part_&apos; + str(stdsize) + &apos;.txt&apos;), &apos;w&apos;)</span><br><span class="line"></span><br><span class="line"># 读取label.txt</span><br><span class="line">with open(anno_file, &apos;r&apos;) as f:</span><br><span class="line">    annotations = f.readlines()</span><br><span class="line">num = len(annotations)</span><br><span class="line">print &quot;%d pics in total&quot; % num</span><br><span class="line">p_idx = 0 # positive</span><br><span class="line">n_idx = 0 # negative</span><br><span class="line">d_idx = 0 # dont care</span><br><span class="line">idx = 0</span><br><span class="line">box_idx = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for annotation in annotations:</span><br><span class="line">    annotation = annotation.strip().split(&apos; &apos;)</span><br><span class="line">    im_path = annotation[0]</span><br><span class="line">    bbox = map(float, annotation[1:])</span><br><span class="line">    boxes = np.array(bbox, dtype=np.float32).reshape(-1, 4)</span><br><span class="line">    print im_path</span><br><span class="line">    img = cv2.imread(im_path)</span><br><span class="line">    idx += 1</span><br><span class="line">    if idx % 100 == 0:</span><br><span class="line">        print idx, &quot;images done&quot;</span><br><span class="line"></span><br><span class="line">    height, width, channel = img.shape</span><br><span class="line"></span><br><span class="line">    neg_num = 0</span><br><span class="line">    while neg_num &lt; 50:</span><br><span class="line">        # 生成随机数，对每张数据集中的图像进行切割，生成一系列小的图像</span><br><span class="line">        size = npr.randint(40, min(width, height) / 2)</span><br><span class="line">        nx = npr.randint(0, width - size)</span><br><span class="line">        ny = npr.randint(0, height - size)</span><br><span class="line">        crop_box = np.array([nx, ny, nx + size, ny + size])</span><br><span class="line">        # 计算小的图像与标注产生的检测框之间的IoU</span><br><span class="line">        Iou = IoU(crop_box, boxes)</span><br><span class="line"></span><br><span class="line">        cropped_im = img[ny : ny + size, nx : nx + size, :]</span><br><span class="line">        resized_im = cv2.resize(cropped_im, (stdsize, stdsize), interpolation=cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        if np.max(Iou) &lt; 0.3:</span><br><span class="line">            # Iou with all gts must below 0.3</span><br><span class="line">            save_file = os.path.join(neg_save_dir, &quot;%s.jpg&quot;%n_idx)</span><br><span class="line">            f2.write(str(stdsize)+&quot;/negative/%s&quot;%n_idx + &apos; 0\n&apos;)</span><br><span class="line">            cv2.imwrite(save_file, resized_im)</span><br><span class="line">            n_idx += 1</span><br><span class="line">            neg_num += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for box in boxes:</span><br><span class="line">        # box (x_left, y_top, x_right, y_bottom)</span><br><span class="line">        x1, y1, x2, y2 = box</span><br><span class="line">        w = x2 - x1 + 1</span><br><span class="line">        h = y2 - y1 + 1</span><br><span class="line"></span><br><span class="line">        # max(w, h) &lt; 40：参数40表示忽略的最小的脸的大小</span><br><span class="line">        # in case the ground truth boxes of small faces are not accurate</span><br><span class="line">        if max(w, h) &lt; 40 or x1 &lt; 0 or y1 &lt; 0:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        # generate positive examples and part faces</span><br><span class="line">        for i in range(20):</span><br><span class="line">            size = npr.randint(int(min(w, h) * 0.8), np.ceil(1.25 * max(w, h)))</span><br><span class="line"></span><br><span class="line">            # delta here is the offset of box center</span><br><span class="line">            delta_x = npr.randint(-w * 0.2, w * 0.2)</span><br><span class="line">            delta_y = npr.randint(-h * 0.2, h * 0.2)</span><br><span class="line"></span><br><span class="line">            nx1 = max(x1 + w / 2 + delta_x - size / 2, 0)</span><br><span class="line">            ny1 = max(y1 + h / 2 + delta_y - size / 2, 0)</span><br><span class="line">            nx2 = nx1 + size</span><br><span class="line">            ny2 = ny1 + size</span><br><span class="line"></span><br><span class="line">            if nx2 &gt; width or ny2 &gt; height:</span><br><span class="line">                continue</span><br><span class="line">            crop_box = np.array([nx1, ny1, nx2, ny2])</span><br><span class="line"></span><br><span class="line">            offset_x1 = (x1 - nx1) / float(size)</span><br><span class="line">            offset_y1 = (y1 - ny1) / float(size)</span><br><span class="line">            offset_x2 = (x2 - nx2) / float(size)</span><br><span class="line">            offset_y2 = (y2 - ny2) / float(size)</span><br><span class="line"></span><br><span class="line">            cropped_im = img[int(ny1) : int(ny2), int(nx1) : int(nx2), :]</span><br><span class="line">            resized_im = cv2.resize(cropped_im, (stdsize, stdsize), interpolation=cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">            box_ = box.reshape(1, -1)</span><br><span class="line">            if IoU(crop_box, box_) &gt;= 0.65:</span><br><span class="line">                save_file = os.path.join(pos_save_dir, &quot;%s.jpg&quot;%p_idx)</span><br><span class="line">                f1.write(str(stdsize)+&quot;/positive/%s&quot;%p_idx + &apos; 1 %.2f %.2f %.2f %.2f\n&apos;%(offset_x1, offset_y1, offset_x2, offset_y2))</span><br><span class="line">                cv2.imwrite(save_file, resized_im)</span><br><span class="line">                p_idx += 1</span><br><span class="line">            elif IoU(crop_box, box_) &gt;= 0.4:</span><br><span class="line">                save_file = os.path.join(part_save_dir, &quot;%s.jpg&quot;%d_idx)</span><br><span class="line">                f3.write(str(stdsize)+&quot;/part/%s&quot;%d_idx + &apos; -1 %.2f %.2f %.2f %.2f\n&apos;%(offset_x1, offset_y1, offset_x2, offset_y2))</span><br><span class="line">                cv2.imwrite(save_file, resized_im)</span><br><span class="line">                d_idx += 1</span><br><span class="line">        box_idx += 1</span><br><span class="line">        print &quot;%s images done, pos: %s part: %s neg: %s&quot;%(idx, p_idx, d_idx, n_idx)</span><br><span class="line"></span><br><span class="line">f1.close()</span><br><span class="line">f2.close()</span><br><span class="line">f3.close()</span><br></pre></td></tr></table></figure><p>这里是产生第一个P-Net的训练样本，产生后续R-Net和O-Net的训练样本只需要将上面的 stdsize = 12 参数改成24和48即可，里面有些参数也可以根据自己的需要进行修改。</p><p>上面获得了随机切分原图后得到的Negative，Positive，Part faces三类样本的图片路径和样本中的每一张图片里检测框的坐标，我们要进行训练，还是需要将这些信息保存为第三步中label.txt的形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">save_dir = &quot;./12&quot;</span><br><span class="line">if not os.path.exists(save_dir):</span><br><span class="line">    os.mkdir(save_dir)</span><br><span class="line">f1 = open(os.path.join(save_dir, &apos;pos_12.txt&apos;), &apos;r&apos;)</span><br><span class="line">f2 = open(os.path.join(save_dir, &apos;neg_12.txt&apos;), &apos;r&apos;)</span><br><span class="line">f3 = open(os.path.join(save_dir, &apos;part_12.txt&apos;), &apos;r&apos;)</span><br><span class="line"></span><br><span class="line">pos = f1.readlines()</span><br><span class="line">neg = f2.readlines()</span><br><span class="line">part = f3.readlines()</span><br><span class="line">f = open(os.path.join(save_dir, &apos;label-train.txt&apos;), &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">for i in range(int(len(pos))):</span><br><span class="line">    p = pos[i].find(&quot; &quot;) + 1</span><br><span class="line">    pos[i] = pos[i][:p-1] + &quot;.jpg &quot; + pos[i][p:-1] + &quot;\n&quot;</span><br><span class="line">    f.write(pos[i])</span><br><span class="line"></span><br><span class="line">for i in range(int(len(neg))):</span><br><span class="line">    p = neg[i].find(&quot; &quot;) + 1</span><br><span class="line">    neg[i] = neg[i][:p-1] + &quot;.jpg &quot; + neg[i][p:-1] + &quot; -1 -1 -1 -1\n&quot;</span><br><span class="line">    f.write(neg[i])</span><br><span class="line"></span><br><span class="line">for i in range(int(len(part))):</span><br><span class="line">    p = part[i].find(&quot; &quot;) + 1</span><br><span class="line">    part[i] = part[i][:p-1] + &quot;.jpg &quot; + part[i][p:-1] + &quot;\n&quot;</span><br><span class="line">    f.write(part[i])</span><br><span class="line"></span><br><span class="line">f1.close()</span><br><span class="line">f2.close()</span><br><span class="line">f3.close()</span><br></pre></td></tr></table></figure><p>接下来要将其转换成caffe用的lmdb形式，这里我们利用caffe自带的工具，转换代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;caffe/convert_imageset.exe&quot; &quot;&quot; 12/label.txt train_lmdb12 --backend=mtcnn --shuffle=true</span><br></pre></td></tr></table></figure><p>由于将原始图片切分成Negative，Positive，Part faces三个部分后数据量很大，所以可能转换的时间会很长。<br>至此，P_Net的训练数据就准备好了，接下来就可以进行训练了。</p><p>训练我们需要配置到caffe的相关prototxt：<br>上面训练参考链接中的：det1-train.prototxt，solver-12.prototxt，注意调整这两个文件中的路径，然后在根目录下新建models-12文件夹用于存储snapshot，最后使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;caffe/caffe.exe&quot; train --solver=solver-12.prototxt --weights=det1.caffemodel</span><br></pre></td></tr></table></figure><p>进行训练即可。</p><h2 id="2-R-Net-的训练"><a href="#2-R-Net-的训练" class="headerlink" title="(2)   R_Net 的训练"></a>(2)   R_Net 的训练</h2><p>进行完上面P_Net的训练后，继续参考上面的产生数据的代码产生R_Net所需的训练数据，同时因为论文中强调了产生hard_sample会提高模型的预测精度：<br><img src="https://img-blog.csdn.net/20180618125823933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI4NzMxNTc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>所以我们使用下面的代码来产生hard_sample：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">import tools</span><br><span class="line">import caffe</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">from utils import *</span><br><span class="line">deploy = &apos;det1.prototxt&apos;</span><br><span class="line">caffemodel = &apos;det1.caffemodel&apos;</span><br><span class="line">net_12 = caffe.Net(deploy,caffemodel,caffe.TEST)</span><br><span class="line">def view_bar(num, total):</span><br><span class="line">    rate = float(num) / total</span><br><span class="line">    rate_num = int(rate * 100)</span><br><span class="line">    r = &apos;\r[%s%s]%d%%  (%d/%d)&apos; % (&quot;#&quot;*rate_num, &quot; &quot;*(100-rate_num), rate_num, num, total)</span><br><span class="line">    sys.stdout.write(r)</span><br><span class="line">    sys.stdout.flush()</span><br><span class="line">def detectFace(img_path,threshold):</span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    caffe_img = img.copy()-128</span><br><span class="line">    origin_h,origin_w,ch = caffe_img.shape</span><br><span class="line">    scales = tools.calculateScales(img)</span><br><span class="line">    out = []</span><br><span class="line">    for scale in scales:</span><br><span class="line">        hs = int(origin_h*scale)</span><br><span class="line">        ws = int(origin_w*scale)</span><br><span class="line">        scale_img = cv2.resize(caffe_img,(ws,hs))</span><br><span class="line">        scale_img = np.swapaxes(scale_img, 0, 2)</span><br><span class="line">        net_12.blobs[&apos;data&apos;].reshape(1,3,ws,hs)</span><br><span class="line">        net_12.blobs[&apos;data&apos;].data[...]=scale_img</span><br><span class="line">caffe.set_device(0)</span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line">out_ = net_12.forward()</span><br><span class="line">        out.append(out_)</span><br><span class="line">    image_num = len(scales)</span><br><span class="line">    rectangles = []</span><br><span class="line">    for i in range(image_num):    </span><br><span class="line">        cls_prob = out[i][&apos;cls_score&apos;][0][1]</span><br><span class="line">        roi      = out[i][&apos;conv4-2&apos;][0]</span><br><span class="line">        out_h,out_w = cls_prob.shape</span><br><span class="line">        out_side = max(out_h,out_w)</span><br><span class="line">        rectangle = tools.detect_face_12net(cls_prob,roi,out_side,1/scales[i],origin_w,origin_h,threshold[0])</span><br><span class="line">        rectangles.extend(rectangle)</span><br><span class="line">    return rectangles</span><br><span class="line">anno_file = &apos;wider_face_train.txt&apos;</span><br><span class="line">im_dir = &quot;WIDER_train/images/&quot;</span><br><span class="line">neg_save_dir  = &quot;24/negative&quot;</span><br><span class="line">pos_save_dir  = &quot;24/positive&quot;</span><br><span class="line">part_save_dir = &quot;24/part&quot;</span><br><span class="line">image_size = 24</span><br><span class="line">f1 = open(&apos;24/pos_24.txt&apos;, &apos;w&apos;)</span><br><span class="line">f2 = open(&apos;24/neg_24.txt&apos;, &apos;w&apos;)</span><br><span class="line">f3 = open(&apos;24/part_24.txt&apos;, &apos;w&apos;)</span><br><span class="line">threshold = [0.6,0.6,0.7]</span><br><span class="line">with open(anno_file, &apos;r&apos;) as f:</span><br><span class="line">    annotations = f.readlines()</span><br><span class="line">num = len(annotations)</span><br><span class="line">print &quot;%d pics in total&quot; % num</span><br><span class="line"></span><br><span class="line">p_idx = 0 # positive</span><br><span class="line">n_idx = 0 # negative</span><br><span class="line">d_idx = 0 # dont care</span><br><span class="line">image_idx = 0</span><br><span class="line"></span><br><span class="line">for annotation in annotations:</span><br><span class="line">    annotation = annotation.strip().split(&apos; &apos;)</span><br><span class="line">    bbox = map(float, annotation[1:])</span><br><span class="line">    gts = np.array(bbox, dtype=np.float32).reshape(-1, 4)</span><br><span class="line">    img_path = im_dir + annotation[0] + &apos;.jpg&apos;</span><br><span class="line">    rectangles = detectFace(img_path,threshold)</span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    image_idx += 1</span><br><span class="line">    view_bar(image_idx,num)</span><br><span class="line">    for box in rectangles:</span><br><span class="line">        x_left, y_top, x_right, y_bottom, _ = box</span><br><span class="line">        crop_w = x_right - x_left + 1</span><br><span class="line">        crop_h = y_bottom - y_top + 1</span><br><span class="line">        # ignore box that is too small or beyond image border</span><br><span class="line">        if crop_w &lt; image_size or crop_h &lt; image_size :</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        # compute intersection over union(IoU) between current box and all gt boxes</span><br><span class="line">        Iou = IoU(box, gts)</span><br><span class="line">        cropped_im = img[y_top:y_bottom + 1, x_left:x_right + 1]</span><br><span class="line">        resized_im = cv2.resize(cropped_im, (image_size, image_size), interpolation=cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        # save negative images and write label</span><br><span class="line">        if np.max(Iou) &lt; 0.3:</span><br><span class="line">            # Iou with all gts must below 0.3</span><br><span class="line">            save_file = os.path.join(neg_save_dir, &quot;%s.jpg&quot;%n_idx)</span><br><span class="line">            f2.write(&quot;%s/negative/%s&quot;%(image_size, n_idx) + &apos; 0\n&apos;)</span><br><span class="line">            cv2.imwrite(save_file, resized_im)</span><br><span class="line">            n_idx += 1</span><br><span class="line">        else:</span><br><span class="line">            # find gt_box with the highest iou</span><br><span class="line">            idx = np.argmax(Iou)</span><br><span class="line">            assigned_gt = gts[idx]</span><br><span class="line">            x1, y1, x2, y2 = assigned_gt</span><br><span class="line"></span><br><span class="line">            # compute bbox reg label</span><br><span class="line">            offset_x1 = (x1 - x_left)   / float(crop_w)</span><br><span class="line">            offset_y1 = (y1 - y_top)    / float(crop_h)</span><br><span class="line">            offset_x2 = (x2 - x_right)  / float(crop_w)</span><br><span class="line">            offset_y2 = (y2 - y_bottom )/ float(crop_h)</span><br><span class="line"></span><br><span class="line">            # save positive and part-face images and write labels</span><br><span class="line">            if np.max(Iou) &gt;= 0.65:</span><br><span class="line">                save_file = os.path.join(pos_save_dir, &quot;%s.jpg&quot;%p_idx)</span><br><span class="line">                f1.write(&quot;%s/positive/%s&quot;%(image_size, p_idx) + &apos; 1 %.2f %.2f %.2f %.2f\n&apos;%(offset_x1, offset_y1, offset_x2, offset_y2))</span><br><span class="line">                cv2.imwrite(save_file, resized_im)</span><br><span class="line">                p_idx += 1</span><br><span class="line"></span><br><span class="line">            elif np.max(Iou) &gt;= 0.4:</span><br><span class="line">                save_file = os.path.join(part_save_dir, &quot;%s.jpg&quot;%d_idx)</span><br><span class="line">                f3.write(&quot;%s/part/%s&quot;%(image_size, d_idx)     + &apos; -1 %.2f %.2f %.2f %.2f\n&apos;%(offset_x1, offset_y1, offset_x2, offset_y2))</span><br><span class="line">                cv2.imwrite(save_file, resized_im)</span><br><span class="line">                d_idx += 1</span><br><span class="line"></span><br><span class="line">f1.close()</span><br><span class="line">f2.close()</span><br><span class="line">f3.close()</span><br></pre></td></tr></table></figure><p>注意修改上面代码的路径，用上面P_Net同样的处理方式将以上数据处理成lmdb的形式并进行训练。O_Net同理。上面的训练完成后就可以进行测试了。</p><h1 id="4、MTCNN的测试"><a href="#4、MTCNN的测试" class="headerlink" title="4、MTCNN的测试"></a>4、MTCNN的测试</h1><p>经过以上的步骤，在models-12、models-24和models-48会有三个网络对应的caffemodel，再加上det1.prototxt、det2.prototxt和det3.prototxt就可以利用下面的代码进行测试了（主要参考<a href="https://github.com/CongWeilin/mtcnn-caffe/tree/master/demo中的代码）：" target="_blank" rel="noopener">https://github.com/CongWeilin/mtcnn-caffe/tree/master/demo中的代码）：</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">import tools_matrix as tools</span><br><span class="line">import caffe</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">deploy = &apos;det1.prototxt&apos;</span><br><span class="line">caffemodel = &apos;det1.caffemodel&apos;</span><br><span class="line">net_12 = caffe.Net(deploy,caffemodel,caffe.TEST)</span><br><span class="line"></span><br><span class="line">deploy = &apos;det2.prototxt&apos;</span><br><span class="line">caffemodel = &apos;det2.caffemodel&apos;</span><br><span class="line">net_24 = caffe.Net(deploy,caffemodel,caffe.TEST)</span><br><span class="line"></span><br><span class="line">deploy = &apos;det3.prototxt&apos;</span><br><span class="line">caffemodel = &apos;det3.caffemodel&apos;</span><br><span class="line">net_48 = caffe.Net(deploy,caffemodel,caffe.TEST)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def detectFace(img_path,threshold):</span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    caffe_img = (img.copy()-127.5)/128</span><br><span class="line">    origin_h,origin_w,ch = caffe_img.shape</span><br><span class="line">    scales = tools.calculateScales(img)</span><br><span class="line">    out = []</span><br><span class="line">    for scale in scales:</span><br><span class="line">        hs = int(origin_h*scale)</span><br><span class="line">        ws = int(origin_w*scale)</span><br><span class="line">        scale_img = cv2.resize(caffe_img,(ws,hs))</span><br><span class="line">        scale_img = np.swapaxes(scale_img, 0, 2)</span><br><span class="line">        net_12.blobs[&apos;data&apos;].reshape(1,3,ws,hs)</span><br><span class="line">        net_12.blobs[&apos;data&apos;].data[...]=scale_img</span><br><span class="line">caffe.set_device(0)</span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line">out_ = net_12.forward()</span><br><span class="line">        out.append(out_)</span><br><span class="line">    image_num = len(scales)</span><br><span class="line">    rectangles = []</span><br><span class="line">    for i in range(image_num):    </span><br><span class="line">        cls_prob = out[i][&apos;prob1&apos;][0][1]</span><br><span class="line">        roi      = out[i][&apos;conv4-2&apos;][0]</span><br><span class="line">        out_h,out_w = cls_prob.shape</span><br><span class="line">        out_side = max(out_h,out_w)</span><br><span class="line">        rectangle = tools.detect_face_12net(cls_prob,roi,out_side,1/scales[i],origin_w,origin_h,threshold[0])</span><br><span class="line">        rectangles.extend(rectangle)</span><br><span class="line">    rectangles = tools.NMS(rectangles,0.7,&apos;iou&apos;)</span><br><span class="line"></span><br><span class="line">    if len(rectangles)==0:</span><br><span class="line">        return rectangles</span><br><span class="line">    net_24.blobs[&apos;data&apos;].reshape(len(rectangles),3,24,24)</span><br><span class="line">    crop_number = 0</span><br><span class="line">    for rectangle in rectangles:</span><br><span class="line">        crop_img = caffe_img[int(rectangle[1]):int(rectangle[3]), int(rectangle[0]):int(rectangle[2])]</span><br><span class="line">        scale_img = cv2.resize(crop_img,(24,24))</span><br><span class="line">        scale_img = np.swapaxes(scale_img, 0, 2)</span><br><span class="line">        net_24.blobs[&apos;data&apos;].data[crop_number] =scale_img</span><br><span class="line">        crop_number += 1</span><br><span class="line">    out = net_24.forward()</span><br><span class="line">    cls_prob = out[&apos;prob1&apos;]</span><br><span class="line">    roi_prob = out[&apos;conv5-2&apos;]</span><br><span class="line">    rectangles = tools.filter_face_24net(cls_prob,roi_prob,rectangles,origin_w,origin_h,threshold[1])</span><br><span class="line"></span><br><span class="line">    if len(rectangles)==0:</span><br><span class="line">        return rectangles</span><br><span class="line">    net_48.blobs[&apos;data&apos;].reshape(len(rectangles),3,48,48)</span><br><span class="line">    crop_number = 0</span><br><span class="line">    for rectangle in rectangles:</span><br><span class="line">        crop_img = caffe_img[int(rectangle[1]):int(rectangle[3]), int(rectangle[0]):int(rectangle[2])]</span><br><span class="line">        scale_img = cv2.resize(crop_img,(48,48))</span><br><span class="line">        scale_img = np.swapaxes(scale_img, 0, 2)</span><br><span class="line">        net_48.blobs[&apos;data&apos;].data[crop_number] =scale_img</span><br><span class="line">        crop_number += 1</span><br><span class="line">    out = net_48.forward()</span><br><span class="line">    cls_prob = out[&apos;prob1&apos;]</span><br><span class="line">    roi_prob = out[&apos;conv6-2&apos;]</span><br><span class="line">    pts_prob = out[&apos;conv6-3&apos;]</span><br><span class="line">    rectangles = tools.filter_face_48net(cls_prob,roi_prob,pts_prob,rectangles,origin_w,origin_h,threshold[2])</span><br><span class="line"></span><br><span class="line">    return rectangles</span><br><span class="line"></span><br><span class="line">threshold = [0.6,0.6,0.7]</span><br><span class="line">imgpath = &quot;&quot;</span><br><span class="line">rectangles = detectFace(imgpath,threshold)</span><br><span class="line">img = cv2.imread(imgpath)</span><br><span class="line">draw = img.copy()</span><br><span class="line">for rectangle in rectangles:</span><br><span class="line">    cv2.putText(draw,str(rectangle[4]),(int(rectangle[0]),int(rectangle[1])),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0))</span><br><span class="line">    cv2.rectangle(draw,(int(rectangle[0]),int(rectangle[1])),(int(rectangle[2]),int(rectangle[3])),(255,0,0),1)</span><br><span class="line">    for i in range(5,15,2):</span><br><span class="line">    cv2.circle(draw,(int(rectangle[i+0]),int(rectangle[i+1])),2,(0,255,0))</span><br><span class="line">cv2.imshow(&quot;test&quot;,draw)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.imwrite(&apos;test.jpg&apos;,draw)</span><br></pre></td></tr></table></figure><p>上面只是一个简单的实现过程的介绍，但是需要实现论文里面的效果，还需要很复杂的处理数据和调参过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;配置环境为win7 64位，主要完成的任务是用MTCNN完成人脸检测，即使用目标检测框将图像中的人脸框出来，配置过程如下：&lt;/p&gt;
&lt;h1 id=&quot;1、环境配置&quot;&gt;&lt;a href=&quot;#1、环境配置&quot; class=&quot;headerlink&quot; title=&quot;1、环境配置&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习算法" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="目标检测" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="人脸检测" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="MTCNN" scheme="http://camlinzhang.com/tags/MTCNN/"/>
    
  </entry>
  
  <entry>
    <title>详细解释递归原理</title>
    <link href="http://camlinzhang.com/2018/04/03/%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%E9%80%92%E5%BD%92%E5%8E%9F%E7%90%86/"/>
    <id>http://camlinzhang.com/2018/04/03/详细解释递归原理/</id>
    <published>2018-04-03T10:58:41.000Z</published>
    <updated>2019-03-25T05:03:02.343Z</updated>
    
    <content type="html"><![CDATA[<p>一直以来对于递归原理都不是很了解，最近找实习非得要学习了，于是今天好好的研究了一下，这里以《剑指offer》上面第93页递归的例子来进行分析。<br>问题是：求a的n次方。<br>下面是一个比较简单的求法的公式：<br>$$a^n=<br>\begin{cases}<br>a^{n/2}<em>a^{n/2}&amp; \text{n为偶数}\<br>a^{(n-1)/2}</em>a^{(n-1)/2}*a&amp; \text{n为奇数}<br>\end{cases}$$</p><p>由以上公式就可以写出下面的代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">double Power(double base, unsigned int exponent) &#123;</span><br><span class="line">// 递归的终止条件</span><br><span class="line">    if (exponent == 0)</span><br><span class="line">        return 1;</span><br><span class="line">    if (exponent == 1)</span><br><span class="line">        return base;</span><br><span class="line">    // 递归进栈和出栈处    </span><br><span class="line">    double result = PowerExponent(base, exponent &gt;&gt; 1);</span><br><span class="line">    result *= result;</span><br><span class="line">    if ((exponent &amp; 0x1)== 1) &#123;</span><br><span class="line"></span><br><span class="line">        result *= base;</span><br><span class="line">        cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里拿一个简单的例子来进行分析：<br>假设我们调用函数为：Power(3, 15);</p><p>1、Power(3, 15)中base=3，exponent=15，递归的终止条件均不满足，进入到      递归进栈和出栈处，此时result = Power(3, 7)（此处exponent &gt;&gt; 1的运算过程为：exponent=15，用二进制表示为1111，右移一位即为0111，表示7，所以exponent变为7，下同）进栈；<br> 2、Power(3, 7)中base=3，exponent=7，递归的终止条件均不满足，进入到递归进栈和出栈处，此时result = Power(3, 3)进栈；<br> 3、Power(3, 3)中base=3，exponent=3，递归的终止条件均不满足，进入到递归进栈和出栈处，此时result = Power(3, 1)进栈；</p><p>此时exponent=1，递归的终止条件第二个if条件满足(注意此处即使有两个终止条件也不要继续往下计算，因为函数遇到return就会终止，每个函数只有一个return)，因此全部进栈完毕，开始出栈</p><p> 4、Power(3, 1)出栈，base=3，exponent=1，可以看做执行函数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">double Power(double base, unsigned int exponent) &#123;</span><br><span class="line">    if (exponent == 1)</span><br><span class="line">        return base;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>即此时在递归进栈和出栈处result = Power(3, 1) = base = 3</p><p>5、Power(3, 3)出栈，base=3，exponent=3，可以看做执行函数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">double Power(double base, unsigned int exponent) &#123;</span><br><span class="line">    // 即为将Power(3, 1)代入函数中来计算Power(3, 3)的值</span><br><span class="line">    double result = Power(3, 1);</span><br><span class="line">    result *= result;</span><br><span class="line">    if ((exponent &amp; 0x1)== 1) &#123;</span><br><span class="line"></span><br><span class="line">        result *= base;</span><br><span class="line">        cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即此时在递归进栈和出栈处result = Power(3, 3) = Power(3, 1)<em> Power(3, 1) </em> base = 3<em>3</em>3 = 27(最后乘base因为exponent=3满足if的条件，其中if的条件中(exponent &amp; 0x1)指的是将exponent与八进制的1即用二进制表示为0001作位与运算，即判断exponent的二进制形式最后一位是否为1，即判断exponent是否为奇数，下同)</p><p>6、Power(3, 7)出栈，base=3，exponent=7，可以看做执行函数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">double Power(double base, unsigned int exponent) &#123;</span><br><span class="line">    // 即为将Power(3, 3)代入函数中来计算Power(3, 7)的值</span><br><span class="line">    double result = Power(3, 5);</span><br><span class="line">    result *= result;</span><br><span class="line">    if ((exponent &amp; 0x1)== 1) &#123;</span><br><span class="line"></span><br><span class="line">        result *= base;</span><br><span class="line">        cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>即此时在递归进栈和出栈处result = Power(3, 7) = Power(3, 3)<em> Power(3, 3) </em> base = 27<em>27</em>3 = 2187(最后乘base因为exponent=3满足if的条件)</p><p>7、最后计算Power(3, 15)，base=3，exponent=15，可以看做执行函数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">double Power(double base, unsigned int exponent) &#123;</span><br><span class="line">    // 即为将Power(3, 7)代入函数中来计算Power(3, 15)的值</span><br><span class="line">    double result = Power(3, 7);</span><br><span class="line">    result *= result;</span><br><span class="line">    if ((exponent &amp; 0x1)== 1) &#123;</span><br><span class="line"></span><br><span class="line">        result *= base;</span><br><span class="line">        cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>即此时result = Power(3, 15) = Power(3, 7)<em> Power(3, 7) </em> base = 2187<em>2187</em>3 = 14348907(最后乘base因为exponent=3满足if的条件)</p><p>（注意出栈的4，5，6，7步中实际的运行是会重新运行一遍<strong>整个函数</strong>的，此处为了强调递归的终止和递归出栈后的执行部分所以将没有运行的代码段部分省略掉了）</p><h2 id="规律总结"><a href="#规律总结" class="headerlink" title="规律总结"></a>规律总结</h2><p>从上面的步骤分析可以看出，递归在没有满足递归终止条件的时候，将每次递归的中间值都进栈，因此每一次递归的过程中相当于在递归进栈和出栈处打了一个断点，然后在第一次满足递归终止条件时将会开始出栈，出栈实际上是从每一次停止的断点处开始执行，将这一次递归得到的值返回给上一次递归，并以此继续进行下去，下面几个经典的递归例子可以仿照上面的分析过程进行分析。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 计算阶乘</span><br><span class="line">long factorial_recursion(int n)&#123;</span><br><span class="line"></span><br><span class="line">    if(n &lt;= 0)</span><br><span class="line">        return 1;</span><br><span class="line"></span><br><span class="line">    else</span><br><span class="line">        return n * factorial_recursion(n-1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 计算Fibonacci数列</span><br><span class="line">long long Fibonacci(unsigned int n)&#123;</span><br><span class="line"></span><br><span class="line">    if (n &lt;= 0)</span><br><span class="line">        return 0;</span><br><span class="line">    if (n == 1)</span><br><span class="line">        return 1;</span><br><span class="line"></span><br><span class="line">    return Fibonacci(n-1) + Fibonacci(n - 2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直以来对于递归原理都不是很了解，最近找实习非得要学习了，于是今天好好的研究了一下，这里以《剑指offer》上面第93页递归的例子来进行分析。&lt;br&gt;问题是：求a的n次方。&lt;br&gt;下面是一个比较简单的求法的公式：&lt;br&gt;$$a^n=&lt;br&gt;\begin{cases}&lt;br&gt;
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="代码学习" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法题" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
    
      <category term="递归" scheme="http://camlinzhang.com/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Unbuntu配置Caffe以及调试DeepLab记录</title>
    <link href="http://camlinzhang.com/2018/01/03/Unbuntu%E9%85%8D%E7%BD%AECaffe%E4%BB%A5%E5%8F%8A%E8%B0%83%E8%AF%95DeepLab%E8%AE%B0%E5%BD%95/"/>
    <id>http://camlinzhang.com/2018/01/03/Unbuntu配置Caffe以及调试DeepLab记录/</id>
    <published>2018-01-03T02:14:51.000Z</published>
    <updated>2019-03-25T05:00:38.423Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文档主要是记录这一个星期以来配置caffe和deeplab，以备以后忘记了使用。中间遇到无数坑，但是总算是配置好了。</p><h2 id="配置caffe"><a href="#配置caffe" class="headerlink" title="配置caffe"></a>配置caffe</h2><p>首先我是拿到了一台导师给的空空的服务器，所以参考下面的文章首先进行了配置：<br><a href="http://www.linuxidc.com/Linux/2016-12/138870.htm" target="_blank" rel="noopener">Ubuntu 16.04 安装配置Caffe 图文详解</a></p><p>上面的配置过程中遇到以下坑：<br>1、配置NVIDIA驱动的时候选择了NVIDIA-Linux-x86_64-384.98.run这个包，对应型号是GeForce的TITAN Xp。<br>2、安装NVIDIA驱动的时候由于要取消使用本机自带的驱动，所以此时重启一下看到ubuntu系统的信息中图形中为llvmpipe (LLVM 5.0, 256 bits)，一查才知道是OpenGL，所以在安装的时候记得使用命令：</p><blockquote><p>sudo ./NVIDIA-Linux-x86_64-375.20.run –no-opengl-files</p></blockquote><p>后面的–no-opengl-files代表不安装OpenGL，否则就会陷入反复重启的无奈中，参考：<br><a href="https://www.jianshu.com/p/80c9b42dca9b?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">ubuntu 16.04 安装英伟达gtx1080显卡驱动 解决反复重启问题 以及 cuda8.0 cudnn 安装</a></p><p>3、安装CUDA的时候又是一个巨大的坑，导致反复重装多遍，大家不要用最新版的9.0，因为会有很多的兼容问题，一定找到之前的8.0版本就行，就是上面配置文档中的版本。<br><strong><em>还有就是特别注意安装的过程中一定要注意提示！！！！！</em></strong><br>注意：执行后会有一系列提示让你确认，但是注意，有个让你选择是否安装nvidia367驱动时，一定要选择否：<br>Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 367.48?<br>因为前面我们已经安装了更加新的nvidia367，所以这里不要选择安装。其余的都直接默认或者选择是即可。</p><p>我就是没注意所以又得卸载重装，不过也有文档说也可以就装这个版本，但是毕竟我们找的NVIDIA版本是和我们的GPU最合适的，所以还是不要选了吧。</p><p>4、我的python是anaconda自带的python，很方便就可以安装python和很多科学计算的库，大家也可以参考：<br><a href="http://blog.csdn.net/qq_36620489/article/details/73658151" target="_blank" rel="noopener">ubuntu 16.04 +caffe+anaconda (CPU) 安裝詳細教程</a><br>进行anaconda的配置。</p><p>5、在配置的过程中还遇到了两个问题找了很久才找到的两个很好的解决方案：<br>(1)一个是关于matio的配置问题：<br><a href="https://github.com/TheLegendAli/DeepLab-Context/issues/8" target="_blank" rel="noopener">https://github.com/TheLegendAli/DeepLab-Context/issues/8</a><br><img src="http://img.blog.csdn.net/20180103094548096?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这位大佬的答案完美解决问题"><br>其中的文件大家可以在上面链接中找到下载地址。<br>其中的cmake文件的产生是用下面这个方法来编译生成的：<br><a href="https://github.com/BVLC/caffe/issues/3671" target="_blank" rel="noopener">https://github.com/BVLC/caffe/issues/3671</a><br><img src="http://img.blog.csdn.net/20180103094914965?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>(2)还有就是一直出现的找不到opencv_dep_cudart<br>我在这里找到答案：<br><a href="https://stackoverflow.com/questions/37534604/opencv-with-cmake-version-3-5-2-vs-cmake-2-x-x" target="_blank" rel="noopener">https://stackoverflow.com/questions/37534604/opencv-with-cmake-version-3-5-2-vs-cmake-2-x-x</a><br>其中的：</p><blockquote><p>sudo cmake .. -DCUDA_USE_STATIC_CUDA_RUNTIME=false</p></blockquote><p>完美解决，这里的cmake就是上面(1)中第二幅图的cmake时候使用。</p><p>基本上就是这些方法帮助我解决了配置的问题，每台机器的“个性”可能不太一样，所以大家在配置的时候会遇到各种坑，一个一个bug慢慢解决吧，最终总会成功的。</p><h2 id="配置DeepLab"><a href="#配置DeepLab" class="headerlink" title="配置DeepLab"></a>配置DeepLab</h2><p>其中参考了：<br><a href="http://blog.csdn.net/xmo_jiao" target="_blank" rel="noopener">http://blog.csdn.net/xmo_jiao</a><br>这个大牛的一系列配置文章，但是最终发现还是这个简单的比较好：<br><a href="http://blog.csdn.net/ruotianxia/article/details/78331964" target="_blank" rel="noopener">http://blog.csdn.net/ruotianxia/article/details/78331964</a></p><p>其中也有一些一些我遇到的坑：<br>1、首先是# USE_CUDNN := 1，我如果注释掉会出现很多bug，找不到各种层的文件，但是注释掉这个就好了，我也不知道为啥，有哪位大牛知道望告知（感谢！！！）</p><p>2、再有就是编译的时候一直显示找不到/.bulid_release/caffe.bin，闹了很久这个问题，主要是编译的问题，不要按照上面编译caffe时候用cmake编译，上面xmo_jiao的代码里面已经有了cmake编译好的文件，直接按照教程make就好了，还有就是run_pascal.sh文件里面按照下面教程里面的：</p><blockquote><p>CAFFE_BIN=${CAFFE_DIR}/build/tools/caffe.bin</p></blockquote><p>来填，而不是/.bulid_release/caffe.bin，因为我这样填一直找不到这个caffe.bin。</p><p>3、再就是这个Error loading shared library libhdf5_hl.so.XXX的问题，也是弄了很久，其实就是将anaconda2的lib文件夹里面的相关的这个文件拷贝到/usr/lib/x86_64-linux-gnu/这个文件夹下就好了。但是还是很困惑，我用</p><blockquote><p>ldconfig -p | less</p></blockquote><p>这个命令查看到很多这个报错的库都是已经在共享了，为什么还报错呢（也希望大牛告知）。</p><p>反正就是最终按照上面的第一篇文档就是将数据集融合了一下，始终没有配置成功，但是第二篇文档的步骤配置成功了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇文档主要是记录这一个星期以来配置caffe和deeplab，以备以后忘记了使用。中间遇到无数坑，但是总算是配置好了。&lt;/p&gt;
&lt;h2 id=&quot;配置caffe&quot;&gt;&lt;a href=&quot;#配置caffe&quot; class=&quot;headerlink&quot; title=&quot;配置caffe&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习环境" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    
      <category term="环境配置" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="Ubuntu" scheme="http://camlinzhang.com/tags/Ubuntu/"/>
    
      <category term="Caffe" scheme="http://camlinzhang.com/tags/Caffe/"/>
    
      <category term="DeepLab" scheme="http://camlinzhang.com/tags/DeepLab/"/>
    
  </entry>
  
  <entry>
    <title>mac环境下上传项目到github</title>
    <link href="http://camlinzhang.com/2017/10/02/mac%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0github/"/>
    <id>http://camlinzhang.com/2017/10/02/mac环境下上传项目到github/</id>
    <published>2017-10-02T09:41:19.000Z</published>
    <updated>2019-03-25T04:20:20.698Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备："><a href="#准备：" class="headerlink" title="准备："></a>准备：</h2><ul><li>1 下载安装git客户端 <a href="http://code.google.com/p/git-osx-installer/downloads/list?can=3" target="_blank" rel="noopener">http://code.google.com/p/git-osx-installer/downloads/list?can=3</a></li><li>2 注册github账号 <a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a></li></ul><h2 id="创建ssh"><a href="#创建ssh" class="headerlink" title="创建ssh"></a>创建ssh</h2><p>首先看自己电脑里面有没有安装ssh</p><p>方法一：<br>显示finder根目录下的所有内容（包括隐藏该文件）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -a</span><br></pre></td></tr></table></figure></p><p>如果有.ssh文件就将其删除，或者自己备份一份（因为这个文件并没有多大用处，之前弄树莓派的时候经常用ssh连接会保存一些ip地址，但是感觉并没有什么用，下次连接输入ip之后还会自动保存的）</p><p>方法二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ defaults write com.apple.finder AppleShowAllFiles -bool true  //  终端 显示隐藏文件（需要重新运行Finder)。将上面的true改为false就是不显示隐藏文件</span><br></pre></td></tr></table></figure><p>然后新建一个ssh目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir .ssh</span><br></pre></td></tr></table></figure></p><p>进入ssh的目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd .ssh</span><br></pre></td></tr></table></figure></p><p>新建一个github的ssh的连接:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &quot;your_email@example.com&quot;       //后面“ ”里面 输入之前注册github账号时候的邮箱，并输入密码。</span><br></pre></td></tr></table></figure></p><p>查看是否存在 id_rsa(私钥)  id_rsa.pub(公钥) 这两个东西，如果存在就成功了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -la</span><br></pre></td></tr></table></figure></p><p>将上面的公钥信息复制到剪贴板用于后面新建一个ssh连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pbcopy &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>登陆到github中进入个人账户的设置中选择SSH and GPG keys选项，再选择new SSH key，将其中的Title填上一个用于标识的名称（例如你的邮箱账号），Key填上刚刚复制到剪贴板的公钥信息粘贴到这里面，点击Add SSH key就可以新建一个ssh连接了。新建好后如下图所示：<br><img src="http://img.blog.csdn.net/20171002172301876?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><h2 id="连接github"><a href="#连接github" class="headerlink" title="连接github"></a>连接github</h2><p>输入命令来测试连接是否正常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>如果显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi username! You&apos;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>则代表连接正常。<br>接着就需要在github中新建一个个人项目：<br><img src="http://img.blog.csdn.net/20171002172708320?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>如上图新建完成后就可以将本地的项目上传到github上了：</p><h2 id="上传项目"><a href="#上传项目" class="headerlink" title="上传项目"></a>上传项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cd 你的项目目录 //进入你需要同步的项目根目录</span><br><span class="line">$ touch README.md //新建一个记录提交操作的文档</span><br><span class="line">$ git init //初始化本地仓库</span><br><span class="line">$ git add README.md //添加</span><br><span class="line">$ git add *  //加入所有项目</span><br><span class="line">$ git status //检查状态 如果都是绿的 证明成功</span><br><span class="line">$ git commit -m &quot;first commit&quot;//提交到要地仓库，并写一些注释</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20171002173243127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin http://github.com/yourname/Test.git //连接远程仓库,origin后面填写你新建的个人项目中如图所示处的链接，并建了一个名叫：origin的别名</span><br><span class="line">$ git push -u origin master //将本地仓库的东西提交到地址是origin的地址，master分支下</span><br></pre></td></tr></table></figure></p><h2 id="相关错误及解决办法"><a href="#相关错误及解决办法" class="headerlink" title="相关错误及解决办法"></a>相关错误及解决办法</h2><p>提示出错信息：fatal: remote origin already exists.</p><p>解决办法如下：</p><p>1、先输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote rm origin</span><br></pre></td></tr></table></figure></p><p>2、再输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin http://github.com/yourname/Test.git</span><br></pre></td></tr></table></figure></p><p>就不会报错了！</p><p>3、如果输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote rm origin</span><br></pre></td></tr></table></figure></p><p>还是报错的话，error: Could not remove config section ‘remote.origin’. 我们需要修改gitconfig文件的内容</p><p>4、找到你的github的安装路径，我的是C:\Users\ASUS\AppData\Local\GitHub\PortableGit_ca477551eeb4aea0e4ae9fcd3358bd96720bb5c8\etc</p><p>5、找到一个名为gitconfig的文件，打开它把里面的[remote “origin”]那一行删掉就好了！</p><p>如果输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure></p><p>出现错误提示：Permission denied (publickey).因为新生成的key不能加入ssh就会导致连接不上github。</p><p>解决办法如下：</p><p>1、先输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-agent</span><br></pre></td></tr></table></figure></p><p>，再输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add ~/.ssh/id_key</span><br></pre></td></tr></table></figure></p><p>，这样就可以了。</p><p>2、如果还是不行的话，输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add ~/.ssh/id_key</span><br></pre></td></tr></table></figure></p><p>命令后出现报错Could not open a connection to your authentication agent.解决方法是key用Git Gui的ssh工具生成，这样生成的时候key就直接保存在ssh中了，不需要再ssh-add命令加入了，其它的user，token等配置都用命令行来做。</p><p>3、最好检查一下在你复制id_rsa.pub文件的内容时有没有产生多余的空格或空行，有些编辑器会帮你添加这些的。</p><p>如果输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure></p><p>提示出错信息：error:failed to push som refs to …….</p><p>解决办法如下：</p><p>1、先输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git pull origin master //先把远程服务器github上面的文件拉下来</span><br></pre></td></tr></table></figure></p><p>2、再输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure></p><p>3、如果出现报错 fatal: Couldn’t find remote ref master或者fatal: ‘origin’ does not appear to be a git repository以及fatal: Could not read from remote repository.</p><p>4、则需要重新输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin http://github.com/yourname/Test.git</span><br></pre></td></tr></table></figure></p><p>5、如果依然报之前的错误，可以参考下面的有关git push命令的链接：<br><a href="http://www.cnblogs.com/renkangke/archive/2013/05/31/conquerAndroid.html" target="_blank" rel="noopener">http://www.cnblogs.com/renkangke/archive/2013/05/31/conquerAndroid.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;准备：&quot;&gt;&lt;a href=&quot;#准备：&quot; class=&quot;headerlink&quot; title=&quot;准备：&quot;&gt;&lt;/a&gt;准备：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;1 下载安装git客户端 &lt;a href=&quot;http://code.google.com/p/git-osx-insta
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="其他" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="github" scheme="http://camlinzhang.com/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络相关知识以及数学推导</title>
    <link href="http://camlinzhang.com/2017/09/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/"/>
    <id>http://camlinzhang.com/2017/09/29/卷积神经网络相关知识以及数学推导/</id>
    <published>2017-09-29T12:37:43.000Z</published>
    <updated>2019-03-25T05:00:37.090Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h1><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><p><img src="http://img.blog.csdn.net/20170929161737407?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>以上就是经典的“M-P神经元模型”。在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。</p><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机（Perceptron）是由两层神经元组成，输入层接收外界输出信号后传递给输出层，输出层是M-P神经元，也称为“阈值逻辑单元”。</p><p><img src="http://img.blog.csdn.net/20170929164741558?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>感知机能够很容易的实现逻辑与、或、非运算，但是由于其只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，所以学习能力非常有限。此时就产生了多层感知机（MLP）来处理这些更加复杂的运算。</p><p>对于以上的感知机，我们可以建立模型：<br>$$f(x)=act(θ^T+b)$$<br>其中激活函数 act 可以使用{sign, sigmoid, tanh}之一，个人感觉这种建模方式就是将每个神经元的输入作为笛卡尔坐标系中的x轴的值，对应的输出值作为y轴上的值，通过这些已知的训练集合中的值来进行拟合，根据训练集在坐标系中的分布特征来选择不同的激活函数，也即是拟合方式的不同，就比如下面的线性回归，逻辑回归以及softmax回归。</p><ul><li>激活函数使用符号函数 sign ，可求解损失函数最小化问题，通过梯度下降确定参数</li><li>激活函数若使用一次多项式进行拟合，就成为了线性回归，但是一般不用此种回归方法来拟合。</li><li>激活函数使用 sigmoid （或者 tanh ），即解决的问题为二分问题，则分类器事实上成为Logistic Regression，可通过梯度上升极大化似然函数，或者梯度下降极小化损失函数，来确定参数。</li><li>如果需要多分类，则事实上成为Softmax Regression。</li><li>如要需要分离超平面恰好位于正例和负例的正中央，则成为支持向量机（SVM）。</li></ul><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p>感知机存在的问题是，对线性可分数据工作良好，如果设定迭代次数上限，则也能一定程度上处理近似线性可分数据。但是对于非线性可分的数据，比如最简单的异或问题，感知器就无能为力了。这时候就需要引入多层感知器这个大杀器。</p><p>多层感知器的思路是，尽管原始数据是非线性可分的，但是可以通过某种方法将其映射到一个线性可分的高维空间中，从而使用线性分类器完成分类。下面卷积神经网络概述神经网络的大体结构图中，从X到O这几层，正展示了多层感知器的一个典型结构，即输入层-隐层-输出层。</p><p><strong><em>输入层-隐层</em></strong><br>是一个全连接的网络，即每个输入节点都连接到所有的隐层节点上。更详细地说，可以把输入层视为一个向量 x ，而隐层节点 j有一个权值向量$\theta_j$以及偏置$b_j$，激活函数使用 sigmoid 或 tanh ，那么这个隐层节点的输出应该是<br>$$f_j(x) = act(\theta_j^Tx + b_j)$$<br>也就是每个隐层节点都相当于一个感知器。每个隐层节点产生一个输出，那么隐层所有节点的输出就成为一个向量，即<br>$$f(x) =  act({\Theta}x + b)$$<br>若输入层有$m$个节点，隐层有$n$个节点，那么$\Theta = [\theta^T]$为$n×m$的矩阵，$x$为长为$m$的向量，$b$为长为$n$的向量，激活函数作用在向量的每个分量上， $f(x)$返回一个向量。</p><p><strong><em>隐层-输出层</em></strong><br>可以视为级联在隐层上的一个感知器。若为二分类，则常用Logistic Regression；若为多分类，则常用Softmax Regression。</p><h2 id="解决非线性最优问题的常见算法"><a href="#解决非线性最优问题的常见算法" class="headerlink" title="解决非线性最优问题的常见算法"></a>解决非线性最优问题的常见算法</h2><p>为讨论下面几种算法，采用最简单的线性回归作为例子。相关的参数如下（一般机器学习中都相关问题都是采用下列的参数）：<br>$n$：训练集合的特征数量，例如房价预测中的房子的大小和卧室的数量<br>$m$：训练样本的数量<br>$x$：输入变量/特征<br>$y$：输出变量/特征<br>$(x,y)$：训练样例<br>$i^{th}$：训练样例的组数，表示为：$(x^{(i)},y^{(i)})$<br>$h(x)$：拟合函数<br>$J(x)$：损失函数</p><p><img src="http://img.blog.csdn.net/20170929213522108?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>（<strong>此处注意上面推出的这个迭代式，下面要与逻辑回归中推出的迭代式进行比较</strong>）<br>（3）随机梯度下降法（stochastic gradient descent，SGD）<br>SGD是最速梯度下降法的变种。<br>使用最速梯度下降法，将进行N次迭代，直到目标函数收敛，或者到达某个既定的收敛界限。每次迭代都将对m个样本进行计算，计算量大。<br>为了简便计算，SGD每次迭代仅对一个样本计算梯度，直到收敛。伪代码如下（以下仅为一个loop，实际上可以有多个这样的loop，直到收敛）：</p><p><img src="http://img.blog.csdn.net/20170929213650086?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>1、由于SGD每次迭代只使用一个训练样本，因此这种方法也可用作online learning。<br>2、每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。<br>（4）牛顿法</p><p><img src="http://img.blog.csdn.net/20170930114654194?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>（5）高斯牛顿法<br>以上两种方法的详细数学推导<a href="http://blog.csdn.net/jinshengtao/article/details/51615162" target="_blank" rel="noopener">http://blog.csdn.net/jinshengtao/article/details/51615162</a></p><h2 id="线性回归和逻辑回归"><a href="#线性回归和逻辑回归" class="headerlink" title="线性回归和逻辑回归"></a>线性回归和逻辑回归</h2><p>线性回归的一个具体实现可以从上面解决非线性最优问题的常见算法的例子里学习，下面首先给出一个对于线性回归的一个可能的解释。</p><p><strong><em>Probabilistic Interpretation</em></strong></p><p><img src="http://img.blog.csdn.net/20170930115503214?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930115516219?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930115535492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p><strong><em>局部加权回归（Locally weight regression，Loess/Lowess）</em></strong></p><p><img src="http://img.blog.csdn.net/20170930120202190?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930120211770?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p><strong><em>逻辑回归（Logistic Regression）</em></strong></p><p><img src="http://img.blog.csdn.net/20170930125024943?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930124954303?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><h2 id="误差逆传播（BP）算法"><a href="#误差逆传播（BP）算法" class="headerlink" title="误差逆传播（BP）算法"></a>误差逆传播（BP）算法</h2><p>通过以上的介绍我们弄清楚了神经网络的结构，常见的神经网络有多层前馈网络（每层神经元与下一层神经元全连接，神经元之间不存在同层连接，也不存在跨层连接），下面就是介绍训练类似的多层网络（即估计权重和阈值这些参数）的方法了。对于一般的问题，可以通过求解损失函数极小化问题来进行参数估计。但是对于多层感知器中的隐层，因为无法直接得到其输出值，当然不能够直接使用到其损失了。这时，就需要将损失从顶层反向传播（Back Propagate）到隐层，来完成参数估计的目标。</p><p>首先，我们给出下面的BP网络（用BP算法训练的多层前馈神经网络）：<br><img src="http://img.blog.csdn.net/20170929185240000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>给定训练集$D={(x_1,y_1),(x_2,y_2),……,(x_m,y_m)},x_i∈R^d,y_i∈R^l$，即输入示例由$d$个属性描述，输出$l$维实值向量。为了便于讨论，上图给出了一个拥有$d$个神经元、$l$个输出神经元、$q$个隐层神经元的多层前馈网络结构，其中输出层第$j$个神经元的阈值用$θ_j$表示，隐层第$h$个神经元的阈值用$γ<em>h$表示。输出层第$i$个神经元与隐层第$h$个神经元之间的连接权为$ν</em>{ih}$，隐层第$h$个神经元与输出层第$j$个神经元之间的连接权为$ω_{hj}$。记隐层第$h$个神经元接收到的输入为$α_h=\sum_d^{i=1}ν_{ih}x_i$,输出层第$j$个神经元接收到的输入为$β<em>j=\sum</em>{h=1}^qω_{hj}b_h$，其中$b_h$为隐层第$h$个神经元的输出。假设隐层和输出层神经元都使用sigmoid函数。<br>具体推导过程如下两张图片所示：</p><p><img src="http://img.blog.csdn.net/20170929203736362?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170929203211369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" alt="图片还在路上，稍等..."></p><p>最后补充几个常用的激活函数的导数结果：</p><p>$\begin {aligned}f’(x) &amp; = sigmoid’(x) = f(x)(1 - f(x))\ f’(x) &amp; = tanh’(x) = 1 - f^2(x)\ f’(x) &amp; = softmax’(x) = f(x) - f^2(x) \end{aligned}$</p><p>将以上的数学推导过程转化成伪代码为：</p><p><img src="http://img.blog.csdn.net/20170929204620853?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" style="text-align:center" alt="图片还在路上，稍等..."></p><h1 id="卷积神经网络概述"><a href="#卷积神经网络概述" class="headerlink" title="卷积神经网络概述"></a>卷积神经网络概述</h1><p>卷积神经网络沿用了普通的神经元网络即多层感知器的结构，是一个前馈网络（网络拓扑结构上不存在环或者回路）。以应用于图像领域的CNN为例，大体结构如下图所示：</p><p><img src="http://img.blog.csdn.net/20170929155602955?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" alt="图片还在路上，稍等..."></p><p>很明显，这个典型的结构分为四个大层次：</p><ul><li>输入图像I。为了减小复杂度，一般使用灰度图像。当然，也可以使用RGB彩色图像，此时输入图像有三张，分别为RGB分量。输入图像一般需要归一化，如果使用sigmoid激活函数，则归一化到[0, 1]，如果使用tanh激活函数，则归一化到[-1, 1]。</li><li>多个卷积（C）-下采样（S）层。将上一层的输出与本层权重W做卷积得到各个C层，然后下采样（池化）得到各个S层。这些层的输出称为Feature Map。</li><li>光栅化（X）。是为了与传统的多层感知器全连接。即将上一层的所有Feature Map的每个像素依次展开，排成一列。</li><li>传统的多层感知器（N&amp;O）。最后的分类器一般使用Softmax Regression(针对多分类问题)，如果是二分类，当然也可以使用Logistic Regression。</li></ul><p>根据上面的基本结构，我们就逐层进行分析。</p><h1 id="卷积层和下采样层"><a href="#卷积层和下采样层" class="headerlink" title="卷积层和下采样层"></a>卷积层和下采样层</h1><p>以上介绍的多层感知机存在一定的问题，它是一个全连接的网络，因此在输入比较大的时候，权值会特别多。比如一个有1000个节点的隐层，连接到一个1000×1000的图像上，那么就需要 10^9 个权值参数（外加1000个偏置参数）！这个问题，一方面限制了每层能够容纳的最大神经元数目，另一方面也限制了多层感知器的层数即深度。<br>多层感知器的另一个问题是梯度发散。一般情况下，我们需要把输入归一化，而每个神经元的输出在激活函数的作用下也是归一化的；另外，有效的参数其绝对值也一般是小于1的；这样，在BP过程中，多个小于1的数连乘，得到的会是更小的值。也就是说，在深度增加的情况下，从后传播到前边的残差会越来越小，甚至对更新权值起不到帮助，从而失去训练效果，使得前边层的参数趋于随机化（补充一下，其实随机参数也是能一定程度上捕捉到图像边缘的）。<br>有关神经网络训练过程中梯度的有关问题可以详细见：<br><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap5/c5s0.html" target="_blank" rel="noopener">https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap5/c5s0.html</a></p><p>既然多层感知器存在问题，那么卷积神经网络的出现，就是为了解决它的问题。卷积神经网络的核心出发点有三个。</p><ul><li><strong>局部感受野</strong>。形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。举个栗子，依旧是1000×1000的图像，使用10×10的感受野，那么每个神经元只需要100个权值参数；不幸的是，由于需要将输入图像扫描一遍，共需要991×991个神经元！参数数目减少了一个数量级，不过还是太多。</li><li><strong>权值共享</strong>。形象地说，就如同你的某个神经中枢中的神经细胞，它们的结构、功能是相同的，甚至是可以互相替代的。也就是，在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。继续上一个栗子，虽然需要991×991个神经元，但是它们的权值是共享的呀，所以还是只需要100个权值参数，以及1个偏置参数。从MLP的 10^9 到这里的100，就是这么狠！作为补充，在CNN中的每个隐藏，一般会有多个卷积核。</li><li><strong>池化</strong>。形象地说，你先随便看向远方，然后闭上眼睛，你仍然记得看到了些什么，但是你能完全回忆起你刚刚看到的每一个细节吗？同样，在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</li></ul><p>更加生动形象的对于这三个问题的解释可以参考以下的文章：<br><a href="http://blog.csdn.net/stdcoutzyx/article/details/41596663/" target="_blank" rel="noopener">http://blog.csdn.net/stdcoutzyx/article/details/41596663/</a></p><p>现在来看，需要训练参数过多的问题已经完美解决。关于梯度发散，因为多个神经元共享权值，因此它们也会对同一个权值进行修正，积少成多，积少成多，积少成多，从而一定程度上解决梯度发散的问题！</p><p>接下来有关卷积的问题可以参考下面这两篇文章：<br><a href="http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi#toc_0" target="_blank" rel="noopener">卷积神经网络全面解析</a>（之前的很多内容也是参考这篇文章里面的）<br>以及一些有关图像语义分割的反卷积的知识：<a href="http://blog.csdn.net/fate_fjh/article/details/52882134" target="_blank" rel="noopener">图像卷积与反卷积</a></p><h2 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h2><p><strong><em>指数分布族（The Exponential Family）</em></strong></p><p>如果一个分布可以用如下公式表达，那么这个分布就属于指数分布族：<br>$$p(y;η)=b(y)exp(η^TT(y)-a(η))$$<br>公式中y是随机变量；<br>η称为分布的自然参数(natural parameter)，也称为标准参数(canonical parameter)；<br>T(y)称为充分统计量，通常情况下T(y)=y；<br>a(η)称为对数分割函数(log partition function)，本质上是一个归一化常数，确保概率和为1。</p><p>当T(y)被固定时，a(η)、b(y)就定义了一个以η为参数的一个指数分布。我们变化η就得到不同的概率分布。<br>在$T(y)=y$的通常情况下，η也仅仅是个实数，所以$η^TT(y)$也是实数。</p><p>下面是几个小例子：</p><p><img src="http://img.blog.csdn.net/20170930162526052?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930162709685?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" alt="图片还在路上，稍等..."></p><p><strong><em>广义线性模型（GLM）</em></strong></p><p>在分类和回归问题中，我们通过构建一个关于x的模型来预测y。这种问题可以利用广义线性模型（Generalized linear models，GMLs）来解决。构建广义线性模型我们基于三个假设，也可以理解为我们基于三个设计决策，这三个决策帮助我们构建广义线性模型：</p><ol><li>$y|x;θ∽ExponentialFamily(η)$，假设$y|x;θ$满足一个以为参数的指数分布。例如，给定了输入x和参数θ，那么可以构建y关于η的表达式。</li><li>给定x，我们的目标是要确定T(y)，即$h(x)=E[T(y)|x]$。大多数情况下$T(y)=y$，那么我们实际上要确定的是。即给定x，假设我们的目标函数是$h(x)=E[T(y)|x]$。</li><li>假设自然参数η和x是线性相关，即假设：$η=θ^Tx$。</li></ol><p>相关问题可以参考：<br><a href="http://www.cnblogs.com/BYRans/p/4735409.html" target="_blank" rel="noopener">http://www.cnblogs.com/BYRans/p/4735409.html</a></p><p><strong><em>多分类问题</em></strong><br>多分类问题符合多项分布。有许多算法可用于解决多分类问题，像决策树、朴素贝叶斯等。这篇文章主要讲解多分类算法中的Softmax回归（Softmax Regression)</p><p>   推导思路为：首先证明多项分布属于指数分布族，这样就可以使用广义线性模型来拟合这个多项分布，由广义线性模型推导出的目标函数$h_θ(x)$即为Softmax回归的分类模型。</p><p><img src="http://img.blog.csdn.net/20170930174117037?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" alt="图片还在路上，稍等..."><br><img src="http://img.blog.csdn.net/20170930174139923?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" alt="图片还在路上，稍等..."></p><p>上面的推导过程是参考：<a href="http://www.cnblogs.com/BYRans/p/4905420.html这篇博文的，有关于Softmax回归的更多问题可以看：" target="_blank" rel="noopener">http://www.cnblogs.com/BYRans/p/4905420.html这篇博文的，有关于Softmax回归的更多问题可以看：</a><br><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="noopener">Softmax回归</a></p><p>至此，整个卷积神经网络的相关知识就总结完了，其中参考了很多大牛们的博客，写这篇博客也是为了自己总结梳理相关的知识，本人的能力有限，如有错误的地方还请大家留言指正。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;神经网络概述&quot;&gt;&lt;a href=&quot;#神经网络概述&quot; class=&quot;headerlink&quot; title=&quot;神经网络概述&quot;&gt;&lt;/a&gt;神经网络概述&lt;/h1&gt;&lt;h2 id=&quot;神经元模型&quot;&gt;&lt;a href=&quot;#神经元模型&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="深度学习基本知识" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="卷积神经网络" scheme="http://camlinzhang.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="数学推导" scheme="http://camlinzhang.com/tags/%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>Mac上Anaconda+Tensorflow安装</title>
    <link href="http://camlinzhang.com/2017/07/06/Mac%E4%B8%8AAnaconda-Tensorflow%E5%AE%89%E8%A3%85/"/>
    <id>http://camlinzhang.com/2017/07/06/Mac上Anaconda-Tensorflow安装/</id>
    <published>2017-07-06T14:59:36.000Z</published>
    <updated>2019-03-25T04:10:27.633Z</updated>
    
    <content type="html"><![CDATA[<p>终于准备开始学习谷歌的深度学习框架tensorflow了，花了一上午的时间，终于配置好了，下面就是详细的步骤：</p><p>首先大家可以参考一下官方的安装tensorflow的方法：<br><a href="https://www.tensorflow.org/install/install_mac" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_mac</a></p><p>官方提供了<br>1、virtualenv<br>2、”native” pip<br>3、Docker<br>4、installing from sources, which is for experts and is documented in a separate guide.(也就是Installing with Anaconda)<br>四种方法，我们采用最后一种方法，因为参考《TensorFlow实战》中是采用这种方法（书里面说Anaconda是python的一个科学计算发行版，里面集成了很多的依赖库，所以提供了一个很好的编译环境）。</p><h2 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a><strong>安装Anaconda</strong></h2><p>所以，首先我们就需要下载Anaconda，官方网站为：<br><a href="https://www.continuum.io/downloads" target="_blank" rel="noopener">https://www.continuum.io/downloads</a><br>mac系统，python2.7版本对应的下载链接有以下两种：<br><img src="http://img.blog.csdn.net/20170706193726838?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>上面一种是图形界面的Anaconda，下面一种是命令行形式的Anaconda，我选择的是上面一种(作为程序员中的菜鸟还是觉得图形界面更加舒服点)。</p><p>下载完成以后，会在根目录下面多一个anaconda的文件夹(安装过程中没有指定路径的前提下，不过最好就安装在这里)：<br><img src="http://img.blog.csdn.net/20170706194057681?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>安装好Anaconda之后，可以进入终端利用Anaconda的包管理工具来进行一些简单的操作：<br>查询安装信息<br><figure class="highlight plain"><figcaption><span>conda info```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">查询当前已经安装的库</span><br><span class="line">```$ conda list</span><br></pre></td></tr></table></figure></p><p>安装库(***代表库名称）<br><figure class="highlight plain"><figcaption><span>conda install ***```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">更新库</span><br><span class="line">```$ conda update ***</span><br></pre></td></tr></table></figure></p><p>还有一个问题就是Anaconda仓库镜像，很多地方说官方下载很慢，需要换成清华的镜像，但是奇怪的是我换了之后还下载不了了，换成官方的之后倒是又能够下了，此处还是摆出来给有需要的同学吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">$ conda config --set show_channel_urls yes</span><br><span class="line">$ conda install numpy   #测试是否添加成功</span><br></pre></td></tr></table></figure><p>之后会自动在用户根目录生成“.condarc”文件，可以在终端用<br><figure class="highlight plain"><figcaption><span>ls -a```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">命令查看该文件，如果要删除镜像，直接删除“.condarc”文件即可：</span><br><span class="line">```$ rm .condarc</span><br></pre></td></tr></table></figure></p><p>至此，安装Anaconda的任务就完成了，有需要了解conda的使用方法的，也可以查看下面这篇博文：<a href="http://www.cnblogs.com/harvey888/p/5465452.html" target="_blank" rel="noopener">http://www.cnblogs.com/harvey888/p/5465452.html</a></p><h2 id="在Anaconda环境中安装TensorFlow"><a href="#在Anaconda环境中安装TensorFlow" class="headerlink" title="在Anaconda环境中安装TensorFlow"></a><strong>在Anaconda环境中安装TensorFlow</strong></h2><p>此处采用pip方式来进行安装：<br>pip方式需要首先激活conda环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source activate tensorflow</span><br></pre></td></tr></table></figure><p>然后根据要安装的不同tensorflow版本选择对应的一条环境变量设置export语句（操作系统，Python版本，CPU版本还是CPU+GPU版本）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># Ubuntu/Linux 64-bit, CPU only, Python 2.7  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7  </span><br><span class="line"># Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see &quot;Install from sources&quot; below.  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Mac OS X, CPU only, Python 2.7:  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.1-py2-none-any.whl  </span><br><span class="line"></span><br><span class="line"># Mac OS X, GPU enabled, Python 2.7:  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-1.2.1-py2-none-any.whl  </span><br><span class="line"></span><br><span class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.4  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.4  </span><br><span class="line"># Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see &quot;Install from sources&quot; below.  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.5  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp35-cp35m-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.5  </span><br><span class="line"># Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see &quot;Install from sources&quot; below.  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-1.2.1-cp35-cp35m-linux_x86_64.whl  </span><br><span class="line"></span><br><span class="line"># Mac OS X, CPU only, Python 3.4 or 3.5:  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.1-py3-none-any.whl  </span><br><span class="line"></span><br><span class="line"># Mac OS X, GPU enabled, Python 3.4 or 3.5:  </span><br><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-1.2.1-py3-none-any.whl</span><br></pre></td></tr></table></figure><p>看到上面每个版本中的tensorflow后面都有一个1.2.1的版本号，我们如果需要最新的版本，可以到<a href="https://github.com/tensorflow/tensorflow/中去查看，进入网站之后，根据具体情况点击链接查看：" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/中去查看，进入网站之后，根据具体情况点击链接查看：</a><br><img src="http://img.blog.csdn.net/20170706200324505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>比如我要安装的是Mac CPU-only中的python2，就可以点击这个链接来查看：<br><img src="http://img.blog.csdn.net/20170706200600758?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>可以看到后面的最新地址是1.2.1，就可以将上面的地址改成这个最新的版本号，再用下面的命令进行安装：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line"># Python 2 的选择下面的命令来进行安装  </span><br><span class="line">(tensorflow)$ pip install --ignore-installed --upgrade $TF_BINARY_URL  </span><br><span class="line"></span><br><span class="line"># Python 3 的选择下面的命令来进行安装</span><br><span class="line">(tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL</span><br></pre></td></tr></table></figure></p><p>（该命令行的用户名前面的(tensorflow)是因为前面激活了tensorflow而出现的）。</p><p>然后就可以测试安装了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ source activate tensorflow  </span><br><span class="line">(tensorflow)$  # Your prompt should change.</span><br><span class="line"></span><br><span class="line"># Run Python programs that use TensorFlow.  </span><br><span class="line"></span><br><span class="line">$ python</span><br><span class="line">Python 2.7.13 |Anaconda 4.4.0 (x86_64)| (default, Dec 20 2016, 23:05:08)</span><br><span class="line">[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">Anaconda is brought to you by Continuum Analytics.</span><br><span class="line">Please check out: http://continuum.io/thanks and https://anaconda.org</span><br><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</span><br><span class="line">&gt;&gt;&gt; sess = tf.Session()</span><br><span class="line">&gt;&gt;&gt; print(sess.run(hello))</span><br><span class="line">Hello, TensorFlow!</span><br><span class="line"># 以上程序没有报错的话就是安装成功了</span><br><span class="line"></span><br><span class="line"># When you are done using TensorFlow, deactivate the environment.  </span><br><span class="line">(tensorflow)$ source deactivate</span><br></pre></td></tr></table></figure><p>本步骤参考博文：<a href="http://blog.csdn.net/nxcxl88/article/details/52704877" target="_blank" rel="noopener">http://blog.csdn.net/nxcxl88/article/details/52704877</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;终于准备开始学习谷歌的深度学习框架tensorflow了，花了一上午的时间，终于配置好了，下面就是详细的步骤：&lt;/p&gt;
&lt;p&gt;首先大家可以参考一下官方的安装tensorflow的方法：&lt;br&gt;&lt;a href=&quot;https://www.tensorflow.org/insta
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="其他" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="Anaconda" scheme="http://camlinzhang.com/tags/Anaconda/"/>
    
      <category term="Tensorflow" scheme="http://camlinzhang.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>MAC Windows linux(树莓派)三平台配置 opencv2.4.13 + QT环境</title>
    <link href="http://camlinzhang.com/2017/05/31/MAC-Windows-linux-%E6%A0%91%E8%8E%93%E6%B4%BE-%E4%B8%89%E5%B9%B3%E5%8F%B0%E9%85%8D%E7%BD%AE-opencv2-4-13-QT%E7%8E%AF%E5%A2%83/"/>
    <id>http://camlinzhang.com/2017/05/31/MAC-Windows-linux-树莓派-三平台配置-opencv2-4-13-QT环境/</id>
    <published>2017-05-31T09:28:32.000Z</published>
    <updated>2019-03-25T04:06:53.703Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、MAC平台"><a href="#一、MAC平台" class="headerlink" title="一、MAC平台"></a>一、MAC平台</h2><p>mac平台上用xcode配置使用opencv的具体操作过程可以参考这篇博文：<br><a href="http://blog.csdn.net/u014365862/article/details/53067565" target="_blank" rel="noopener">http://blog.csdn.net/u014365862/article/details/53067565</a></p><p>在xcode上面配置好了opencv之后就可以进行QT的配置：<br><a href="http://blog.csdn.net/tianzhaixing2013/article/details/52077064" target="_blank" rel="noopener">http://blog.csdn.net/tianzhaixing2013/article/details/52077064</a><br>在上面的博文的基础上我补充一点，在helloCV.pro 末尾添加的代码中可以替换成下面的完整版本，但是注意不同版本的opencv的库不同，此处用的是opencv2.4.13：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 导入opencv的路径</span><br><span class="line">INCLUDEPATH += /usr/local/include</span><br><span class="line">INCLUDEPATH += /usr/local/include/opencv</span><br><span class="line">INCLUDEPATH += /usr/local/include/opencv2</span><br><span class="line"></span><br><span class="line"># 导入opencv的完整库</span><br><span class="line">LIBS += -L/usr/local/lib \</span><br><span class="line">    -lopencv_calib3d \</span><br><span class="line">    -lopencv_contrib \</span><br><span class="line">    -lopencv_core \</span><br><span class="line">    -lopencv_features2d \</span><br><span class="line">    -lopencv_flann \</span><br><span class="line">    -lopencv_gpu \</span><br><span class="line">    -lopencv_highgui \</span><br><span class="line">    -lopencv_imgproc \</span><br><span class="line">    -lopencv_legacy \</span><br><span class="line">    -lopencv_ml \</span><br><span class="line">    -lopencv_nonfree \</span><br><span class="line">    -lopencv_objdetect \</span><br><span class="line">    -lopencv_ocl \</span><br><span class="line">    -lopencv_photo \</span><br><span class="line">    -lopencv_stitching \</span><br><span class="line">    -lopencv_superres \</span><br><span class="line">    -lopencv_video \</span><br><span class="line">    -lopencv_videostab \</span><br></pre></td></tr></table></figure><h2 id="二、Windows平台"><a href="#二、Windows平台" class="headerlink" title="二、Windows平台"></a>二、Windows平台</h2><p>win10中配置opencv的具体步骤：<br><a href="http://blog.csdn.net/hfxmath/article/details/53452874" target="_blank" rel="noopener">http://blog.csdn.net/hfxmath/article/details/53452874</a><br>还是补充一下，在上文中写VS2015中项目的属性表时可能会出现以下的情况，和博文中的稍有不符，只需要在项目根目录上面点反键选择属性在那个对话框里面设置即可，这样就可以把整个下面呢四个分目录里面的内容都设置了。<br><img src="http://img.blog.csdn.net/20170531160717476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>按照上面的步骤配置好opencv的环境之后就可以接着在QT中配置opencv的环境了，这里说明一下，在下载QT的时候要下载msvc版本，而不要下载MinGW版本，否则要cmake编译OpenCV然后再导入，这样就会很麻烦，大家可以参考：<br><a href="http://www.tuicool.com/articles/VZfue2" target="_blank" rel="noopener">http://www.tuicool.com/articles/VZfue2</a><br>只需要把下载的opencv的包换成2.4.13版本就可以了。其中有些位置还是要更改一下的，在我按照上面的博文实践的过程中，第五步修改工程的pro文件中，按照上面的博文的方法不成功，导入头文件时会报错，可以使用下面的代码来更改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 导入opencv的路径</span><br><span class="line">INCLUDEPATH+=D:/Opencv/opencv/build/include/opencv</span><br><span class="line">INCLUDEPATH+=D:/Opencv/opencv/build/include/opencv2</span><br><span class="line">INCLUDEPATH+=D:/Opencv/opencv/build/include</span><br><span class="line"></span><br><span class="line"># 判断是debug还是release模式的编译然后导入不同的库文件</span><br><span class="line">CONFIG(debug,debug|release) &#123;</span><br><span class="line">LIBS += -LD:/Opencv/opencv/build/x64/vc14/lib \</span><br><span class="line">    -lopencv_world320d \</span><br><span class="line">&#125;</span><br><span class="line">else&#123;</span><br><span class="line">LIBS += -LD:/Opencv/opencv/build/x64/vc14/lib \</span><br><span class="line">    -lopencv_world320 \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用以上的代码就可以实现配置了。</p><h2 id="三、Linux平台"><a href="#三、Linux平台" class="headerlink" title="三、Linux平台"></a>三、Linux平台</h2><p>在Linux平台上面安装opencv还是一个比较耗时的工作，我是采用树莓派平台作为对象来进行配置的。<br>    首先通过mac的终端或者win下的Xshell(比putty的界面要友好一些)连接树莓派对其进行控制。以下的命令都是通过上面的终端或者Xshell进行输入的。<br>    利用ssh通信连接树莓派：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh pi@192.168.0.102</span><br></pre></td></tr></table></figure><p>后面的是IP地址，根据你们树莓派的ip地址来修改。连接之后输入密码即可启动树莓派的ssh通信。<br>利用<code>sudo raspi-config</code> 可以进入树莓派的配置页面，激活相应需要的选项。</p><p>现在来升级树莓派，以提供有关的最新发行包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure><p>为了获取发行包，你可以安装一个synaptic软件包管理器，可用于图形用户界面取代输入<code>sudo apt-get</code> 命令来安装这个软件包，安装好opencv之后，用它来安装丢失的包非常的便利：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install synaptic</span><br></pre></td></tr></table></figure><p>（其中的 -y 是指在安装过程中对于所有需要用户确认的步骤都默认为yes）。</p><p>我们必须安装python的科学计算库和数学库来处理图像以及获得一些附加功能，输入以下命令来进行安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install python-numpy python-scipy python-nose python-pandas python-matplotlib ipython-notebook python-sympy</span><br></pre></td></tr></table></figure><p>接下来需要安装GtkGLExt以支持OpenGL的渲染，OpenGL是一个通常由GPU使用以获取三维和二维图形的API，我们将在构建OpenCV时启用它：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install libgtkglext1-dev</span><br></pre></td></tr></table></figure><p>为了在树莓派中打击那开发环境并将所有的必备库更新到最新版本中，同时使其与Linux操作系统兼容，可以用CMake配置安装包病管理构建过程，输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install build-essential camke pkg-config</span><br></pre></td></tr></table></figure><p>然后就可以安装QT了(以下安装的是QT4)，输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install  qtcreator qt4-dev-tools libqt4-dev libqt4-core libqt4-gui v4l-utils</span><br></pre></td></tr></table></figure><p>以上我们就准备好了所有的必备软件包，就可以下载opencv库了，我们还是和上面一样，下载opencv2.4.13版本，并且以后可以升级opencv，输入以下命令来下载OpenCV库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.4.13/opencv-2.4.13.zip</span><br></pre></td></tr></table></figure><p>下载完官方源码文件之后，用以下命令来解压文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip opencv-2.4.13.zip</span><br></pre></td></tr></table></figure><p>如果下载的文件是原始码的压缩格式，则可以用以下命令来解压：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xzvf opencv-2.4.13.tar.gz</span><br></pre></td></tr></table></figure><p>执行完上述命令之后，整个文件会放到opencv-2.4.13文件夹中，在该文件夹内创建一个build文件(用于装后面的cmake编译文件)，依次输入以下命令完成上述过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd opencv-2.4.13</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br></pre></td></tr></table></figure><p>接下来，我们准备用CMake命令为生成发行版的库配置类型，查看CMake的输出，检查哪些包安装了，哪些没有安装，没有安装的可以手动安装CMake没有安装的包。在查看CMake输出之后，你可以发现所有的库和包之前都标记有YES和TRUE，如果不是这样，只需要用synaptic包管理器列出这样额库和包的名称进行安装即可。在如下的命令最后加上 “..” 对于成功创建make文件非常重要：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE -D WITH_OPENGL=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D WITH_QT=ON -D CMAKE_INSTALL_PREFIX=/USR/LOCAL -D WITH_TBB=ON -D WITH_V4L=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure><p>下一步就是编译整个库，正如我们所看见的那样，OpenCV有很多的组件。<br>我们用前述命令cmake生成了一个内部的make文件。它将被用于我们所选择的环境当中，由于OpenCV库中各组件之间的依赖性，执行make命令非常有用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure><p>在启动对话莓派编译的过程中不要打扰它，即使中间的某个阶段编译挂起或者停滞不前，甚至树莓派意外关机都不用担心，只要再次开启对话，进入到同一个文件夹下重新输入make命令，这就可以从之前停止的地方继续进行。</p><p>一旦这个耗时最长的任务结束了，我们就可以在树莓派上安装已编译好的库，安装库花费的时间不会像前面的编译那样漫长：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>我们应该在/etc文件夹下降OpenCV的配置文件设置为ldconfig。ldconfig可以在/etc/ld.so.conf文件、/lib与/usr/lib路径下显示在命令行中的目录里的最新共享库生成必要的链接，这样做的目的是告诉树莓派操作系统我们已经安装了OpenCV库，因此要针对OpenCV的配置单独创建一个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/ld.so.conf.d/opencv.conf</span><br></pre></td></tr></table></figure><p>在刚才打开的文件中输入下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/lib</span><br></pre></td></tr></table></figure><p>然后按下Ctrl+O键保存，Ctrl+X键退出文件</p><p>输入 sudo ifconfig, 使文件中所做的修改生效，我们将在交互式shell源文件（可用作终端仿真器）中把代码粘贴到文件的末尾。文件相当长，所以要用键盘向下滚动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/bash.bashrc</span><br></pre></td></tr></table></figure><p>在文件最后输入下面的几行代码，请注意在新的一行代码中输入export命令，然后保存、退出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig</span><br><span class="line">export PKG_CONFIG_PATH</span><br></pre></td></tr></table></figure><p>对该文件的编辑可以让我们在任意目录中自由编译OpenCV代码，这和将库的环境变量添加到Linux操作系统中类似。至此，我们的配置就全部完成了。</p><p>如果感兴趣的话，也可以在上面安装的QT中配置wringPi库来用QT控制树莓派连接的相关传感器，给树莓派安装wringPi库参考：<br><a href="http://blog.csdn.net/xukai871105/article/details/17737005" target="_blank" rel="noopener">http://blog.csdn.net/xukai871105/article/details/17737005</a><br>安装好之后在QT的后缀是 .pro文件最后加上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LIBS += -L/usr/lib \</span><br><span class="line">-lwringPi</span><br></pre></td></tr></table></figure><p>就配置好了。</p><p>花了两次监考的时间来码，也是第一次码这么多，希望能对大家有所帮助哦！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、MAC平台&quot;&gt;&lt;a href=&quot;#一、MAC平台&quot; class=&quot;headerlink&quot; title=&quot;一、MAC平台&quot;&gt;&lt;/a&gt;一、MAC平台&lt;/h2&gt;&lt;p&gt;mac平台上用xcode配置使用opencv的具体操作过程可以参考这篇博文：&lt;br&gt;&lt;a href=
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="其他" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="opencv2.4.13" scheme="http://camlinzhang.com/tags/opencv2-4-13/"/>
    
      <category term="QT" scheme="http://camlinzhang.com/tags/QT/"/>
    
  </entry>
  
  <entry>
    <title>树莓派实现上传文件到百度云</title>
    <link href="http://camlinzhang.com/2017/05/25/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%9E%E7%8E%B0%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%88%B0%E7%99%BE%E5%BA%A6%E4%BA%91/"/>
    <id>http://camlinzhang.com/2017/05/25/树莓派实现上传文件到百度云/</id>
    <published>2017-05-25T02:03:38.000Z</published>
    <updated>2019-03-25T04:00:43.539Z</updated>
    
    <content type="html"><![CDATA[<p>自己开的第一篇帖，因为因为这个小问题一直弄了两天，终于在小伙伴的帮助下完成了，开个贴纪念下。。。<br>以下介绍两种方法：</p><h2 id="一、利用python的baidupcsapi包来进行上传"><a href="#一、利用python的baidupcsapi包来进行上传" class="headerlink" title="一、利用python的baidupcsapi包来进行上传"></a>一、利用python的baidupcsapi包来进行上传</h2><p>（网上一般都是我下面要说的第二种方法）<br>完成上面的任务首先需要在树莓派上面用pip装上一个baidupcsapi的python包：<code>sudo pip install baidupcsapi</code><br>（注意此处如果把GitHub里面的包复制到树莓派pyhton下的dist-package中是不行的，必须用以上命令安装才可以。）<br>但是问题就来了，pip安装这个包会出现以下问题：<img src="http://img.blog.csdn.net/20170525100432926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>（这个是我安装另一个bypy包出现的问题，跟安装baidupcsapi的问题是一样的，因为安装baidupcsapi出问题的图我找不到了。。。）<br>网上有很多这个的教程：<a href="http://www.jianshu.com/p/785bb1f4700d" target="_blank" rel="noopener">http://www.jianshu.com/p/785bb1f4700d</a><br>但是问题又来了，当我在/root下添加文件夹.pip，然后再这个文件夹下添加pip.conf之后，始终还是报上面的错误。这时小伙伴的作用就显现了，他把.pip这个文件夹放在了所谓的主目录/home下，而不是根目录下，结果神奇般的好了。其实刚开始也还是有问题，删了一遍然后重新新建就好了。（有大神知道为啥可以告诉我一下）具体命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd home</span><br><span class="line">sudo mkdir .pip</span><br><span class="line">cd .pip</span><br><span class="line">sudo nano pip.conf</span><br></pre></td></tr></table></figure><p>再在pip.conf里面加上上面教程里面的内容就可以了。<br>这个时候不论是baidupcsapi这个python包还是后面要讲到的bypy都可以下载了。<br>安装成功后，就可以参考 <a href="https://github.com/ly0/baidupcsapi" target="_blank" rel="noopener">https://github.com/ly0/baidupcsapi</a> 里面的有关baidupcs的有关内容，相关api参考<a href="http://baidupcsapi.readthedocs.io/en/latest/api.html#" target="_blank" rel="noopener">http://baidupcsapi.readthedocs.io/en/latest/api.html#</a><br>以下贴一段测试代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from baidupcsapi import PCS</span><br><span class="line"></span><br><span class="line">pcs = PCS(&apos;百度云用户名&apos;, &apos;百度云密码&apos;)</span><br><span class="line">test_file = open(&apos;文件路径&apos;, &apos;r&apos;)</span><br><span class="line">ret = pcs.upload(&apos;百度云存储路径&apos;, test_file, &apos;12345.png&apos;, callback=None)</span><br></pre></td></tr></table></figure><p>运行上面的代码后会显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://passport.baidu.com/cgi-bin/genimage?jxGf007e28cc192c11502d114fa9801657b072c4406c3053114</span><br><span class="line">open url aboved with your web browser, then input verify code &gt;</span><br></pre></td></tr></table></figure><p>把上面的地址复制到浏览器打开后，会有个验证码复制上去就好了。多次登录之后就会默认用户，就可以不用验证码了。</p><h2 id="二、利用百度云盘的python客户端进行命令行上传"><a href="#二、利用百度云盘的python客户端进行命令行上传" class="headerlink" title="二、利用百度云盘的python客户端进行命令行上传"></a>二、利用百度云盘的python客户端进行命令行上传</h2><p>网上的教程基本上都是用这个这种方法来进行上传的，如下两个链接：<br><a href="http://blog.csdn.net/a_lpha/article/details/53637669" target="_blank" rel="noopener">http://blog.csdn.net/a_lpha/article/details/53637669</a><br><a href="http://tieba.baidu.com/p/3439470932" target="_blank" rel="noopener">http://tieba.baidu.com/p/3439470932</a><br>但是我试了之后先开始是没有办法用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install bypy</span><br></pre></td></tr></table></figure><p>安装bypy包，然后用上面的方法换pip源之后就可以安装了，可是又没法用命令：<br><code>sudo bypy.py info</code><br>来执行命令，结合下面那个链接的内容，抱着试一试的心态试了一下用命令：<br><code>python -m bypy info</code></p><p><img src="http://img.blog.csdn.net/20170525165652795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjg3MzE1NzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>结果成功了，网上说python后面的-m参数是将模块按照脚本执行，其实也并不是很清楚为啥，感觉自己真的是码代码靠火的赶脚。</p><p>至此，因为pip的换源成功终于把两种方法都弄好了，好开森！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自己开的第一篇帖，因为因为这个小问题一直弄了两天，终于在小伙伴的帮助下完成了，开个贴纪念下。。。&lt;br&gt;以下介绍两种方法：&lt;/p&gt;
&lt;h2 id=&quot;一、利用python的baidupcsapi包来进行上传&quot;&gt;&lt;a href=&quot;#一、利用python的baidupcsapi包
      
    
    </summary>
    
      <category term="技术博客" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="其他" scheme="http://camlinzhang.com/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="树莓派" scheme="http://camlinzhang.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
      <category term="百度云" scheme="http://camlinzhang.com/tags/%E7%99%BE%E5%BA%A6%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>追风筝的人</title>
    <link href="http://camlinzhang.com/2015/03/27/%E8%BF%BD%E9%A3%8E%E7%AD%9D%E7%9A%84%E4%BA%BA/"/>
    <id>http://camlinzhang.com/2015/03/27/追风筝的人/</id>
    <published>2015-03-27T09:24:48.000Z</published>
    <updated>2019-03-25T11:21:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1f93s0qm9j30go0ndt9s.jpg" alt=""></p><h2 id="读书笔记"><a href="#读书笔记" class="headerlink" title="读书笔记"></a>读书笔记</h2><blockquote><p>许多年过去了，人们说陈年旧事可以被埋葬，然而我终于明白这是错的，因为往事回自行爬上来。</p></blockquote><blockquote><p>罪行只有一种，那就是盗窃，其他罪行都是盗窃的变种。当你杀害一个人，你偷走一条性命，你偷走他妻子身为人妇的权利，夺走他子女的父亲。当你说谎，你偷走别人知道真相的权利。当你诈骗，你偷走公平的权利。</p></blockquote><blockquote><p>我可以蹚进这条大川，让自己的罪恶沉在最深处，让流水把我带往远方，带往没有灵魂，没有往事，没有罪恶的远方。（一个违背自己良心的阿米尔的希望）</p></blockquote><blockquote><p>雅尔达的朝阳。（黑暗里的光明）</p></blockquote><blockquote><p>拉辛汗打电话来那晚，我躺在黑暗中，眼望月光刺穿黑暗、在墙壁上投射出来的银光。也许快到黎明的某一刻，我昏昏睡去。梦见哈桑在雪地奔跑，绿色长袍的后摆拖在他身后，黑色的橡胶靴子踩得积雪吱吱响。他举臂挥舞：为你，千千万万遍！</p></blockquote><blockquote><p>美国给你灌输了乐观的性子，这也是她了不起的地方。那非常好。我们是忧郁的民族，我们阿富汗人，对吧？我们总是陷在悲伤和自恋中。我们在失败、灾难面前屈服，将这些当成生活的实质，甚至视为必须。我们总是说，生活会继续的。但我在这里，没有向命运投降，我看过几个很好的大夫，他们给的答案都一样。我信任他们，相信他们。像这样的事情，是真主的旨意。”“只有你想做和不想做的事情罢了。”我说。</p></blockquote><blockquote><p>得到了再失去，总是比从来就没有得到更伤人。</p></blockquote><blockquote><p>你懦弱，这是你的天性。这并非什么坏事，因为你从不强装勇敢，这是你的优点。只要三思而后行，懦弱并没有错。可是，当一个懦夫忘了自己是什么人……愿真主保佑他。</p></blockquote><blockquote><p>亲爱的阿米尔：<br>安拉保佑，愿你毫发无损地看到这封信。我祈祷我没让你受到伤害，我祈祷阿富汗人对你不至于太过刻薄。自从你离开那天，我一直在为你祈祷。那些年来，你一直在怀疑我是否知道。我确实知道。事情发生之后不久，哈桑就告诉我了。你做错了，亲爱的阿米尔，但别忘记，事情发生的时候，你还只是个孩子，一个骚动不安的小男孩。当时你对自己太过苛刻，现在你依然如此——在白沙瓦时，我从你的眼神看出来。但我希望你会意识到：没有良心、没有美德的人不会痛苦。我希望这次你到阿富汗去，能结束你的苦楚。亲爱的阿米尔，那些年来，我们一直瞒着你，我感到羞耻。你在白沙瓦大发雷霆并没错。你有权利知道，哈桑也是。我知道这于事无补，但那些年月，我们生活的喀布尔是个奇怪的世界，在那儿，有些事情比真相更加重要。亲爱的阿米尔，我深知在你成长过程中，你父亲对你有多么严厉。我知道你有多么痛苦，多么渴望得到他的宠爱，而我为你感到心痛。但你父亲是一个被拉扯成两半的男人，亲爱的阿米尔：被你和哈桑。他爱你们两个，但他不能公开表露对哈桑的爱，以尽人父之责。所以他将怨气发泄在你身上——你恰好相反，阿米尔，你是社会承认的一半，他所继承的财富，以及随之而来的犯罪免受刑罚的特权，统统都会再赠给你。当他看到你，他看到自己，还有他的疚恨。你现在依然愤愤不平，而我明白，要你接受这些为时尚早。但也许有朝一日，你会明白，你父亲对你严厉，也是对自己严厉。你父亲跟你一样，也是个痛苦的人，亲爱的阿米尔。我无法向你形容，在听到你父亲的死讯之后，我心里的悲恸有多么深。我爱他，因为他是我的朋友，但也因为他是个好人，也许甚至是个了不起的人。而我想让你明白的是，你父亲的深切自责带来了善行，真正的善行。我想 起他所做的一切，施舍街头上的穷人，建了那座恤孤院，把钱给有需要的朋友，这些统统是他自我救赎的方式。而我认为，亲爱的阿米尔，当罪行导致善行，那就是真正的获救。我知道到头来，真主会宽恕。他会宽恕你父亲，宽恕我，还有你。我希望你也一样。如果你可以的话，宽恕你父亲。如果你愿意的话，宽恕我。但，最重要的是，宽恕你自己。我给你留下一些钱，实际上，我所能留下的，也无非就是这些了。我想你若回到这儿，兴许会有些开销，而那些钱足够让你用的了。白沙瓦有个银行，法里德知道在哪里。钱存在保险箱里面，我给你留了钥匙。至于我，是该走的时候了。我来日无多，而我希望独自度过。请别找我。这是我最后的请求。我将你交在真主手中。你永远的朋友拉辛汉。</p></blockquote><blockquote><p>大的走廊上，没有窗，墙边的金属折叠椅上坐满了人，还有人坐在薄薄的破地毯上。我又想尖叫。我想起上次有这种感觉，是跟爸爸在油罐车的油罐里面，埋在黑暗和其他难民之间。我想把自己撕成碎片，离开这个地方，离开现实世界，像云朵那样升起，飘荡而去，融进湿热的夏夜，在某个遥远的地方，在山丘上方飘散。但我就在这儿，双脚沉重如水泥块，肺里空气一泻而空，喉咙发热。无法随风而去。今晚没有别的世界。我合上双眼，鼻子里塞满走廊的种种味道：汗水和氨水的气味、药用酒精和咖喱的气味。整条走廊的天花板上布满昏暗的灯管，飞蛾围绕，我听见它们拍打翅膀的声音。我听见谈话声、默默的啜泣声、擤鼻声；有人在呻吟，有人在哀叹，电梯门砰地一声打开，操作员用乌尔都语呼喊某人。</p></blockquote><blockquote><p>俯视索拉博，他嘴角的一边微微翘起。微笑。斜斜的。几乎看不见。但就在那儿。在我们后面，孩子们在飞奔，追风筝的人不断尖叫，乱成一团，追逐那只在树顶高高之上飘摇的断线风筝。我眨眼，微笑不见了。但它在那儿出现过，我看见了。“你想要我追那只风筝给你吗？”他的喉结吞咽着上下蠕动。风掠起他的头发。我想我看到他点头。“为你，千千万万遍。”我听见自己说。然后我转过身，我追。它只是一个微笑，没有别的了。它没有让所有事情恢复正常。它没有让任何事情恢复正常。只是一个微笑，一件小小的事情，像是树林中的一片叶子，在惊鸟的飞起中晃动着。但我会迎接它，张开双臂。因为每逢春天到来，它总是每次融化一片雪花；而也许我刚刚看到的，正是第一片雪花的融化。我追。一个成年人在一群尖叫的孩子中奔跑。但我不在乎。我追，风拂过我的脸庞，我唇上挂着一个像潘杰希尔峡谷那样大大的微笑。我追。</p></blockquote><h2 id="读后感"><a href="#读后感" class="headerlink" title="读后感"></a>读后感</h2><p>还是那条街道，充斥着来来往往的人群，习惯夜幕里随着音乐的旋律在喧闹和嬉戏里创造一个自己的世界。拖着白色的城堡，看着擦肩而过的人们在欢笑，交谈，打闹，我却只属于我的城堡……仰望鲜有星星的黑暗夜空，仿佛看到了阿米尔和哈桑奔跑的身影，追逐着高高于天际的那美丽的风筝，不曾疲倦。</p><p>曾经的你为了得到严厉的父亲的一次亲近，努力的牵着挂满玻璃片的绳子，用尽全身力气，不吝惜小小的手掌上留下一道道伤痕，只为得到第一名，只为得到那个远方赞美的眼神，只为将跌至父爱深渊里懦弱的自己拽起来，重新获得属于你的那份慈祥的父爱。可是不知道身体的一部分似乎彻底将自己背叛，越是想证明自己的坚强却越是懦弱，那份本不该属于你的嫉妒，那份扭曲的渴望塞满了你毫无抵抗力的渴望父爱的心。孩子的心本是简单，也简单的只想一直拥有着属于自己的那份简单的爱，可是现实总是将这份简单置于无尽的复杂中。</p><p>曾经的你们，沐浴阳光，享受雨水，在树梢上嬉戏，开着永远不厌倦的玩笑，在庄园的草地里打闹，追逐，一切的美好只是属于两个孩子，两个简简单单的孩子。多少次在你割断了对手的风筝之后，他沉着冷静的追赶那个断了线的风筝，最终将它自豪的放在你的面前，只为那句“为你，千千万万遍”。可是大人的世界总是充斥着复杂，年轻的我们总是尝试着用最简单的方式去理解，去解决，可是不层想到却将父亲的良心债扛到了自己的肩上，辗转反侧，始终无法摆脱……<br>你终于明白往事不会被时间埋葬，而是会自己爬上来，年轻的你也过早的了解了那种痛，那种失去你已经拥有的东西的痛。也许真的是太痛了，也可能是自私的心在肆虐，你就是站在那里，看着别人伤害那个曾经为你付出所有的哈桑什么也不做，像你平常一样，一样懦弱，但这次却深深的伤害了那个为你千千万万遍的男孩。但你童真的心却还是不满足，又因此饱受煎熬，于是在冲动中又一次使哈桑陷入困境，最终将这个抢夺那份只属于你的爱的但又为你千千万万遍的哈桑彻底的赶离了你的生活，但你万万没想到的是那属于你的父爱似乎也随着哈桑的离去而永远的离开了你，留下的只有一份深深的良心债。</p><p>你是继承你父亲一切外人看得到的财富的人，而哈桑却是继承着那些看不到的财富的人。年轻的你不明白，但是在一场战争洗礼之后，那些生命中曾经离不开的人一个个离去，那个划破宁静的电话终于让你明白了，也彻底震惊了，但这些还是没有阻止你去寻找那个曾经追风筝的男孩，那个曾经你生命中最宝贵的人，在你无数次像割断别人风筝线一样伤害他之后，你也要做这个追风筝的人，历尽艰辛，满身伤痕的在那个满目疮痍的国度里，将承载着数十载的良心债还的干干净净，彻彻底底。终于在那个阳光洒落的午后又一次割断了风筝线，奔跑在追风筝的路上，又一次看到了那个曾经为你千千万万遍的人的美丽的微笑。</p><p>在这个很深很深的世界里，美丽的生命总是如期而至，一切美好会在不经意间降临，一切苦痛也会在流转的年华里如期离去。我们永远是徜徉在那撒着阳光雨露的美丽世界中的一粒微尘，在风的指引下追逐着，追逐着我们的风筝。曾经的我们用一根线紧紧的拉着它，只是拉着，但是很安心，仿佛拉着他，我们便可以看到世界，看到一切最美的风景。可是世事无常，生活总是会给我们制造最无情的玩笑，我们一个不经意的趔趄，起来后却忽然发现那根我们依赖的风筝线早已被无情的斩断，我们似乎突然明白了这个埋在美丽生命下的巨大阴谋，可是一切好像已经太晚了。随着风筝，我们曾经的美好早已远远的飘走，生命的恶魔只是孤傲的站在那个我们曾经开始的地方，看着阴谋逐渐将我们吞噬。但是固执的我们怎能让那个曾经带领我们感受美丽大千世界的风筝就这样溜走。于是我们不顾浮云遮眼，不顾满身伤痕，只是任曾经的美好充斥着内心，鼓舞激励着我们，让那颗玻璃般的童心第一次面对这世界，真实的世界，也让我们终于成为了那个追风筝的人……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1g1f93s0qm9j30go0ndt9s.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;读书笔记&quot;&gt;&lt;a href=&quot;#读书笔记&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
      <category term="读书笔记" scheme="http://camlinzhang.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
